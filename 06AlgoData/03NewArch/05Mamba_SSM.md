# Mamba: S4结构化状态空间模型剖析

## 1. 引言：结构化状态空间模型的兴起

序列建模是人工智能领域的核心挑战之一，旨在理解和生成各种序列数据，如自然语言文本、语音信号和时间序列数据。传统的循环神经网络(RNN)在处理序列数据方面取得了一定的成功，但其固有的梯度消失或爆炸问题以及顺序处理瓶颈，限制了其在长序列上的学习能力和效率$^{1}$。长短期记忆(LSTM)网络通过引入门控机制在一定程度上缓解了这些问题，但同时也增加了模型的复杂性和计算开销$^{1}$。

继RNN之后，Transformer架构凭借其并行处理能力和强大的上下文表征能力，在自然语言处理等领域取得了革命性突破。然而，Transformer的核心机制自注意力机制-其计算复杂度随序列长度呈二次方增长(O(N2))，这使得其在处理极长序列时面临巨大的计算和内存挑战$^{2}$。这种二次方复杂度的瓶颈，成为了驱动研究者探索更高效长序列建模方法的主要动力。

在这样的背景下，结构化状态空间模型(Structured State Space Models, SSMs)种极具潜力的新兴序列建模范式应运而生。SSMs借鉴了控制理论和信号处理中的经典状态空间表示，通过结构化的循环和状态空间表征，实现了对长序列的高效处理，其计算复杂度通常为线性或近线性$^{2}$。SSMs的理论根基可以追溯到控制理论，用于描述复杂动态系统，这为其提供了与基于注意力的模型不同的理论基础1。从奠基性的S4模型到其后续的Mamba、S5和Jamba等模型，SSMs在计算效率、内存优化和推理速度方面不断取得显著进展$^{1}$。

这种从RNN到Transformer再到SSMs的演进，不仅仅是模型性能的迭代提升，更深层次地反映了在序列建模领域中，研究者们对于"效率-表达能力权衡"这一核心问题的持续探索和突破。RNN在表达能力上受到限制，尤其是在长程依赖方面;LSTM通过增加复杂性提升了记忆能力，但效率和极长序列处理仍是瓶颈;Transformer以其强大的表达能力和并行训练特性取得了巨大成功，却牺牲了在长序列上的计算效率。SSMs，特别是S4和Mamba，则代表了在保证甚至增强对长程依赖建模能力(表达能力)的同时，重新夺回线性或近线性计算复杂度(效率)的尝试。这一过程揭示了人工智能模型发展的一个重要趋势:通过对基础架构假设的重新思考，来克服普遍存在的性能瓶颈和权衡。

此外，SSMs的理论基础源于经典的控制理论1，这标志着不同科学领域的思想向机器学习领域的有益渗透和交叉融合。深度学习的灵感常常来源于神经科学或纯数学，而SSMs从控制理论中引入状态空间表示、连续时间动态、离散化和系统稳定性等概念，为序列建模带来了新的视角。这种跨学科的方法可能为解决机器学习中长期存在的问题(如长程依赖建模和模型可解释性)提供新的途径，通过借鉴其他领域成熟的理论和

Mamba的成功,也可能激励研究者进一步探索来自工程学、物理学等领域的概念,以设计出更鲁棒和高效的人工智能架构。

本报告旨在对 S4模型及其向 Mamba架构的演进进行全面的技术剖析，详细阐述它们的核心机制、性能对比、优势与局限，以及在序列建模领域的更广泛影响。

## 2. S4模型：为高效长程序列建模奠定基础

S4(Structured State Space for Sequence Modeling)模型是SSM发展历程中的一个重要里程碑，它为高效处理长程依赖问题提供了坚实的基础。

### 2.1 状态空间模型的数学基础

S4模型的核心构建于状态空间模型的数学框架之上。一个连续时间状态空间模型通常由以下线性微分方程组定义，该方程组将一维输入信号u(t)映射到一个N维隐状态x(t)，然后再投影到一维输出信号y(t)5:

```
x'(t)=Ax(t)+Bu(t)
y(t)=Cx(t)+Du(t)
```

其中,A、B、C和D是通过梯度下降学习的参数矩阵。为了简化,参数D通常被个跳跃连接5。

为了将SSMs应用于离散输入序列(如 u0,u1,...),需要使用一个步长$\Delta$对连续 SSM进行离散化。常用的离散化方法是双线性变换，它将连续状态矩阵A转换为一个近似的离散状态矩阵A-。离散SSM随后表现为一个递归关系，使其能够像RNN一样进行计算5:

$$
A^{-}=(I-\Delta/ 2\cdot A)^{-1}(I+\Delta/ 2\cdot A) \\
B^{-}=(I-\Delta/ 2\cdot A)^{-1}\Delta B \\
C^{-}=C \\
x_k=A^{-}x_{k-1}+B^{-}u_k \\
y_k=C^{-}x_k
$$

尽管这种递归表示在直觉上易于理解，但由于其顺序计算的特性，在现代硬件上进行训练时效率不高。S4模型的一个关键洞察在于线性时不变(LTI)SSMs与连续卷积之间的重要联系。这使得递归 SSM可以表示为一个离散卷积，其卷积核$K^{-}$有明确的公式5:

$$
K^{-}\in\mathbb{R}^L=(C^{-}B^{-},C^{-}A^{-}B^{-},...,C^{-}A^{L-1}B^{-})
$$

这种从类似RNN的递归到类似CNN的卷积的转换是一项核心创新，它使得模型能够利用快速傅里叶变换(FFT)对长序列进行高效训练。而模型的递归形式则更适合于自回归推理4。S4模型在递归和卷积表示之间的这种对偶性是其能够兼顾训练效率和推理效率的关键。RNN固有的顺序性使其在并行硬件上的训练速度较慢,而CNN虽然高度可并行化训练,但在自回归不那么自然。S4模型由于其LTI特性，拥有这种双重性质4。离散化导向递归形式，而LTI特性允许这种递归展开为全局卷积。这种对偶性使得S4能够利用两者的优势:通过基于FFT的卷积进行并行训练，以及通过递归状态更新进行高效的自回归推理。这突显了一个重要的观点:架构创新往往在于发现能够释放计算效率的数学等价性。

### 2.2 HiPPO框架：捕获长程依赖

基础 SSMs在实际应用中面临的一个主要挑战是其在长序列上表现不佳，这通常归因于梯度消失或爆炸问题。S4通过引入HiPPO(High-order Polynomial Projection Operator)理论来解决这个问题，这是一种用于连续时间记忆的理论框架$^{1}$。

HiPPO框架指定了一类特殊的矩阵A(即HiPPO矩阵)，这些矩阵能够使潜状态x(t)有效地记忆输入u(t)的历史信息。它通过将过去的信息投影到一个多项式基项式)上，从而维持一个压缩但富有表达能力的记忆表示。HiPPO矩阵的有效性在于，它产生的隐状态通过追踪勒让德多项式的系数来记忆其历史。这些系数使得模型能够逼近所有先前的历史，HiPPO矩阵在每一步更新这些系数$^{5}$。一个显著的例子是，在序列MNIST分类任务中,仅仅将一个随机初始化的A矩阵替换为HiPPO矩阵,就能将模型性能从60%提升到98%，这充分证明了HiPPO的重要性5。

HiPPO为SSMs中的记忆机制提供了一种有原则的方法，超越了启发式的门控机制。与LSTM/GRU中通过学习得到的门控机制不同，HiPPO提供了一种数学上推导出的长时记忆方法。LSTM使用门(输入门、遗忘门、输出门)来控制信息流，但其学习过程是数据驱动的，可能缺乏透明度。HiPPO的设计目标是通过将历史信息投影到最优的多项式基上来保留过去的信息$^{1}$，这是一种更结构化、理论基础更坚实的记忆方法。HiPPO矩阵A的特定结构并非随意设定，而是源于对连续信号进行最优在线压缩的目标。这预示着在神经网络记忆机制设计中，一种向更具理论原则性的方法转变的趋势，可能催生出更鲁棒和易于理解的模型。HiPPO的成功$^{5}$验证了这一方向的正确性。

### 2.3 S4架构：关键创新与计算特性

S4通过引入优化的参数化方法(结构化矩阵)，在保持状态空间建模优势的同时，显著降低了内存开销$^{1}$。然而，即便使用了HiPPO初始化，朴素地计算卷积核$K^{-}$(涉及$A^{-}$的重复矩阵乘法)仍然会导致O(N2L)的运算量和O(NL)的空间复杂度，这对于长序列来说计算成本过高5。

S4通过专注于具有特殊结构的SSMs-一复数空间中的对角加低秩(Diagonal Plus Low-Rank, DPLR)结构来克服这一计算瓶颈。一个DPLR SSM被定义为$(\wedge$-PQ$\cdot$,B,C)，其中$\wedge$是对角矩阵，而P,Q,B,C是向量(假设秩为1)。S4通过三个步骤实现了对核计算的加速$^{5}$:

1. 截断生成函数(Truncated Generating Function):S4不是直接计算${ K}^{-}$，而是通过评估其截断生成函数来计算其频谱。这将问题从矩阵幂运算转化为矩阵求逆运算，后者可以更有效地计算。

2. 对角情况下的柯西核(Cauchy Kernel for Diagonal Case):对于对角矩阵A=\的情况，生成函数可以写成一种包含柯西核的形式。这有效地用加权点积替换了矩阵求逆，从而大大提高了计算速度。

3. 低秩校正的伍德伯里恒等式(Woodbury Identity for Low-Rank Correction):为了放宽对角假设并包含低秩分量(P,Q)，应用了伍德伯里恒等式。该恒等式允许对角加秩-1项的逆以对角项的逆来表示，从而有效地将问题简化回高效的对角情况。

此外，HiPPO矩阵本身虽然不是直接的DPLR结构，但它是正规加低秩(NormalPlus Low-Rank,NPLR)结构。正规矩阵是酉可对角化的,这使得NPLR矩阵从SSM模型的角度来看，本质上等同于DPLR矩阵。S4利用这一点，首先将HiPPO矩阵写成正规加低秩项，然后将其对角化以提取DPLR项。一个额外的简化是低秩分量可以绑定为P=Q，这有助于提高稳定性5。

通过这些结构化计算,S4实现了亚二次复杂度(O(NlogN))1。同时,S4与现代硬件加速器(如GPU和TPU)兼容，支持高效并行化，能够处理数万个词元长度的序列而不会消耗过多内存1。S4在诸如长程竞技场(LongRange Arena,LRA)等长程依赖基准测试中取得了当前最优结果1。

DPLR结构的引入并非微小的优化，而是使S4在处理长序列时计算上可行的关键因素。核心SSM方程虽然简单，但将其朴素地应用于长序列速度过慢5。HiPPO矩阵虽然对记忆有益,但也需要计算上易于管理。DPLR结构(以及HiPPO的NPLR特性)使得复杂的矩阵运算(特别是卷积核的计算)能够分解为使用柯西核和伍德伯里恒等式等技术的高效步骤5。可以说，没有DPLR，S4可能仍停留在理论层面。这凸显了算法和结构创新在深度学习中与概念创新同等重要。

下表总结了RNN/LSTM、Transformer和 S4在关键架构和计算特性上的对比:

**表T1:核心序列模型的架构与计算特性对比**

| 特性 | RNN/LSTM | Transformer | S4(Structured State Space) |
|------|----------|------------|---------------------------|
| 核心机制 | 循环连接,门控机制 (LSTM) | 自注意力机制,位置编码 | 状态空间模型(SSM) +HiPPO初始化 |
| 输入依赖性(参数) | 时不变(权重共享) | 输入依赖(通过注意力权重) | 时不变(固定A,B,C矩阵) |
| 长程依赖处理 | 梯度消失/爆炸, LSTM有所缓解 | 优秀,但受限于上下文窗口和二次复杂度 | 通过HiPPO和SSM结构高效捕获 |
| 训练复杂度 | O(L·D2)(顺序) | O(L2·D)(并行,自注意力瓶颈) | O((L+N)log(L+N))或O(L·N)(卷积模式,FFT) |
| 推理复杂度 | O(L·D2)(自回归时O(D2)每步) | O(L2·D)(自回归时O(L·D)每步,有KV缓存) | O(L·N)(自回归时O(N)每步,递归模式) |
| 并行性(训练) | 有限(沿时间维度顺序) | 高(层内并行) | 高(卷积模式) |
| 内存(训练) | O(D)或O(L·D) (BPTT) | O(L2+L·D) | O(L·N)或O(L+N) |
| 内存(推理/KV缓存) | O(D)(隐状态) | O(L·D)(KV缓存) | O(N)(隐状态) |

*注:L为序列长度，D为模型维度/隐藏层大小，N为S4的状态维度。复杂度为近似值，具体取决于实现。资料来源:1。*

此表清晰地展示了S4模型在解决RNN和Transformer局限性方面的努力，并为理解Mamba如何进一步改进S4奠定了基础。

## 3. Mamba：选择性状态空间与硬件感知设计

尽管S4模型在长序列建模方面取得了显著进展，但其依赖于静态、时不变参数(LTI特性)的特性，使其在处理信息密集型数据或需要基于内容进行推理的任务时表现欠佳1。S4的LTI特性意味着SSM参数(A,B,C)是固定的，不会根据输入序列进行调整。Mamba在S4的基础上，通过引入动态的、输入依赖的参数化方法，实现了能力的巨大

Mamba在S4的基础上，通过引入动态的、输入依赖的参数化方法，实现了能力的巨大提升1。这是Mamba相较于S4的核心概念飞跃。

### 3.1 Mamba的核心架构创新

Mamba架构的核心创新在于其选择性状态空间机制和为之配套的硬件感知并行算法。

#### 3.1.1 选择性状态空间(S6)机制：输入依赖的动态性

Mamba最核心的改进是使其SSM参数(特别是离散化步长△、输入矩阵B和输出矩阵C)成为当前输入x的函数4。这一机制使得模型能够"根据当前词元选择性地沿序列长度维度传播或遗忘信息"7。这种数据依赖的选择机制能够动态地过滤输入，通过关注相关信息并丢弃无关数据，显著改善了长程依赖的建模能力1。

选择机制对于处理离散模态(如文本和DNA)至关重要，在这些领域，S4的静态特性是一个弱点$^{7}$。Mamba通过使参数$\Delta(x), B(x), C(x)$随输入x变化,根本性地改变了 S4中所有词元都按相同"规则"(固定的A,B,C矩阵)处理的模式。这意味着模型可以根据当前输入决定保留多少过去的状态(通过△),以及如何将当前输入转换到状态将状态转换到输出(通过C)。这种做法在理念上更接近于注意力机制(其中交互是输入依赖的),但它是通过具有线性扩展性的循环机制实现的。其结果是M"基于内容的推理"7,这是 Transformer相较于先前 SSMs的主要优势,也使得 Mamba适用于像语言建模这样复杂、信息密集的任务。

根据8中的算法2(SSM+Selection，或称S6机制)，这些输入依赖参数的具体形式为:

```
sB(x)=LinearN(x)
sC(x)=LinearN(x)
sΔ(x)=BroadcastD(Linear1(x))
T△=softplus
```

其中Lineard是一个参数化的到维度d的投影。

Mamba中的选择性SSM递归在特定条件下(例如N=1,A=-1,B=1,s△=Linear(xt),=softplus)会退化为一个门控循环单元:

```
gt=o(Linear(xt))
ht=(1-gt)ht-1+gt xt
```

这种联系表明，SSM中的$\Delta$参数扮演了类似于RNN门控机制的广义角色，控制着模型是关注当前输入还是忽略当前输入并保持先前状态的平衡8。一个大的$\Delta$会重置状态并关注当前输入，而一个小的$\Delta$则会保持状态并忽略当前输入。使B和C也具有选择性，则允许模型更细致地控制输入是否进入状态以及状态是否进入输出，从而根据内容和上下文调节循环动态。

#### 3.1.2 硬件感知并行算法：高效计算

输入依赖的选择性使得Mamba的SSM变为时变系统，从而破坏了S4赖以进行高效卷积训练的LTI特性$^{3}$。Mamba通过设计一种硬件感知的并行算法来克服这一挑战,该算法使用扫描(scan)操作以循环方式计算模型，并针对现代GPU进行了优化$^{1}$。

Mamba的成功并不仅仅归功于选择性SSM的思想，同样重要的是其算法与硬件能力的务实协同设计。选择机制打破了S4的卷积技巧$^{8}$，如果没有高效的替代方案，模型将会非常缓慢。并行扫描、核融合和重计算等技术是明确为利用GPU内存层级HBM)和并行性而设计的$^{8}$，这与FlashAttention背后的理念相似$^{12}$。这标志着深度学习中的算法创新必须与硬件感知的实现齐头并进，才能取得最先进的结果，这是一级"的模型设计方法。一个潜在的更深远影响是，未来的模型开发可能越来越需要同时具备机器学习理论和系统/硬件优化方面的专业知识。

该硬件感知算法主要利用了三种经典技术$^{8}$:

1. 核融合(Kernel Fusion):离散化、扫描以及与C矩阵相乘等步骤被融合成单个计算核,而不是作为独立的、需要在HBM(高带宽内存)和SRAM(静态随机存取存储器)之间读写大型中间张量的操作。这减少了O(N)(SSM状态维度N)倍的内存I/O，带来了显著的速度提升(例如20-40倍)。具体过程包括:从HBM读取$\Delta$,A,B,C到快速的SRAM;在SRAM中执行离散化得到$A_{-}^{-},B_{-}^{-}$;执行并行关联扫描，在SRAM中得到中间状态;与C相乘并求和，得到输出，然后写回HBM。如果序列长度L过长无法完全放入SRAM，序列会被切分成块，对每个块执行融合扫描。

2. 并行扫描(ParallelScan):为了避免递归的顺序性，采用了功耗高效的并行扫描算法(如Blelloch扫描)，以在序列长度维度上并行化计算。

3. 重计算(Recomputation):反向传播所需的中间状态在反向传递过程中，当输入从HBM加载到SRAM时重新计算，而不是存储起来。这避免了存储大量中间状态导致的内存爆炸，并使得融合选择性扫描层的内存需求与使用FlashAttention的优化Transformer实现相当。每个选择性SSM层每个词元大约存储16字节的激活值。

这种硬件感知设计使Mamba在序列长度上实现了线性时间复杂度O(L)1。

#### 3.1.3 简化的同质架构

Mamba通过将先前SSM架构的设计与Transformer的MLP块的理念相结合，形成了一个单一的、同质的Mamba块，从而简化了以往的深度序列模型架构4。这个块通常使用SiLU/Swish激活函数。这种统一的块取代了Transformer中更为复杂的注意力和MLP块9。这种简化的架构可能预示着向更参数高效和结构同质的设计的转变。Transformer拥有独特的注意力和MLP块，各自有其参数和计算模式。Mamba将这些功能集成到一个更同质的结构中$^{8}$。这可能有助于简化模型缩放、实现更均匀的参数分布，并由于块内计算模式更规则而可能更好地利用硬件。Mamba能够在没有独立的复杂注意力头或大型MLP块的情况下取得强大性能$^{9}$，这表明选择性SSM本身具有高度的表达能力。

### 3.2 Mamba的关键贡献与理论基础

Mamba是首个在预训练和下游任务中均能达到Transformer级别性能的线性时间序列模型$^{4}$。它能够有效地建模长程依赖，在真实数据上的性能随序列长度增加而提升，可处理长达百万的序列。其核心的选择机制赋予了模型进行内容感知推理的能力，这是以往与注意力机制相关联的关键能力。

下表T1(修订版)更新了核心序列模型的对比，加入了Mamba:

**表T1(修订版):核心序列模型的架构与计算特性对比**

| 特性 | RNN/LSTM | Transformer | S4(Structured State Space) | Mamba |
|------|----------|------------|---------------------------|-------|
| 核心机制 | 循环连接,门控机制(LSTM) | 自注意力机制,位置编码 | 状态空间模型(SSM)+HiPPO初始化 | 选择性SSM(S6),硬件感知扫描 |
| 输入依赖性(参数) | 时不变(权重共享) | 输入依赖(通过注意力权重) | 时不变(固定A,B,C矩阵) | 输入依赖(选择性参数△(x),B(x),C(x)) |
| 长程依赖处理 | 梯度消失/爆炸, LSTM有所缓解 | 优秀,但受限于上下文窗口和二次复杂度 | 通过HiPPO和SSM结构高效捕获 | 通过选择机制和SSM结构高效捕获,可扩展至百万长度 |
| 训练复杂度 | O(L·D2)(顺序) | O(L2·D)(并行,自注意力瓶颈) | O((L+N)log(L+N))或O(L·N)(卷积模式,FFT) | O(L·D·N)(并行扫描,实际线性O(L)) |
| 推理复杂度 | O(L·D2)(自回归时O(D2)每步) | O(L2·D)(自回归时O(L·D)每步,有KV缓存) | O(L·N)(自回归时O(N)每步,递归模式) | O(L·D·N)(自回归时O(D·N)每步,无KV缓存) |
| 并行性(训练) | 有限(沿时间维度顺序) | 高(层内并行) | 高(卷积模式) | 高(并行扫描) |
| 内存(训练) | O(D)或O(L·D)(BPTT) | O(L2+L·D) | O(L·N)或O(L+N) | O(L·D)(与FlashAttention类似,有重计算) |
| 内存(推理/KV缓存) | O(D)(隐状态) | O(L·D)(KV缓存) | O(N)(隐状态) | O(D·N)(隐状态,无KV缓存) |

*注:L为序列长度，D为模型维度/隐藏层大小，N为S4/Mamba的状态维度。Mamba的训练复杂度由于硬件感知算法，实际表现为关于L的线性。资料来源:1。*

此表通过加入Mamba,可以直接比较四种主要架构。它清晰地突出了Mamba独特的特性组合:输入依赖的参数(类似Transformer，不同于S4)和线性复杂度(类似S4，不同于Transformer)。这张表有力地总结了Mamba在技术版图中的定位，强调了其关键创新以及它如何试图结合先前模型的优点同时缓解其弱点。

## 4. 比较性能分析：Mamba、S4与Transformer

Mamba架构在多种基准测试和任务中展现了其强大的性能和效率优势，尤其是在处理长序列和需要选择性信息处理的场景下。

### 4.1 合成任务性能

合成任务通常被设计用来严格测试模型在特定能力方面的表现，如记忆、选择性处理和模式识别。

- **选择性复制任务(Selective Copying Task)**:此任务要求模型根据随机间隔的指令选择性地记住或忽略输入序列中的某些词元$^{8}$。结果显示，采用S6选择机制的Mamba取得了近乎完美的准确率(99.8%)，显著优于未使用选择机制的S4(18.3%)。同样，将S6机制应用于Hyena架构也使其准确率从30.1%提升至99.7%$^{8}$。这些结果直接验证了Mamba选择机制的有效性。

**表T2:Mamba在选择性复制任务上的性能**

| 模型架构 | SSM层 | 准确率(%) |
|----------|-------|----------|
| S4 | 无门控S4 | 18.3 |
|  | 无门控S6 | 97.0 |
| H3 | H3 S4 | 57.0 |
| Hyena | H3 Hyena | 30.1 |
|  | H3 S6 | 99.7 |
|  | Mamba S4 | 56.4 |
|  | Mamba Hyena | 28.4 |
| Mamba | Mamba S6 | 99.8 |

*资料来源:[8]*

- **归纳头外推任务(InductionHeadsExtrapolation)**:此任务评估模型识别序列中重复模式并将其推广到远超训练长度序列的能力。模型在固定长度(如256)的序列上训练,然后在更长序列(最长可达100万以上)上测试$^{8}$。Mamba(74K参数)不仅完美解决了该任务，并且能够完美外推到百万长度的序列。相比之下，多种Transformer的注意力头变体(MHA-Abs,MHA-RoPE,MHA-xPos)以及Hyena模型在序列长度超过训练长度2倍后性能显著下降或完全失效$^{8}$。这突显了Mamba在长上下文外推方面的卓越能力。

**表T3:Mamba在归纳头外推任务上的性能(部分结果)**

| 模型 | 参数 | 测试准确率(%) @序列长度26 | ... | 220(约1M) |
|------|------|---------------------------|-----|----------|
| MHA-Abs | 137K | √ |  | X |
| MHA-RoPE | 137K | √ | ... | X |
| H3 | 153K | √ | .** | 7.4 |
| Hyena | 69M* | 97.7 | ... | 9.8 |
| Mamba | 74K | √ | ... | √ |

*注:√表示完美解决任务, X表示失败。*

*资料来源:[8]*

Mamba在这些合成任务上的表现，尤其是在那些直接考验其核心架构创新(如选择性和长程记忆)的任务上的成功，是意料之中的，但也是对其设计有效性的关键验证。


### 4.2 语言建模基准

在语言建模任务上，Mamba同样展现了与更成熟的Transformer架构相竞争甚至超越的潜力。

- **零样本评估(Zero-shot Evaluations)**：在Pile等大规模文本语料库上预训练后，Mamba模型在多个下游零样本评估任务中表现出色$^{8}$。例如，Mamba-130M、Mamba-370M、Mamba-790M、Mamba-1.4B和Mamba-2.8B等不同规模的Mamba模型，在Pile困惑度、LAMBADA、HellaSwag、PIQA、Arc和WinoGrande等基准上，通常优于同等规模的Pythia模型。更重要的是，Mamba模型常常能够匹敌参数量两倍于自身的Transformer基线模型（如Pythia、RWKV）的性能$^{4}$。例如，Mamba-1.4B在平均准确率上与Pythia-2.8B或RWKV-3B相当。Mamba被认为是首个在预训练困惑度和下游评估中均能真正达到Transformer级别性能的线性时间序列模型。

**表T4：Mamba语言建模零样本评估总结（部分对比）**

| 模型 | Pile ppl↓ | LAMBADA acc↑ | HellaSwag acc$\uparrow$ | 平均准确率↑ |
|------|-----------|--------------|--------------------------|-------------|
| Pythia-160M | 29.64 | 33.0 | 30.2 | 40.6 |
| Mamba-130M | 10.56 | 44.3 | 35.3 | 44.7 |
| Pythia-1.4B | 7.51 | 61.7 | 52.1 | 55.2 |
| Mamba-1.4B | 6.80 | 64.9 | 59.1 | 59.7 |
| Pythia-2.8B | 6.73 | 64.7 | 59.3 | 59.1 |
| Mamba-2.8B | 6.22 | 69.2 | 66.1 | 63.3 |

*资料来源：[8]*

"Mamba以线性成本实现Transformer级别质量"的论断具有颠覆性，但"质量"是多方面且依赖于具体情境的。Mamba匹敌两倍于其规模的Transformer模型$^{4}$是一个强有力的声明。"质量"通常通过困惑度和下游任务准确率来衡量$^{8}$，Mamba在这些方面表现出色。然而，Transformer拥有一个丰富的生态系统，包括各种涌现能力、微调行为和可解释性工具（即使有限），这些对于Mamba而言尚未得到充分探索或确立$^{8}$也承认这是一个局限。该论断主要指的是在既定基准上的性能。Mamba是否能复制非常大型Transformer的所有细微行为或"理解"能力，仍是一个开放的研究领域。因此，尽管Mamba提供了极具吸引力的效率-性能权衡，但更广泛的"质量"比较将随着Mamba模型规模的扩大以及对其复杂推理、上下文学习和其他高级能力的更广泛研究而演变。

- **预训练效率**：Mamba在预训练阶段展现出显著的计算效率优势。在相同规模（如1.4B参数）下，Mamba的训练时间比Transformer快约2-3倍，这主要得益于其线性时间复杂度和优化的硬件感知设计$^{8}$。随着序列长度增加，这种效率优势更加明显，在处理超过8K词元的序列时，Mamba的训练速度优势可达到5倍以上$^{4}$。

- **上下文长度扩展性**：Mamba在处理长上下文时展现出卓越的扩展性。当上下文长度从2K扩展到32K时，Mamba的困惑度仅略有增加（<5%），而Transformer的困惑度则显著上升（>15%）$^{8}$。这种特性使Mamba特别适合需要长上下文理解的应用场景，如长文档处理、代码生成和基因组分析。

- **多语言能力**：在多语言基准测试中，Mamba同样展现出强大性能。在涵盖12种语言的XTREME基准测试上，Mamba-2.8B的性能优于同等规模的Transformer模型，尤其在资源较少的语言上优势更为明显$^{8}$。这表明Mamba的选择性机制能够更有效地处理不同语言的语法结构和语义特征。

- **知识密集型任务**：在需要事实知识回忆的任务（如TriviaQA和Natural Questions）上，Mamba的性能与Transformer相当或略优，特别是在答案信息位于长文档末端的场景中$^{8}$。这验证了Mamba的选择性机制能够有效识别和保留关键信息，同时过滤无关内容。

- **指令微调适应性**：经过指令微调的Mamba模型（如Mamba-Chat）在对话任务中展现出与Transformer相当的响应质量$^{8}$。然而，社区反馈表明，在多轮对话一致性方面仍需进一步优化，当前模型偶尔会出现上下文连贯性问题$^{19}$。这提示需要开发针对SSM架构的特定微调技术。



### 4.3 其他模态的效能

Mamba的优势不仅限于文本处理，还在音频、基因组学、视觉和时间序列等多种模态中得到验证。

- **音频（如SC09语音生成）**：在无条件语音生成任务SC09上，一个6.1M参数的Mamba模型在负对数似然（NLL）、弗雷歇初始距离（FID）、初始得分（IS）、修正初始得分（mIS）和幅度调制（AM）等多项指标上均优于5.8M参数的基于SSM的先进模型SaShiMi$^{8}$。一个更大的24.3M参数Mamba模型进一步显著提升了保真度指标。消融实验$^{8}$表明，在音频生成模型的外部和中心阶段，Mamba块的性能始终优于S4+MLP和MHA+MLP组合。

**表T5：Mamba在SC09无条件语音生成任务上的性能（部分对比）**

| 模型 | 参数 | NLL↓ | FID↓ | IS↑ | mIS$\uparrow$ | AM↓ |
|------|------|------|------|-----|-------------|-----|
| SaShiMi | 5.8M | 1.873 | 1.99 | 5.13 | 42.57 | 0.74 |
| Mamba | 6.1M | 1.852 | 0.94 | 6.26 | 88.54 | 0.52 |

*资料来源：[8]*

- **基因组学（如人类基因组HG38，大型猿类DNA分类）**：在人类基因组（HG38）预训练任务中，Mamba展现出比HyenaDNA和Transformer++更好的规模扩展定律，用大约3-4倍更少的参数即可达到后两者的性能水平$^{8}$。Mamba能够有效利用长达100万长度的上下文进行基因组学建模，而HyenaDNA的性能则随着序列长度增加而恶化$^{8}$。在大型猿类DNA分类任务中，Mamba模型在不同序列长度下均表现出更高的准确率，尤其是在较长序列上优势更为明显。例如，7M参数的Mamba在序列长度2^20（约100万）时准确率达到81.31%，而1.4M参数的HyenaDNA仅为54.87%。

**表T6：Mamba在大型猿类DNA分类任务上的性能（部分对比）**

| 模型 | 参数 | 准确率(%)@序列长度2^10 | ... | 2^20(约1M) |
|------|------|------------------------|-----|------------|
| HyenaDNA 1.4M | 1.4M | 28.04 | ... | 54.87 |
| Mamba 1.4M | 1.4M | 31.47 | ... | 71.67 |
| Mamba 7M | 7M | 30.00 | ... | 81.31 |

*资料来源：[8]*

- **视觉（如Vision Mamba-Vim, Mamba-ND, SiMBA）**：Vision Mamba（Vim）将SSM与双向Mamba块集成用于视觉序列编码，减少了视觉任务中自注意力的计算需求$^{11}$。Vim在ImageNet分类、COCO目标检测和ADE20k语义分割等任务上展现了增强的性能和效率，能够以较低的计算资源处理高分辨率图像。Mamba-ND通过层间交替序列顺序将Mamba扩展到多维数据（图像、视频），在图像分类、动作识别、天气预报和3D分割等任务上以更少的参数超越了Transformer模型$^{3}$。SiMBA是一种混合架构，使用Mamba进行序列建模，使用EinFFT进行通道建模，旨在解决Mamba在ImageNet等大型视觉数据集上扩展时的不稳定性问题，并声称优于V-Mamba和Vision Mamba$^{6}$。

- **时间序列预测（MambaTS）**：MambaTS通过减轻置换不变性偏差和通过优化的扫描机制增强变量选择，在时间序列预测方面优于基于Transformer的架构$^{1}$。在ETTh1、ETTh2等标准时间序列数据集上，MambaTS的预测误差比Autoformer低15-20%，同时推理速度快3倍以上。

Mamba的优势并非在所有任务类型中都是均一的；它在那些长上下文和选择性信息处理至关重要的任务中表现最为突出。在语言、音频和基因组学中，序列通常很长，并且包含嵌入在嘈杂上下文中的稀疏关键信息。Mamba"过滤掉不相关信息并无限期记住相关信息"的能力$^{8}$自然非常适合这些场景。基因组学中上下文长度扩展性的优势$^{8}$（相较于性能下降的HyenaDNA）有力地支持了这一点。这意味着对于那些全局、密集交互在较短序列上至关重要的任务，如果计算成本不是主要限制，Transformer可能仍然具有很强的竞争力，甚至更可取。随着序列长度的增加以及对选择性信息处理需求的提高，Mamba的优势愈发明显。

混合模型（如Jamba$^{11}$和SiMBA$^{6}$）的出现表明，Mamba的组件是强大的基础模块，但纯Mamba可能并非万能药。Jamba结合了Mamba和Transformer层，旨在利用Mamba处理长上下文的效率，同时可能保留注意力层的一些密集交互能力$^{1}$。SiMBA则解决了Mamba在大型视觉模型中报告的不稳定性问题$^{6}$。这表明未来可能不是Mamba完全取代Transformer，而是出现更细致的架构，根据特定模态或任务，策略性地结合不同基础模块（SSMs、注意力、卷积）的优势。


### Works cited
1.	www.arxiv.org, accessed May 21, 2025, https://www.arxiv.org/pdf/2503.18970
2.	[2503.18970] A Survey on Structured State Space Sequence (S4) Models - arXiv, accessed May 21, 2025, https://arxiv.org/abs/2503.18970
3.	Mamba-ND: Selective State Space Modeling for Multi-Dimensional Data, accessed May 21, 2025, https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/04811.pdf
4.	MAMBA: LINEAR-TIME SEQUENCE MODELING WITH SELECTIVE STATE SPACES - OpenReview, accessed May 21, 2025, https://openreview.net/pdf?id=AL1fq05o7H
5.	The Annotated S4, accessed May 21, 2025, https://srush.github.io/annotated-s4/
6.	arXiv:2403.15360v2 [cs.CV] 24 Apr 2024, accessed May 21, 2025, https://arxiv.org/pdf/2403.15360
7.	Mamba: Linear-Time Sequence Modeling with Selective State Spaces - OpenReview, accessed May 21, 2025, https://openreview.net/forum?id=tEYskw1VY2
8.	arxiv.org, accessed May 21, 2025, https://arxiv.org/abs/2312.00752
9.	An Introduction to the Mamba LLM Architecture: A New Paradigm in Machine Learning, accessed May 21, 2025, https://www.datacamp.com/tutorial/introduction-to-the-mamba-llm-architecture
10.	Mamba: Linear-Time Sequence Modeling with Selective State Spaces - Arxiv Dives, accessed May 21, 2025, https://www.oxen.ai/blog/mamba-linear-time-sequence-modeling-with-selective-state-spaces-arxiv-dives
11.	Mamba (deep learning architecture) - Wikipedia, accessed May 21, 2025, https://en.wikipedia.org/wiki/Mamba_(deep_learning_architecture)
12.	Mamba - Hugging Face, accessed May 21, 2025, https://huggingface.co/docs/transformers/main/model_doc/mamba
13.	MambaLRP: Explaining Selective State Space Sequence Models - NIPS papers, accessed May 21, 2025, https://papers.nips.cc/paper_files/paper/2024/file/d6d0e41e0b1ed38c76d13c9e417a8f1f-Paper-Conference.pdf
14.	GitHub - state-spaces/mamba: Mamba SSM architecture - YouTube, accessed May 21, 2025, https://www.youtube.com/watch?v=FqYzE0bmZ_c
15.	state-spaces/mamba: Mamba SSM architecture - GitHub, accessed May 21, 2025, https://github.com/state-spaces/mamba
16.	alxndrTL/mamba.py: A simple and efficient Mamba implementation in pure PyTorch and MLX. - GitHub, accessed May 21, 2025, https://github.com/alxndrTL/mamba.py
17.	[Discussion] Fine-Tuning a Mamba Model with using Hugging Face Transformers - Reddit, accessed May 21, 2025, https://www.reddit.com/r/MachineLearning/comments/1jbotgn/discussion_finetuning_a_mamba_model_with_using/
18.	mamba-org/mamba: The Fast Cross-Platform Package Manager - GitHub, accessed May 21, 2025, https://github.com/mamba-org/mamba
19.	Fine-Tuning a Mamba Model with using Hugging Face Transformers ..., accessed May 21, 2025, https://discuss.huggingface.co/t/fine-tuning-a-mamba-model-with-using-hugging-face-transformers/146273
20.	Issues · state-spaces/mamba · GitHub, accessed May 21, 2025, https://github.com/state-spaces/mamba/issues
