# BLIP: 统一理解和生成的自举多模态模型

Author By: 李佳函

## 引言

### 核心挑战

在BLIP提出前，视觉-语言预训练领域已有CLIP、ALBEF等代表性工作，但仍存在两个关键瓶颈：

+ 问题一：**任务能力割裂**
  + **架构局限性**。没有一个模型既可以做检索又可以做生成。
    - **Encoder-only模型**（如CLIP）：  
    擅长图文检索等理解任务，但无法执行文本生成任务（如图像字幕生成）
    - **Encoder-Decoder模型**：  
    支持生成任务，但难以有效应用于图像-文本检索任务
  + **根本矛盾**：理解任务需深度跨模态交互，生成任务需自回归建模，传统架构无法同时优化两种目标。
+ 问题二：**数据噪声污染**。过往的这些多模态SOTA模型，都是在网络上爬取大量的数据进行预训练的。由于数据量够大，训练效果也差强人意。但这些噪声数据对于模型的训练来说是非常不利的，如果可以提取到更纯净的数据进行训练，模型的性能还可以进一步提升。

### BLIP的诞生

基于此问题现象，BLIP诞生了。BLIP的作者也是ALBEF的作者，因此可以在BLIP上看到很多ALBEF的影子。

该模型的核心思想主要有两个：

1. **论文提出核心架构——多模态混合编码器-解码器（Multimodal Mixture of Encoder-Decoder, MED）采用双流结构设计**，通过共享参数支持三种模式——单模态编码器（对齐全局特征）、跨模态编码器（细粒度匹配）和跨模态解码器（生成描述），联合优化图像-文本对比（ITC）、匹配（ITM）和语言建模（LM）损失，从而统一理解（如检索）与生成（如描述）任务。
2. **数据层面设计Captioning 和 Filtering（CapFilt）方法**，利用预训练（使用网上获取的有噪声的数据）的MED生成合成描述（Captioner模块）并过滤原始网络文本与合成文本中的噪声（Filter模块），从噪声数据中提炼高质量训练样本。

## 模型架构设计

### MED

MED全称为Multimodal Mixture of Encoder-Decoder，是一个可以完成三个任务的复合型模型。

![](./images/02BLIP01.png)

如图所示，总共有四个模块，从左往右，分别为两个单模态编码器：图像编码器和文本编码器，两个多模态编码器：基于图像的文本编码器和解码器。

#### 图像编码器（Unimodal Image Encoder）

+ 架构：基于Transformer的视觉编码器（ViT），将图像划分为固定大小的patch（如224×224分辨率下分割为14×14的196个patch），每个patch被线性嵌入为向量。
+ 关键设计：
  + 全局特征提取：在输入序列中添加一个特殊的[CLS]标记（Global Image Token），用于捕捉图像的整体语义信息。
  + 动量编码器：在对比学习（ITC任务）中，引入了ALBEF的做法即动量编码器（Momentum Encoder）生成软标签，增强负样本的鲁棒性。
+ 输出：生成图像的嵌入向量序列（包括[CLS]标记的特征），作为后续多模态模块的输入。
> 软标签即为概率值，不是像硬标签那样的非1即0。使用软标签可以让模型学习到更细粒度的语义信息，而不是简单地二值化判断。

![alt text](./images/02BLIP02.png)

#### 文本编码器（Unimodal Text Encoder）

+ 功能：将文本序列编码为上下文感知的特征表示。
+ 架构：基于BERT的Transformer编码器，对文本进行双向自注意力建模。
+ 关键设计：
  + 文本结构化：在文本开头插入[CLS]标记，用于聚合整个文本的全局信息。
  + 单模态对齐：通过图像-文本对比损失（ITC）与图像编码器的特征空间对齐。
+ 输出：生成文本的嵌入向量序列（包括[CLS]标记的特征），用于图像-文本匹配（ITM）或跨模态交互。
![alt text](./images/02BLIP03.png)


#### 基于图像的文本编码器（Image-Grounded Text Encoder）

+ 功能：融合图像信息与文本信息，生成多模态联合表示。
+ 架构：在标准文本编码器的基础上，插入交叉注意力层（Cross-Attention Layer）。
  + 结构细节：
    + 在每个Transformer块的自注意力层（Self-Attention, SA）和前馈网络（FFN）之间，增加一个交叉注意力层（Cross-Attention, CA）。
    + CA层的作用：以图像编码器的输出作为query，文本编码器的中间表示作为key和value，实现视觉信息向文本的注入。
  + 任务特定标记：在文本输入中插入一个任务专用的[Encode]标记，其最终输出作为多模态表示的核心。
+ 关键任务：
  + 图像-文本匹配（ITM）：通过二分类头部（ITM Head）判断图像-文本对是否匹配。
  + 多模态检索：生成跨模态嵌入向量，用于图像与文本的相似度计算。

![alt text](./images/02BLIP04.png)

#### 基于图像的文本解码器（Image-Grounded Text Decoder）

+ 功能：以图像为条件生成连贯的文本描述（如图像字幕）。
+ 架构：基于Transformer的解码器，采用因果掩码自注意力（Causal Self-Attention），即decoder只与之前出现的token进行attention操作。
  + 结构细节：
    + 替换标准解码器的双向自注意力为因果掩码自注意力，确保生成过程的顺序性（仅依赖历史和当前token）。
    + 跨模态注意力：在解码器中引入交叉注意力层，直接利用图像编码器的输出作为视觉上下文。
  + 任务特定标记：在文本生成时，使用[Decode]标记作为序列的起始信号。结尾需要添加一个[EOS]字符。
+ 关键任务：
  + 图像条件语言建模（LM）：通过最大化给定图像下文本序列的似然，训练模型生成与图像内容一致的描述。
  + 多任务生成：支持图像描述生成、视觉问答（VQA）等需要生成能力的任务。

![alt text](./images/02BLIP05.png)

### 预训练目标


| 损失函数            | 作用             | 对应模块 |
| :------------------ | :--------------- | :------- |
| ITC（图文对比损失） | 对齐图文特征空间 | 编码器   |
| ITM（图文匹配损失） | 细粒度图文匹配   | 编码器   |
| LM（语言建模损失）  | 生成连贯文本描述 | 解码器   |

## 数据自举(data Bootstrapping)：CapFilt

### 核心流程

BLIP模型通过创新的Bootstrapping自举机制实现了突破性进展。该机制采用三阶段迭代训练策略，构建了"预训练-数据清洗-迭代优化"的闭环系统：

+ 基础预训练阶段
  使用大规模图文对进行初步训练，建立图像与文本的跨模态表征能力。通过对比学习（Contrastive Learning）和掩码语言建模（MLM）任务，模型获得基础的特征提取能力。
+ 动态数据清洗阶段
  引入自监督的过滤机制，利用模型自身评估数据质量。通过计算图像-文本匹配得分（ITM Score），自动识别低质量样本并进行过滤。实验表明该过程可使训练数据纯净度提升40%以上。
+ 迭代优化阶段
  采用生成-评估-反馈的循环机制，通过Captioner生成伪标签数据，Filter验证数据质量，最终实现模型性能的持续提升。这种渐进式优化策略有效缓解了传统方法中负样本污染问题。

下面，会将BLIP的关键组件进行分析讲解Captioner&Filter。

### 关键组件

#### Captioner（生成器）

+ 功能：为图像生成高质量文本描述

+ 架构：基于Transformer的图像条件解码器

#### Filter（过滤器）

+ 功能：评估图文对匹配度

+ 技术：图文匹配（ITM）头部 + 对比学习



## 