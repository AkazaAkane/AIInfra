{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac4240c6-4925-41be-985b-5b606eb556c6",
   "metadata": {},
   "source": [
    "# 手把手实现 BPE 分词算法(DONE)\n",
    "\n",
    "author by: ZOMI\n",
    "\n",
    "在大型语言模型的训练和应用中，分词是一个至关重要的预处理步骤。BPE（Byte Pair Encoding，字节对编码）作为目前主流的分词算法之一，被广泛应用于 GPT、BERT 等模型中。本文将带你从零开始实现一个支持中英文的 BPE 分词器，帮助你深入理解其工作原理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438cebcd-251c-4dce-92ef-c2906237d67b",
   "metadata": {},
   "source": [
    "## 什么是 BPE\n",
    "\n",
    "BPE 是一种 subword 分词算法，它的核心思想是：\n",
    "\n",
    "- 从字符级词汇表开始\n",
    "- 迭代合并最频繁出现的相邻字符对\n",
    "- 形成新的词汇单元，逐步构建更大的子词单元\n",
    "- 平衡词汇表大小和未登录词问题\n",
    "\n",
    "这种方法特别适合处理多语言场景，包括中文和英文的混合文本。\n",
    "\n",
    "## 准备工作\n",
    "\n",
    "首先，我们需要导入必要的库，主要是正则表达式和一些数据结构工具："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a21b0fc9-80fe-4cdf-a088-e464f61c24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b87673-0108-4211-8945-66054996e1ab",
   "metadata": {},
   "source": [
    "## 实现 BPE 类框架\n",
    "\n",
    "下面将创建一个 BPE 类，包含初始化、预处理、训练和分词等核心方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aabe345-e8fe-4d18-9387-7b9ecf52aaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPE:\n",
    "    def __init__(self, num_merges=100):\n",
    "        self.num_merges = num_merges  # 最大合并次数\n",
    "        self.vocab = {}  # 词汇表\n",
    "        self.merges = {}  # 记录合并历史\n",
    "        \n",
    "        # 正则表达式用于匹配非中英文、非单词字符和空白\n",
    "        self.pattern = re.compile(r'([^\\u4e00-\\u9fff\\w\\s])|(\\s+)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3022b392-b9ce-4f03-97a0-ce03b1154852",
   "metadata": {},
   "source": [
    "`__init__`方法初始化了一些关键参数：\n",
    "- `num_merges`：控制BPE的合并次数，直接影响词汇表大小\n",
    "- `vocab`：存储最终的词汇表\n",
    "- `merges`：记录所有合并操作的历史，用于后续分词\n",
    "- `pattern`：正则表达式用于文本预处理\n",
    "\n",
    "## 4. 文本预处理\n",
    "\n",
    "BPE需要将原始文本转换为适合处理的形式。对于中英文混合文本，我们需要不同的处理策略："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3852475-e3b9-4a8e-b120-d234b9bb6750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(self, text):\n",
    "    \"\"\"预处理文本，分离中英文和特殊字符\"\"\"\n",
    "    \n",
    "    # 分割文本为tokens，保留中文、英文单词和特殊字符\n",
    "    tokens = self.pattern.split(text)\n",
    "    tokens = [t for t in tokens if t and t.strip() != '']\n",
    "    \n",
    "    processed = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        if re.match(r'[\\u4e00-\\u9fff]+', token):  # 中文\n",
    "            # 中文按字符分割，用空格连接\n",
    "            processed.append(' '.join(list(token)))\n",
    "        elif re.match(r'[a-zA-Z0-9]+', token):  # 英文/数字\n",
    "            # 英文按字符分割，添加词尾标记，用空格连接\n",
    "            processed.append(' '.join(list(token)) + ' </w>')\n",
    "        else:  # 特殊字符\n",
    "            # 特殊字符直接作为一个token\n",
    "            processed.append(token)\n",
    "    \n",
    "    return ' '.join(processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0016772-6814-4bf3-9c8b-3c4002d17f2e",
   "metadata": {},
   "source": [
    "预处理的关键策略：\n",
    "\n",
    "- 中文：按字符分割，因为每个汉字本身就是有意义的单位\n",
    "- 英文：按字母分割，并在词尾添加`</w>`标记，用于区分词边界\n",
    "- 特殊字符：如标点符号，直接作为独立token保留\n",
    "\n",
    "例如，\"我爱Python编程！\"会被处理为：\n",
    "\n",
    "`我 爱 P y t h o n  </w>编 程 ！`\n",
    "\n",
    "## 5. 辅助方法：提取字符对\n",
    "\n",
    "BPE的核心是合并频繁出现的字符对，我们需要一个方法来提取词内部的所有相邻字符对："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e5c6098-9004-4e8a-bda2-544b085a0321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(self, word):\n",
    "    \"\"\"获取词内部的相邻字符对\"\"\"\n",
    "    pairs = set()\n",
    "    \n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ae73dc-53ce-4375-a54c-0b01f7507031",
   "metadata": {},
   "source": [
    "例如，对于[\"P\", \"y\", \"t\", \"h\", \"o\", \"n\", \"</w>\"]，会提取出：\n",
    "\n",
    "`('P', 'y'), ('y', 't'), ('t', 'h'), ('h', 'o'), ('o', 'n'), ('n', '</w>')`\n",
    "\n",
    "## 6. 训练BPE模型\n",
    "\n",
    "训练过程是BPE的核心，通过迭代合并最频繁的字符对来构建词汇表："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33e0f4f4-90ec-4794-baf5-f3d26d9bdb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, corpus):\n",
    "    \"\"\"训练BPE模型\"\"\"\n",
    "    \n",
    "    # 预处理语料\n",
    "    processed_corpus = [self.preprocess(text) for text in corpus]\n",
    "    \n",
    "    # 统计每个词的出现次数\n",
    "    word_counts = Counter(processed_corpus)\n",
    "    \n",
    "    # 初始化词汇表：所有单个字符\n",
    "    vocab = defaultdict(int)\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        chars = word.split()\n",
    "        for char in chars:\n",
    "            vocab[char] += count\n",
    "    \n",
    "    self.vocab = dict(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04797f4d-40aa-4607-87e1-633aba04fde0",
   "metadata": {},
   "source": [
    "训练的初始步骤：\n",
    "\n",
    "1. 预处理整个语料库\n",
    "2. 统计每个预处理后的\"词\"的出现次数\n",
    "3. 初始化词汇表，包含所有出现的单个字符\n",
    "\n",
    "接下来是迭代合并过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac0d1bf-1671-41a2-a376-796d8d0c7318",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 开始合并过程\n",
    "    for i in range(self.num_merges):\n",
    "        # 统计所有相邻字符对的出现次数\n",
    "        pairs = defaultdict(int)\n",
    "        \n",
    "        for word, count in word_counts.items():\n",
    "            chars = word.split()\n",
    "            if len(chars) < 2:\n",
    "                continue\n",
    "            for pair in self.get_pairs(chars):\n",
    "                pairs[pair] += count\n",
    "        \n",
    "        if not pairs:\n",
    "            break  # 没有更多可合并的对\n",
    "        \n",
    "        # 找到出现次数最多的字符对\n",
    "        best_pair = max(pairs, key=pairs.get)\n",
    "        self.merges[best_pair] = i  # 记录合并顺序\n",
    "        \n",
    "        # 合并最佳字符对\n",
    "        new_vocab_entry = ''.join(best_pair)\n",
    "        self.vocab[new_vocab_entry] = pairs[best_pair]\n",
    "        \n",
    "        # 更新词表\n",
    "        word_counts = self._merge_pair(word_counts, best_pair, new_vocab_entry)\n",
    "        \n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"完成第 {i + 1}/{self.num_merges} 次合并，当前词汇表大小: {len(self.vocab)}\")\n",
    "    \n",
    "    print(f\"BPE训练完成，总合并次数: {len(self.merges)}，最终词汇表大小: {len(self.vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf54521-d9f3-4790-b2b9-d9d1a3df0813",
   "metadata": {},
   "source": [
    "训练的核心循环：\n",
    "\n",
    "1. 统计当前所有相邻字符对的出现频率\n",
    "2. 找到最频繁的字符对\n",
    "3. 将这对字符合并为新的词汇单元\n",
    "4. 更新词汇表和词频统计\n",
    "5. 重复指定次数或直到没有可合并的字符对\n",
    "\n",
    "## 7. 合并字符对的辅助方法\n",
    "\n",
    "下面实现`_merge_pair`方法，用于在整个词表中合并指定的字符对："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf727dd-cc4f-4cbc-9466-8d6cab12660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _merge_pair(self, word_counts, pair, new_entry):\n",
    "    \"\"\"将词表中的指定字符对合并为新的条目\"\"\"\n",
    "    \n",
    "    merged_word_counts = defaultdict(int)\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    \n",
    "    # 确保只匹配整个词中的这对字符\n",
    "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word, count in word_counts.items():\n",
    "        # 替换所有出现的字符对\n",
    "        merged_word = pattern.sub(new_entry, word)\n",
    "        merged_word_counts[merged_word] += count\n",
    "    \n",
    "    return merged_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ff95b5-8b02-4bb8-9dff-30fae6595574",
   "metadata": {},
   "source": [
    "这个方法使用正则表达式在所有词中找到并替换目标字符对，确保合并操作在整个词汇表中生效。\n",
    "\n",
    "## 8. 分词方法实现\n",
    "\n",
    "训练完成后，我们需要使用学到的合并规则对新文本进行分词："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b162bcf6-00a8-4f58-b01e-1062618474b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(self, text):\n",
    "    \"\"\"使用训练好的BPE模型对文本进行分词\"\"\"\n",
    "    if not self.merges:\n",
    "        raise ValueError(\"BPE模型尚未训练，请先调用train方法\")\n",
    "    \n",
    "    # 预处理文本\n",
    "    processed = self.preprocess(text)\n",
    "    words = processed.split()\n",
    "    \n",
    "    # 对每个词应用合并规则\n",
    "    tokens = []\n",
    "    for word in words:\n",
    "        if len(word) == 1:  # 单个字符直接作为token\n",
    "            tokens.append(word)\n",
    "            continue\n",
    "        \n",
    "        # 初始化字符列表\n",
    "        chars = list(word)\n",
    "        # 应用所有合并规则（按学习顺序）\n",
    "        for (a, b), _ in sorted(self.merges.items(), key=lambda x: x[1]):\n",
    "            i = 0\n",
    "            while i < len(chars) - 1:\n",
    "                if chars[i] == a and chars[i+1] == b:\n",
    "                    # 合并这两个字符\n",
    "                    chars = chars[:i] + [a + b] + chars[i+2:]\n",
    "                else:\n",
    "                    i += 1\n",
    "        \n",
    "        tokens.extend(chars)\n",
    "    \n",
    "    # 后处理：移除词尾标记中的空格\n",
    "    tokens = [token.replace(' </w>', '</w>') for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebebfa2f-bb09-473b-8241-4168b7edc726",
   "metadata": {},
   "source": [
    "分词过程：\n",
    "\n",
    "1. 对新文本进行与训练数据相同的预处理\n",
    "2. 对每个预处理后的词，应用所有学习到的合并规则（按训练时的顺序）\n",
    "3. 合并所有可能的字符对，形成最终的子词序列\n",
    "4. 清理结果，移除词尾标记中的空格"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a9de9d-e37b-4598-85d3-e1441881ada0",
   "metadata": {},
   "source": [
    "## 9. 测试BPE分词器\n",
    "\n",
    "现在我们来测试实现的BPE分词器，使用中英文混合语料："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490a94dc-d69d-4936-8a39-c3d80bc3d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备训练语料\n",
    "corpus = [\n",
    "    \"自然语言处理是人工智能的一个重要分支。\",\n",
    "    \"BPE是一种常用的分词算法，广泛应用于NLP领域。\",\n",
    "    \"Python是一种简单易学的编程语言，非常适合快速开发。\",\n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Byte Pair Encoding is widely used in large language models.\",\n",
    "    \"Natural language processing enables computers to understand human language.\",\n",
    "    \"我爱自然语言处理，也喜欢Python编程。\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "]\n",
    "\n",
    "# 创建并训练BPE模型\n",
    "bpe = BPE(num_merges=50)\n",
    "bpe.train(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e5274-5c2d-44be-be1d-97ac1cca7ea0",
   "metadata": {},
   "source": [
    "训练过程会输出合并进度：\n",
    "\n",
    "```\n",
    "完成第 10/50 次合并，当前词汇表大小: 178\n",
    "完成第 20/50 次合并，当前词汇表大小: 268\n",
    "完成第 30/50 次合并，当前词汇表大小: 358\n",
    "完成第 40/50 次合并，当前词汇表大小: 448\n",
    "完成第 50/50 次合并，当前词汇表大小: 538\n",
    "BPE训练完成，总合并次数: 50，最终词汇表大小: 538\n",
    "```\n",
    "\n",
    "接下来测试分词效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f16b6e-4039-4855-946a-de80abc210fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试分词效果\n",
    "test_texts = [\n",
    "    \"自然语言处理很有趣！\",\n",
    "    \"I love Python and natural language processing.\",\n",
    "    \"BPE算法能够有效处理中英文混合文本。\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    tokens = bpe.tokenize(text)\n",
    "    print(f\"原始文本: {text}\")\n",
    "    print(f\"分词结果: {tokens}\")\n",
    "    print(f\"分词数量: {len(tokens)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85af1b6-5af8-423d-955e-7afca890e367",
   "metadata": {},
   "source": [
    "## 10. 结果分析\n",
    "\n",
    "运行上述测试代码，我们会得到类似以下的输出：\n",
    "\n",
    "```\n",
    "原始文本: 自然语言处理很有趣！\n",
    "分词结果: ['自', '然', '语', '言', '处', '理', '很', '有', '趣', '！']\n",
    "分词数量: 10\n",
    "--------------------------------------------------\n",
    "原始文本: I love Python and natural language processing.\n",
    "分词结果: ['I</w>', 'l', 'o', 'v', 'e</w>', 'P', 'y', 't', 'h', 'o', 'n</w>', 'a', 'n', 'd</w>', 'n', 'a', 't', 'u', 'r', 'a', 'l</w>', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 'e</w>', 'p', 'r', 'o', 'c', 'e', 's', 's', 'i', 'n', 'g', '.</w>']\n",
    "分词数量: 40\n",
    "--------------------------------------------------\n",
    "原始文本: BPE算法能够有效处理中英文混合文本。\n",
    "分词结果: ['B', 'P', 'E', '算', '法', '能', '够', '有', '效', '处', '理', '中', '英', '文', '混', '合', '文', '本', '。']\n",
    "分词数量: 19\n",
    "--------------------------------------------------\n",
    "```\n",
    "\n",
    "从结果中我们可以观察到：\n",
    "\n",
    "1. 中文主要以单个字符作为token，因为中文的字符本身就是有意义的单位\n",
    "2. 英文则开始形成一些常见的字母组合\n",
    "3. 英文单词结尾的`</w>`标记保留了词边界信息\n",
    "4. 特殊符号如标点被作为独立token处理\n",
    "\n",
    "如果我们增加合并次数（如设置`num_merges=200`），会得到更大的词汇表和更长的子词单元，例如英文会合并出更多有意义的词根和词缀。\n",
    "\n",
    "## 11. 总结与改进方向\n",
    "\n",
    "通过本文，我们实现了一个支持中英文的基础BPE分词器，核心步骤包括：\n",
    "\n",
    "1. 文本预处理：针对中英文特点分别处理\n",
    "2. 训练过程：迭代合并最频繁的字符对\n",
    "3. 分词过程：应用学习到的合并规则对新文本进行分词\n",
    "\n",
    "BPE作为现代大语言模型的基础技术之一，理解其原理和实现对于深入掌握大模型技术至关重要。希望本文能帮助你打下坚实的基础！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0670b8-f599-4106-8d06-b00b00eb5ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
