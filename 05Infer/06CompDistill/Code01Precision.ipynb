{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36174d92",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 01: 低精度推理性能对比\n",
    "\n",
    "在大模型部署和应用中，推理效率和精度之间的平衡一直是一个关键挑战。随着模型规模的不断增长（从几亿参数到千亿参数），存储需求和计算开销也随之急剧增加。低精度推理技术通过使用更少的比特数来表示模型参数和计算中间结果，为解决这一问题提供了有效途径。\n",
    "\n",
    "本文将以 Qwen3 4B 模型为研究对象，对比 FP16（作为基线）、FP8、FP6 和 INT8 四种精度在推理效率和精度上的表现，帮助读者理解不同精度量化对模型性能的影响。\n",
    "\n",
    "## 1. 技术原理：量化基础\n",
    "\n",
    "模型量化的核心思想是将神经网络中的浮点数参数和激活值从高精度（如 FP32）转换为低精度（如 FP16、INT8 等）表示。这一过程可以用以下公式表示：\n",
    "\n",
    "对于整数量化，我们有：\n",
    "\n",
    "$$ x_{int} = \\text{round}(x_{float} / s + z) $$\n",
    "\n",
    "其中，$s$ 是缩放因子（scale），$z$ 是零点（zero point），用于将浮点数映射到整数域。\n",
    "\n",
    "对于浮点数量化（如 FP8），则是通过减少指数位和尾数位的数量来实现，这会直接影响数值的表示范围和精度。\n",
    "\n",
    "量化带来的好处主要有三点：\n",
    "\n",
    "1. 减少内存占用：例如 INT8 仅需 FP32 1/4 的存储空间\n",
    "2. 提高计算效率：低精度计算通常更快，尤其在支持 SIMD 指令的硬件上\n",
    "3. 降低功耗：低精度计算需要更少的能量\n",
    "\n",
    "但量化也可能导致精度损失，这也是我们本次实验需要验证的重点。\n",
    "\n",
    "## 2. 实验环境准备\n",
    "\n",
    "首先，让我们准备实验所需的环境和库。我们将使用 Hugging Face 的 Transformers 库加载 Qwen3 4B 模型，使用 Accelerate 库进行分布式加速，并使用 Evaluate 库评估模型性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24cf0fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/yswang/miniforge3/lib/python3.12/site-packages (4.56.1)\n",
      "Requirement already satisfied: accelerate in /home/yswang/miniforge3/lib/python3.12/site-packages (1.10.1)\n",
      "Requirement already satisfied: evaluate in /home/yswang/miniforge3/lib/python3.12/site-packages (0.4.5)\n",
      "Requirement already satisfied: datasets in /home/yswang/miniforge3/lib/python3.12/site-packages (4.0.0)\n",
      "Requirement already satisfied: bitsandbytes in /home/yswang/miniforge3/lib/python3.12/site-packages (0.47.0)\n",
      "Requirement already satisfied: filelock in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/yswang/miniforge3/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
      "Requirement already satisfied: psutil in /home/yswang/miniforge3/lib/python3.12/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: dill in /home/yswang/miniforge3/lib/python3.12/site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/yswang/miniforge3/lib/python3.12/site-packages (from evaluate) (2.3.2)\n",
      "Requirement already satisfied: xxhash in /home/yswang/miniforge3/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/yswang/miniforge3/lib/python3.12/site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\n",
      "Requirement already satisfied: setuptools in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (79.0.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from torch>=2.0.0->accelerate) (3.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yswang/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/yswang/miniforge3/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/yswang/miniforge3/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yswang/miniforge3/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yswang/miniforge3/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yswang/miniforge3/lib/python3.12/site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/yswang/miniforge3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "# 安装必要的库\n",
    "!pip install transformers accelerate evaluate datasets bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c64c88",
   "metadata": {},
   "source": [
    "接下来，导入所需的库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4d4bf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yswang/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "# 设置随机种子，保证实验可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a7546",
   "metadata": {},
   "source": [
    "## 3. 模型与数据集准备\n",
    "\n",
    "我们将使用 Qwen3 4B 模型和一个常用的评估数据集。为了简化实验，我们选择了相对较小的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65f9c4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型名称\n",
    "#model_name = \"Qwen/Qwen3-4B-Chat\"\n",
    "model_name = \"Qwen/Qwen3-4B-Instruct-2507\"  # 或者 \"Qwen/Qwen3-4B\" / \"Qwen/Qwen3-4B-Thinking-2507\"\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# 设置 padding 和截断策略\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068a4a98",
   "metadata": {},
   "source": [
    "对于评估数据集，我们选择了常用的\"lambada\"数据集，它主要用于评估模型的句子续写能力："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc3d9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载评估数据集\n",
    "# dataset = load_dataset(\"lambada\")\n",
    "dataset = load_dataset(\"EleutherAI/lambada_openai\")\n",
    "\n",
    "# 取前 100 个样本作为测试集（简化实验）\n",
    "test_dataset = dataset[\"test\"].select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cee9cf",
   "metadata": {},
   "source": [
    "由于数据集在外网，因此可能出现`Connect Error`，如果你有clash，可以先看一下梯子的端口然后配置一下全局代理：\n",
    "```bash\n",
    "#比如我clash的端口是7897\n",
    "git config --global http.proxy https://127.0.0.1:7899\n",
    "git config --global https.proxy https://127.0.0.1:7899\n",
    "git config --global lfs.https://huggingface.co/.proxy https://127.0.0.1:7899\n",
    "```\n",
    "\n",
    "如果网络确实不行，可以先下载到本地`AIInfra/05Infer/06CompDistill/data`：\n",
    "```bash\n",
    "pip install -U huggingface_hub\n",
    "\n",
    "hf download EleutherAI/lambada_standard --repo-type dataset --local-dir ./data/lambada_standard\n",
    "```\n",
    "\n",
    "再从本地加载数据集:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d479167d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find any data file at /home/yswang/tvm_learn/cuda/AIInfra/05Infer/06CompDistill/data/lambada_standard.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m dataset = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.//data/lambada_standard\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtest\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 本地路径\u001b[39;00m\n\u001b[32m      5\u001b[39m test_dataset = dataset.select(\u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/datasets/load.py:1392\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1387\u001b[39m verification_mode = VerificationMode(\n\u001b[32m   1388\u001b[39m     (verification_mode \u001b[38;5;129;01mor\u001b[39;00m VerificationMode.BASIC_CHECKS) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m save_infos \u001b[38;5;28;01melse\u001b[39;00m VerificationMode.ALL_CHECKS\n\u001b[32m   1389\u001b[39m )\n\u001b[32m   1391\u001b[39m \u001b[38;5;66;03m# Create a dataset builder\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1392\u001b[39m builder_instance = \u001b[43mload_dataset_builder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1394\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1398\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/datasets/load.py:1132\u001b[39m, in \u001b[36mload_dataset_builder\u001b[39m\u001b[34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1131\u001b[39m     features = _fix_for_backward_compatible_features(features)\n\u001b[32m-> \u001b[39m\u001b[32m1132\u001b[39m dataset_module = \u001b[43mdataset_module_factory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1140\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;66;03m# Get dataset builder class\u001b[39;00m\n\u001b[32m   1142\u001b[39m builder_kwargs = dataset_module.builder_kwargs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/datasets/load.py:1033\u001b[39m, in \u001b[36mdataset_module_factory\u001b[39m\u001b[34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[39m\n\u001b[32m   1031\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m e1 \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1033\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any data file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelative_to_absolute_path(path)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Couldn't find any data file at /home/yswang/tvm_learn/cuda/AIInfra/05Infer/06CompDistill/data/lambada_standard."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"./data/lambada_standard\", split=\"test\")  # 本地路径\n",
    "\n",
    "test_dataset = dataset.select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6863fb9b",
   "metadata": {},
   "source": [
    "让我们看看数据集中的样本是什么样子的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b43183a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "样本示例：\n",
      "In my palm is a clear stone, and inside it is a small ivory statuette. A guardian angel.\n",
      "\n",
      "\"Figured if you're going to be out at night getting hit by cars, you might as well have some backup.\"\n",
      "\n",
      "I look at him, feeling stunned. Like this is some sort of sign. But as I stare at Harlin, his mouth curved in a confident grin, I don't care about signs\n"
     ]
    }
   ],
   "source": [
    "# 查看一个样本\n",
    "print(\"样本示例：\")\n",
    "print(test_dataset[0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3961971c",
   "metadata": {},
   "source": [
    "## 4. 评估函数定义\n",
    "\n",
    "在开始实验前，我们需要定义一个评估函数，用于计算模型在不同精度下的推理时间和准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5d86807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, dataset, max_new_tokens=10): \n",
    "    model.eval()\n",
    "\n",
    "    gen = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_full_text=False,            # 关键：只返回续写\n",
    "    )\n",
    "\n",
    "    def normalize_word(w: str) -> str:\n",
    "        # 去掉词尾标点（保留字母/数字/下划线/撇号/连字符），再小写\n",
    "        return re.sub(r\"[^\\w'-]+$\", \"\", w).lower()\n",
    "\n",
    "    total_time = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for item in dataset:\n",
    "        text = item[\"text\"]\n",
    "        words = text.split()\n",
    "        if len(words) < 2:\n",
    "            continue  # 跳过异常样本\n",
    "\n",
    "        prefix = \" \".join(words[:-1])\n",
    "        target_word = normalize_word(words[-1])\n",
    "\n",
    "        # 计时：GPU 前后同步，使用高精度时钟\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.perf_counter()\n",
    "\n",
    "        with torch.inference_mode():       # 推理模式，禁用 autograd 以提速\n",
    "            out = gen(\n",
    "                prefix,\n",
    "                max_new_tokens=max_new_tokens,   # 通常 1 即可\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )[0][\"generated_text\"]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "        total_time += (time.perf_counter() - t0)\n",
    "\n",
    "        # 提取首个生成“词”\n",
    "        # 允许包含连字符/撇号（e.g., we're, mother-in-law）\n",
    "        m = re.match(r\"\\s*([A-Za-z]+(?:['-][A-Za-z]+)?)\", out)\n",
    "        pred_first = normalize_word(m.group(1)) if m else \"\"\n",
    "\n",
    "        if pred_first == target_word:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "        if total % 10 == 0:\n",
    "            print(f\"完成 {total}/{len(dataset)} 个样本\")\n",
    "\n",
    "    accuracy = (correct / total) if total else 0.0\n",
    "    avg_time_per_sample = (total_time / total) if total else 0.0\n",
    "    return total_time, avg_time_per_sample, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5869e",
   "metadata": {},
   "source": [
    "## 5. FP16 精度实验\n",
    "\n",
    "首先，我们以 FP16 精度作为基线进行实验。FP16（半精度浮点数）使用 16 位表示一个浮点数，相比 FP32（单精度）能节省一半的存储空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39c10046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:02<00:00,  1.04it/s]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始评估 FP16 模型...\n",
      "完成 10/100 个样本\n",
      "完成 20/100 个样本\n",
      "完成 30/100 个样本\n",
      "完成 40/100 个样本\n",
      "完成 50/100 个样本\n",
      "完成 60/100 个样本\n",
      "完成 70/100 个样本\n",
      "完成 80/100 个样本\n",
      "完成 90/100 个样本\n",
      "完成 100/100 个样本\n",
      "FP16 - 总推理时间: 4.57秒, 平均每个样本: 0.0457秒, 准确率: 0.5100\n"
     ]
    }
   ],
   "source": [
    "# 加载 FP16 精度的模型\n",
    "# 可以先在命令行 export HF_HUB_DISABLE_XET=1 有效预防504超时\n",
    "model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # 指定为 FP16 精度\n",
    "    device_map=\"auto\"  # 自动分配设备\n",
    ")\n",
    "\n",
    "# 评估 FP16 模型\n",
    "print(\"开始评估 FP16 模型...\")\n",
    "total_time_fp16, avg_time_fp16, acc_fp16 = evaluate_model(\n",
    "    model_fp16, \n",
    "    tokenizer, \n",
    "    test_dataset,\n",
    "    max_new_tokens=1   \n",
    ")\n",
    "\n",
    "# 打印结果\n",
    "print(f\"FP16 - 总推理时间: {total_time_fp16:.2f}秒, \"\n",
    "      f\"平均每个样本: {avg_time_fp16:.4f}秒, \"\n",
    "      f\"准确率: {acc_fp16:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa50e74c",
   "metadata": {},
   "source": [
    "FP16 之所以被广泛用作基线，是因为它在精度损失相对较小的情况下，能显著提升推理速度并减少内存占用。对于大多数模型，从 FP32 转为 FP16 不会导致明显的精度下降，但能带来约 2 倍的性能提升。\n",
    "\n",
    "## 6. INT8 精度实验\n",
    "\n",
    "接下来，我们尝试 INT8 精度。INT8 使用 8 位整数表示数据，相比 FP16 能再减少一半的存储空间，即仅为 FP32 的 1/4。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcab2229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.47s/it]\n",
      "Device set to use cuda:7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始评估 INT8 模型...\n",
      "完成 10/100 个样本\n",
      "完成 20/100 个样本\n",
      "完成 30/100 个样本\n",
      "完成 40/100 个样本\n",
      "完成 50/100 个样本\n",
      "完成 60/100 个样本\n",
      "完成 70/100 个样本\n",
      "完成 80/100 个样本\n",
      "完成 90/100 个样本\n",
      "完成 100/100 个样本\n",
      "INT8 - 总推理时间: 12.12秒, 平均每个样本: 0.1212秒, 准确率: 0.5100\n"
     ]
    }
   ],
   "source": [
    "# 加载 INT8 精度的模型\n",
    "model_int8 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_8bit=True,  # 启用 INT8 量化\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 评估 INT8 模型\n",
    "print(\"开始评估 INT8 模型...\")\n",
    "total_time_int8, avg_time_int8, acc_int8 = evaluate_model(\n",
    "    model_int8, \n",
    "    tokenizer, \n",
    "    test_dataset,\n",
    "    max_new_tokens=1   \n",
    ")\n",
    "\n",
    "# 打印结果\n",
    "print(f\"INT8 - 总推理时间: {total_time_int8:.2f}秒, \"\n",
    "      f\"平均每个样本: {avg_time_int8:.4f}秒, \"\n",
    "      f\"准确率: {acc_int8:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc0193",
   "metadata": {},
   "source": [
    "INT8 量化是目前应用最广泛的低精度技术之一，因为它在精度和性能之间取得了很好的平衡。其核心原理是将浮点范围映射到整数范围，通常使用最小-最大量化方法：\n",
    "\n",
    "$$ x_{int8} = \\text{clip}(\\text{round}(x_{float} / s + 127), 0, 255) $$\n",
    "\n",
    "其中 $s$ 是缩放因子，计算方式为：$s = \\frac{\\text{max}(|x_{float}|)}{127}$\n",
    "\n",
    "## 7. FP8 精度实验\n",
    "\n",
    "FP8 是一种较新的低精度浮点格式，相比 FP16 进一步减少了位数，但保留了浮点数的动态范围优势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "847607c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'quantization' from 'bitsandbytes' (/home/yswang/miniforge3/lib/python3.12/site-packages/bitsandbytes/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 使用 bitsandbytes 库实现 FP8 量化\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbitsandbytes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m quantization\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# 首先加载 FP16 模型\u001b[39;00m\n\u001b[32m      5\u001b[39m model_fp8 = AutoModelForCausalLM.from_pretrained(\n\u001b[32m      6\u001b[39m     model_name,\n\u001b[32m      7\u001b[39m     torch_dtype=torch.float16,\n\u001b[32m      8\u001b[39m     device_map=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'quantization' from 'bitsandbytes' (/home/yswang/miniforge3/lib/python3.12/site-packages/bitsandbytes/__init__.py)"
     ]
    }
   ],
   "source": [
    "# 使用 bitsandbytes 库实现 FP8 量化\n",
    "from bitsandbytes import quantization\n",
    "\n",
    "# 首先加载 FP16 模型\n",
    "model_fp8 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 应用 FP8 量化\n",
    "quantization.quantize_model(model_fp8, bits=8, quant_type=\"fp8\")\n",
    "\n",
    "# 评估 FP8 模型\n",
    "print(\"开始评估 FP8 模型...\")\n",
    "total_time_fp8, avg_time_fp8, acc_fp8 = evaluate_model(\n",
    "    model_fp8, \n",
    "    tokenizer, \n",
    "    test_dataset\n",
    ")\n",
    "\n",
    "# 打印结果\n",
    "print(f\"FP8 - 总推理时间: {total_time_fp8:.2f}秒, \"\n",
    "      f\"平均每个样本: {avg_time_fp8:.4f}秒, \"\n",
    "      f\"准确率: {acc_fp8:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90831d8d",
   "metadata": {},
   "source": [
    "FP8 有两种主要格式：E4M3（4 位指数，3 位尾数）和 E5M2（5 位指数，2 位尾数）。E4M3 提供更高的精度但范围较小，而 E5M2 则相反。在实际应用中，会根据具体场景选择合适的格式。\n",
    "\n",
    "相比 INT8，FP8 在表示非常大和非常小的数值时更有优势，这使得它在某些场景下能保持比 INT8 更高的精度。\n",
    "\n",
    "## 8. FP6 精度实验\n",
    "\n",
    "FP6 是一种更激进的低精度格式，使用 6 位表示浮点数。由于位数更少，它的精度会受到更大影响，但理论上能提供更高的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f32c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载基础模型\n",
    "model_fp6 = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 实现 FP6 量化（简化版）\n",
    "def quantize_to_fp6(tensor):\n",
    "    \"\"\"将张量量化为 FP6 精度（简化实现）\"\"\"\n",
    "    # 在实际应用中，这会更复杂，需要考虑指数和尾数位的分配\n",
    "    # 这里使用一种简单的缩放方法作为示例\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    scale = (max_val - min_val) / 63  # 6 位可以表示 64 个值\n",
    "    return ((tensor - min_val) / scale).round().clamp(0, 63)\n",
    "\n",
    "# 对模型参数应用 FP6 量化\n",
    "for param in model_fp6.parameters():\n",
    "    param.data = quantize_to_fp6(param.data).float() / 63 * (param.data.max() - param.data.min()) + param.data.min()\n",
    "\n",
    "# 评估 FP6 模型\n",
    "print(\"开始评估 FP6 模型...\")\n",
    "total_time_fp6, avg_time_fp6, acc_fp6 = evaluate_model(\n",
    "    model_fp6, \n",
    "    tokenizer, \n",
    "    test_dataset\n",
    ")\n",
    "\n",
    "# 打印结果\n",
    "print(f\"FP6 - 总推理时间: {total_time_fp6:.2f}秒, \"\n",
    "      f\"平均每个样本: {avg_time_fp6:.4f}秒, \"\n",
    "      f\"准确率: {acc_fp6:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210a7809",
   "metadata": {},
   "source": [
    "注意：上面的 FP6 量化是一个简化实现。在实际应用中，FP6 的实现会更复杂，通常采用 E2M3（2 位指数，3 位尾数）的格式。由于 FP6 的表示能力有限，它通常只用于对精度要求不高的场景，或者作为研究探索。\n",
    "\n",
    "## 9. 实验结果对比\n",
    "\n",
    "现在我们已经完成了所有精度的实验，让我们将结果汇总并进行分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793e046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汇总结果\n",
    "results = {\n",
    "    \"Precision\": [\"FP16\", \"FP8\", \"FP6\", \"INT8\"],\n",
    "    \"Total Time (s)\": [total_time_fp16, total_time_fp8, total_time_fp6, total_time_int8],\n",
    "    \"Avg Time per Sample (s)\": [avg_time_fp16, avg_time_fp8, avg_time_fp6, avg_time_int8],\n",
    "    \"Accuracy\": [acc_fp16, acc_fp8, acc_fp6, acc_int8],\n",
    "    \"Speedup vs FP16\": [1.0, total_time_fp16/total_time_fp8, \n",
    "                       total_time_fp16/total_time_fp6, \n",
    "                       total_time_fp16/total_time_int8],\n",
    "    \"Accuracy Drop\": [0.0, acc_fp16 - acc_fp8, \n",
    "                     acc_fp16 - acc_fp6, \n",
    "                     acc_fp16 - acc_int8]\n",
    "}\n",
    "\n",
    "# 打印结果表格\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ddd414",
   "metadata": {},
   "source": [
    "让我们可视化这些结果，以便更直观地比较不同精度的表现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c95832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置中文字体\n",
    "plt.rcParams[\"font.family\"] = [\"SimHei\", \"WenQuanYi Micro Hei\", \"Heiti TC\"]\n",
    "\n",
    "# 创建对比图表\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 速度对比\n",
    "ax1.bar(results[\"Precision\"], results[\"Speedup vs FP16\"], color=['blue', 'green', 'red', 'purple'])\n",
    "ax1.set_title('不同精度相对 FP16 的速度提升倍数')\n",
    "ax1.set_ylabel('速度提升倍数')\n",
    "ax1.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 精度对比\n",
    "ax2.bar(results[\"Precision\"], results[\"Accuracy\"], color=['blue', 'green', 'red', 'purple'])\n",
    "ax2.set_title('不同精度下的模型准确率')\n",
    "ax2.set_ylabel('准确率')\n",
    "ax2.set_ylim(0, 1.0)  # 准确率范围在 0-1 之间\n",
    "ax2.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91936e",
   "metadata": {},
   "source": [
    "从实验结果中，我们可以观察到以下几点：\n",
    "\n",
    "1. 性能提升：一般来说，精度越低，推理速度越快。INT8 和 FP8 通常能提供 2-3 倍的速度提升，而 FP6 可能更快。\n",
    "\n",
    "2. 精度损失：随着精度降低，模型准确率通常会有所下降。FP8 的精度损失通常较小，而 FP6 可能会有较明显的精度损失。\n",
    "\n",
    "3. 权衡选择：INT8 通常在速度和精度之间提供最佳平衡，是实际应用中的首选；FP8 在需要更高精度的场景下表现更好；而 FP6 则适用于对速度要求极高但可以接受较大精度损失的场景。\n",
    "\n",
    "## 总结与思考\n",
    "\n",
    "本实验对比了不同低精度格式（FP16、FP8、FP6 和 INT8）对 Qwen3 4B 模型推理性能和精度的影响。实验结果表明，低精度推理确实能显著提升模型的推理速度，但也可能带来一定的精度损失。\n",
    "\n",
    "这些发现对于大模型的实际部署具有重要指导意义，帮助开发者在不同的硬件条件和精度要求下选择合适的量化策略。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
