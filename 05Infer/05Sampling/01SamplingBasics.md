# 大模型采样基础 

Author by: 杨汝琦

在自然语言处理任务中，生成模型通常用于文本生成、机器翻译等任务。为了使模型能够生成符合上下文且连贯的文本，我们需要对生成过程中的输出进行采样。采样策略决定了模型每次生成的词或标记，并直接影响生成文本的质量与多样性。

<p align="center">
  <img src="./images/01SamplingBasics01.png">
</p>

大多数大语言模型以自回归方式运行，这意味着它们根据前面的词元序列来预测下一个词元的概率分布。如图片所示，AI enhances our learning来预测后面的一个词元experience，这种自回归特性使模型能够捕捉复杂的语言模式和依赖关系。从数学角度来看，这个过程可以表示为：

$$
\mathit{P(w_n|w_1,w_2,\ldots,w_{n-1})}
$$

在文本生成过程中，大语言模型通过采样算法确定下一个输出词元。这个过程可以采用不同的策略。

<p align="center">
  <img src="./images/01SamplingBasics02.png">
</p>

本篇文档将介绍三种常见的输出采样方法：随机采样、贪婪采样和束搜索采样，并从简单到复杂逐步深入，帮助大家更好地理解这些采样策略。

## 1. 随机采样 (Random Sampling)
### 原理
随机采样是基于采样的采样中最基本的形式，它从概率分布中随机选择下一个单词，而不是选择最有可能的单词，随机采样可能会选中一个罕见词。在生成过程中，模型会计算出每个候选词的概率分布（通常由 softmax 函数输出）。然后，根据这个概率分布随机选择一个词作为下一个输出。

随机采样的优点是简单直观，适合生成多样化的输出，避免生成重复或单调的文本。但纯粹的随机采样方法在概率分布的长尾部分存在显著缺陷，这也是LLM质量变差的区域。长尾指的是概率分布中模型为大量可能单词分配低概率的区域。虽然随机采样可以带来更多样的输出，但它也增加了从长尾中选择单词的可能性，这可能会导致文本毫无意义、语法错误或不连贯。

<p align="center">
  <img src="./images/01SamplingBasics03.png">
</p>

### 示例
当用户输入 "I have a dream of being a" 这句话时，LLM 会根据其训练数据和模型内部的逻辑，预测下一个最有可能出现的词。它会为每个可能的词（如 Engineer, Doctor, Lawyer 等）分配一个概率值。在图中，这些概率值分别为 0.32、0.30、0.28、0.09 和 0.01。

 随机采样并不是简单地选择概率最高的词（在这个例子中是 Engineer），而是将所有可能的词语及其对应的概率值看作一个**概率分布**。它会根据这个分布，随机地选择一个词作为输出。这意味着即使 "Engineer" 的概率最高，模型依然有可能选择概率较低的词，比如 "Alien"，从而产生一个新颖甚至出人意料的输出。这种方法可以增加文本的多样性和创造性，避免生成重复、死板的句子。

## 2. 贪婪采样 (Greedy Sampling)
### 原理
贪婪采样是一种贪心策略，目的是每一步选择最有可能的词。与随机采样不同，贪婪采样并不会从概率分布中随机选择，而是直接选择概率最高的词。在每一步生成过程中，模型计算出当前候选词的概率分布，然后选择概率最大的词作为输出。

贪婪采样的优点在于能够保证每一步都生成最可能的词，从而使生成的文本通常较为连贯且符合语法规则。然而，由于它总是选择最有可能的单词，这种方法容易陷入局部最优，忽略了隐藏在低概率词之后的高概率词，从而可能错过全局更优的序列。此外，这种策略也常常导致生成的文本缺乏多样性，显得重复且单调，因为它未能充分探索其他可能产生更好结果的替代路径。

下面是一个贪婪采样导致文本重复单调的示例：

+ 输入：“My favorite color is”（我最喜欢的颜色是）
+ 贪婪采样输出：“My favorite color is blue blue blue blue is blue and blue is my favorite color blue”（我最喜欢的颜色是蓝色蓝色蓝色蓝色，蓝色是蓝色，蓝色是我最喜欢的蓝色）

这个例子展示了贪婪采样是如何导致单调且不自然的文本的，多次重复“blue”这个单词，却没有添加任何有意义的信息。

<p align="center">
  <img src="./images/01SamplingBasics04.png">
</p>

### 示例
当用户输入 "I have a dream of being a" 这句话时，LLM 会根据其训练数据和模型内部的逻辑，预测下一个最有可能出现的词。它会为每个可能的词（如 Engineer, Doctor, Lawyer 等）分配一个概率值。在图中，这些概率值分别为 0.32、0.30、0.28、0.09 和 0.01。

贪心采样做的就是每次都选择概率最高的那个词作为输出，然后将这个词作为输入的一部分，继续生成下一个词。就像图片中所示，在给定的例子中，“Engineer”的概率最高（0.32），所以模型直接选择了它作为最终的输出。这种方法简单高效，但在某些情况下可能会因为它只考虑当前最优解而忽略了全局最优解，导致生成的文本质量不够自然或缺乏多样性。

## 3. 束搜索采样 (Beam Search)
### 原理
束搜索（beam search）是贪心搜索的一个改进版本，介于穷举搜索与贪心搜索。束搜索通过在每个时间步保留最可能的 num_beams 个词(即束宽)，并从中最终选择出概率最高的序列来降低丢失潜在的高概率序列的风险。

1. **初始化**：从初始词开始，保留多个候选序列（束宽度为 Beam Width），这些序列是根据每个词的概率排序得到的。
2. **扩展**：每次生成一个词后，模型会根据当前候选序列的概率分布选择多个最有可能的词，并将这些词扩展到已有的序列上。
3. **剪枝**：每次扩展后，只保留概率最高的`Beam Width`个序列，丢弃其他低概率序列。
4. **重复**：该过程持续进行，直到生成结束符或者达到最大生成长度。

束搜索相比贪婪采样可以避免局部最优解的困境，通过探索多个可能的输出序列，能生成更具多样性且较为合理的文本。但是计算成本较高，尤其当束宽度设置较大时，搜索空间会显著增加，导致计算效率下降。

### 示例
例如，以束宽 (num_beams = 2) 生成“the cat sat on the mat”这句话，过程如下：

<p align="center">
  <img src="./images/01SamplingBasics05.png">
</p>

1. 从两个候选词开始：“the”和“a”。
2. 为每个候选词评估最有可能的前两个下一个标记。
3. 每一步之后，将列表修剪为得分最高的两个序列（例如，舍弃“the big”和“a mat”，选择“the cat”和“a cat”）。

<p align="center">
  <img src="./images/01SamplingBasics06.png">
</p>

在下一个步骤中，“the big”被舍弃，“the cat”被选为第一个候选序列。同样，“a mat”被淘汰，而选择累积概率更高的“a cat”。如果上面的候选词（线上方的）比下面的候选词可能性更高，它们就会成为下一步所选的候选序列。

<p align="center">
  <img src="./images/01SamplingBasics07.png">
</p>

更大的束宽可以探索更多的可能性，有可能产生更好的结果，但计算成本也会增加。束宽为1时，本质上就是贪婪搜索，因为它在每一步只考虑最有可能的下一个标记。

束搜索在探索和利用之间取得了平衡，与贪婪采样相比，通常能产生更连贯、更多样的文本。它考虑了多条路径，这可能会导向更全局最优的序列。然而，它仍然可能存在重复问题，并且可能无法捕捉到所有可能输出的多样性。此外，束搜索的计算成本可能很高，尤其是当 (num_beams) 值较大时。

下面是一个束搜索采样导致文本重复单调的示例：
+ 输入：“My favorite color is”（我最喜欢的颜色是）
+ 束搜索采样输出（(k = 3)）：
    - “My favorite color is blue because blue is a great color”（我最喜欢的颜色是蓝色，因为蓝色是很棒的颜色）
    - “My favorite color is blue, and I love blue clothes”（我最喜欢的颜色是蓝色，而且我喜欢蓝色的衣服）
    - “My favorite color is blue, blue is just the best”（我最喜欢的颜色是蓝色，蓝色就是最好的）

在这个例子中，束搜索采样仍然存在重复问题，“blue”这个词在每个序列中都多次出现。这里，束搜索采样生成了多个序列，但它们都很相似且有重复，仅有细微差别。

参考链接：

[一文读懂 LLM 解码策略：从贪婪解码到核采样](https://segmentfault.com/a/1190000046114574)


