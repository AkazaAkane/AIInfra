# VAPO

## 一、VAPO 框架核心概述
VAPO（面向推理模型的**基于价值的增强型近端策略优化框架**）是字节 Seed 团队提出的专为推理模型设计的新型强化学习框架，在长思维链（long-CoT）推理任务中表现卓越。

### 1.1 核心性能表现
- 在 AIME 2024 数据集基准测试中，基于 Qwen 32B 预训练模型构建的 VAPO 获得**60.4 的最高分**
- 相同实验条件下，表现超过先前报告的 DeepSeek-R1-Zero-Qwen-32B 和 DAPO 模型 10 分以上
- 训练效率极高，仅需**5,000 步**即可达到最先进性能水平
- 训练稳定性优异，多次独立运行测试中未出现任何训练崩溃现象

## 二、长思维链推理的核心挑战
基于价值的强化学习方法在长思维链推理任务中面临三大关键挑战：

1. **价值模型偏差**：由于轨迹漫长且以自举方式学习价值，导致学习低偏差的价值模型难度较大
2. **异构序列长度**：长响应和短响应在优化过程中对偏差-方差权衡表现出截然不同的偏好，同步处理难度大
3. **奖励信号稀疏性**：验证者的奖励信号本身具有稀疏性，而长思维链模式进一步加剧了这一问题，对探索与利用的平衡机制提出更高要求

## 三、VAPO 框架的创新解决方案
VAPO 框架通过系统化设计，提供了综合解决方案以缓解上述挑战：

### 3.1 核心创新点
1. **长度自适应 GAE**：根据响应长度自适应调整 GAE（Generalized Advantage Estimation）计算中的参数，满足长度差异极大的响应的独特偏差-方差权衡需求
2. **多技术整合优化**：
   - 整合 DAPO 的 Clip-Higher 和 Token 级损失
   - 引入 VC-PPO 的价值预训练和解耦 GAE
   - 采用 SIL 的自模仿学习
   - 运用 GRPO 的组采样

### 3.2 性能提升表现
- 相较于原始 PPO，性能得分从 5 分提升至 60 分
- 超越先前最先进的无价值函数方法 DAPO 达 10 分
- 训练过程稳定，无崩溃现象，多次运行结果高度一致


## 四、基于价值方法与无价值方法的对比分析
| 方法类型 | 核心特点 | 优势 | 挑战 |
|---------|---------|------|------|
| 无价值方法（如 GRPO、DAPO） | 消除学习价值模型的计算开销，根据整个轨迹的最终奖励计算优势 | 1. 对组内多个轨迹的奖励进行平均，提供准确稳定的基线<br>2. 减轻对显式价值估计的需求 | 1. 信用分配精度有限<br>2. 优势估计方差较高 |
| 基于价值方法（如 VAPO） | 通过价值模型精确追踪每个动作对后续回报的影响 | 1. 实现更精确的信用分配，支持细粒度优化<br>2. 为每个 Token 提供方差更低的值估计<br>3. 价值模型具有内在泛化能力 | 1. 训练可靠价值模型难度大<br>2. 需处理长序列带来的稳定性问题 |

