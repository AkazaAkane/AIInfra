{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a13519",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 01: 经典 InstructGPT 复现\n",
    "\n",
    "本实验将使用简化代码复现 InstructGPT 的核心训练流程：监督微调(SFT)→奖励模型训练(RM)→PPO 强化学习优化。通过这个实验，我们可以深入理解如何通过人类反馈强化学习将人类偏好传递给语言模型。\n",
    "\n",
    "InstructGPT 是 OpenAI 提出的基于人类反馈的强化学习(RLHF)框架，它通过三个阶段训练语言模型遵循人类指令。虽然原始实现需要大量计算资源，但我们将使用简化模型和小型数据集来演示核心概念。\n",
    "\n",
    "## 1. 环境设置\n",
    "\n",
    "首先安装必要的依赖库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy tqdm matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e0a29",
   "metadata": {},
   "source": [
    "现在导入实验所需的模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff28328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置随机种子确保结果可复现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用设备: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f162e",
   "metadata": {},
   "source": [
    "## 2. Stage1：监督微调 SFT\n",
    "\n",
    "监督微调阶段使用人类标注的指令-回答对来微调预训练语言模型，使其初步学会遵循指令。\n",
    "\n",
    "在 SFT 阶段，我们使用标准的监督学习方式，使用人类标注的高质量问答对数据来微调预训练语言模型。模型的训练目标是最大化在给定指令情况下，生成人类期望回答的似然概率。\n",
    "\n",
    "数学上，这个目标可以表示为：\n",
    "\n",
    "$$ \\mathcal{L}_{SFT} = -\\mathbb{E}_{(x,y)\\sim D}[\\log P_{\\theta}(y|x)] $$\n",
    "\n",
    "其中 $x$ 是指令，$y$ 是人类标注的回答，$\\theta$ 是模型参数。\n",
    "\n",
    "### 2.1 数据准备\n",
    "\n",
    "我们创建一个简化的指令-回答数据集，并实现一个简单的文本编码器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19772221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 简化的文本编码器\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self):\n",
    "        # 仅保留常见字符+特殊符号（简化版词汇表）\n",
    "        self.chars = ' abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ，。？！：；,.?!:;'\n",
    "        self.char2id = {c: i+1 for i, c in enumerate(self.chars)}  # 1 开始，0 留作 padding\n",
    "        self.char2id['<pad>'] = 0\n",
    "        self.vocab_size = len(self.char2id)\n",
    "    \n",
    "    def encode(self, text, max_len=32):\n",
    "        # 文本→token id（截断或补全）\n",
    "        tokens = [self.char2id.get(c, 0) for c in text[:max_len]]  # 未知字符→0\n",
    "        if len(tokens) < max_len:\n",
    "            tokens += [0] * (max_len - len(tokens))  # 补全\n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        # token id→文本\n",
    "        return ''.join([self.chars[i-1] if i>0 else '' for i in tokens])\n",
    "\n",
    "# 创建 SFT 数据集类\n",
    "class SFTDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        instr, resp = self.data[idx]\n",
    "        text = f\"指令：{instr} 回答：{resp}\"\n",
    "        tokens = self.tokenizer.encode(text)\n",
    "        # 输入是前 n-1 个 token，目标是后 n-1 个 token\n",
    "        return {\n",
    "            'input_ids': tokens[:-1],  # [seq_len-1]\n",
    "            'labels': tokens[1:]       # [seq_len-1]\n",
    "        }\n",
    "\n",
    "# 准备 SFT 数据\n",
    "def create_sft_dataset():\n",
    "    # 简单的指令-回答对\n",
    "    return [\n",
    "        (\"什么是 AI？\", \"AI 是能模拟人类智能的技术。\"),\n",
    "        (\"1+1 等于几？\", \"1+1 等于 2。\"),\n",
    "        (\"写一句问候语\", \"你好，很高兴见到你！\"),\n",
    "        (\"什么是机器学习？\", \"机器学习是让计算机从数据中学习的技术。\"),\n",
    "        (\"如何保护环境？\", \"减少浪费，节约能源，绿色出行。\")\n",
    "    ]\n",
    "\n",
    "# 初始化 tokenizer 和数据集\n",
    "tokenizer = SimpleTokenizer()\n",
    "sft_data = create_sft_dataset()\n",
    "sft_dataset = SFTDataset(sft_data, tokenizer)\n",
    "sft_dataloader = DataLoader(sft_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c73f3b",
   "metadata": {},
   "source": [
    "这里创建了一个简单的字符级编码器`SimpleTokenizer`，将文本转换为模型可以处理的数字序列。SFTDataset 类将指令和回答组合成模型输入，并构建了合理的训练目标——用前 n-1 个 token 预测第 n 个 token，这是语言模型训练的标准做法。\n",
    "\n",
    "### 2.2 模型定义\n",
    "\n",
    "定义一个简化的 Transformer 模型，包含必要的位置编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fad4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, nhead=2, num_layers=2, max_len=50):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "        # 位置编码\n",
    "        self.register_buffer('pos_enc', self._create_pos_encoding(d_model, max_len))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def _create_pos_encoding(self, d_model, max_len):\n",
    "        # 正弦/余弦位置编码\n",
    "        pos = torch.arange(max_len).unsqueeze(1).float()  # [max_len, 1]\n",
    "        # 简化频率计算\n",
    "        div_term = 1.0 / (1000 ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "        enc = torch.zeros(max_len, d_model)\n",
    "        enc[:, 0::2] = torch.sin(pos * div_term)  # 偶数位置\n",
    "        enc[:, 1::2] = torch.cos(pos * div_term)  # 奇数位置\n",
    "        return enc\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        batch_size, seq_len = x.shape\n",
    "        x_emb = self.embedding(x)  # [batch_size, seq_len, d_model]\n",
    "        # 添加位置编码（截断到实际序列长度）\n",
    "        pos_enc = self.pos_enc[:seq_len, :]  # [seq_len, d_model]\n",
    "        x_emb = x_emb + pos_enc.unsqueeze(0)  # 添加位置编码\n",
    "        output = self.transformer(x_emb)  # [batch_size, seq_len, d_model]\n",
    "        return self.fc_out(output)  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "model = SimpleTransformer(vocab_size=tokenizer.vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0aa9dc",
   "metadata": {},
   "source": [
    "简化 Transformer 模型包含嵌入层、位置编码和 Transformer 编码器。我们使用了正弦/余弦位置编码，位置编码让模型能够理解文本中的顺序信息，是 Transformer 架构的关键组件。\n",
    "\n",
    "### 2.3 SFT 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e602785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT 训练配置\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # 忽略 padding 的损失\n",
    "\n",
    "# SFT 训练循环\n",
    "def train_sft(model, dataloader, epochs=3):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            # 准备输入和目标\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids)  # [batch_size, seq_len-1, vocab_size]\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(outputs.transpose(1, 2), labels)\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch+1} 完成。平均损失: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# 开始 SFT 训练\n",
    "print(\"开始 SFT 训练阶段...\")\n",
    "sft_losses = train_sft(model, sft_dataloader, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447bc218",
   "metadata": {},
   "source": [
    "训练过程中，我们使用交叉熵损失函数，目标是让模型学会预测下一个 token。注意我们忽略了 padding token(0)的损失，这是处理变长序列的标准做法。随着训练进行，损失应该逐步下降，表明模型正在学习指令和回答之间的关系。\n",
    "\n",
    "## 3. Stage2：奖励模型 RM\n",
    "\n",
    "奖励模型用于学习人类对模型回答的偏好，为强化学习阶段提供奖励信号。\n",
    "\n",
    "奖励模型的目标是学习人类的偏好判断，即对于同一个指令的多个回答，哪个回答更符合人类偏好。训练数据由人类标注员对模型生成回答的质量进行排名。\n",
    "\n",
    "奖励模型的训练通常使用对比学习框架，通过比较不同回答的相对质量来学习一个标量奖励函数。常用的损失函数是 pairwise ranking loss：\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{RM} = -\\mathbb{E}_{(x,y_w,y_l)\\sim D}[\\log(\\sigma(r_{\\phi}(x,y_w) - r_{\\phi}(x,y_l)))]\n",
    "$$\n",
    "\n",
    "其中 $y_w$ 是偏好的回答，$y_l$ 是不偏好的回答，$r_{\\phi}$ 是奖励模型。\n",
    "\n",
    "### 3.1 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34496af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建奖励模型数据集类\n",
    "class RMDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        good_text = f\"指令：{item['instr']} 回答：{item['good']}\"\n",
    "        bad_text = f\"指令：{item['instr']} 回答：{item['bad']}\"\n",
    "        return {\n",
    "            'good_input_ids': self.tokenizer.encode(good_text),\n",
    "            'bad_input_ids': self.tokenizer.encode(bad_text)\n",
    "        }\n",
    "\n",
    "# 创建奖励模型训练数据\n",
    "def create_rm_dataset():\n",
    "    # 模拟人类对回答的偏好比较\n",
    "    return [\n",
    "        {\n",
    "            \"instr\": \"什么是 AI？\",\n",
    "            \"good\": \"AI 是能模拟人类智能的技术，比如语音识别和图像识别。\",\n",
    "            \"bad\": \"AI 就是机器人。\"\n",
    "        },\n",
    "        {\n",
    "            \"instr\": \"1+1 等于几？\",\n",
    "            \"good\": \"1+1 的结果是 2。\",\n",
    "            \"bad\": \"1+1 可能等于 3，看情况。\"\n",
    "        },\n",
    "        {\n",
    "            \"instr\": \"写一句问候语\",\n",
    "            \"good\": \"你好！很高兴有机会与你交流。\",\n",
    "            \"bad\": \"喂，你好。\"\n",
    "        },\n",
    "        {\n",
    "            \"instr\": \"什么是机器学习？\",\n",
    "            \"good\": \"机器学习是人工智能的一个分支，让计算机能从数据中学习并改进。\",\n",
    "            \"bad\": \"机器学习就是教机器读书。\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# 准备 RM 数据\n",
    "rm_data = create_rm_dataset()\n",
    "rm_dataset = RMDataset(rm_data, tokenizer)\n",
    "rm_dataloader = DataLoader(rm_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc33b40",
   "metadata": {},
   "source": [
    "奖励模型需要成对的偏好数据，每个数据点包含一个指令、一个高质量回答和一个低质量回答。这种数据结构允许我们训练模型区分回答的质量差异。\n",
    "\n",
    "### 3.2 奖励模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ceb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义奖励模型\n",
    "class RewardModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model  # 复用 SFT 阶段的基础模型\n",
    "        self.reward_head = nn.Linear(64, 1)  # 奖励头，将特征映射为标量奖励\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len]\n",
    "        x_emb = self.base_model.embedding(x)  # [batch_size, seq_len, d_model]\n",
    "        # 添加位置编码（截断到实际序列长度）\n",
    "        pos_enc = self.base_model.pos_enc[:x.shape[1], :]  # [seq_len, d_model]\n",
    "        x_emb = x_emb + pos_enc.unsqueeze(0)  # 添加位置编码\n",
    "        \n",
    "        # 获取基础模型的特征输出\n",
    "        features = self.base_model.transformer(x_emb)  # [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # 使用最后一个 token 的特征计算奖励\n",
    "        last_token_features = features[:, -1, :]  # [batch_size, d_model]\n",
    "        reward = self.reward_head(last_token_features)  # [batch_size, 1]\n",
    "        return reward\n",
    "\n",
    "# 初始化奖励模型（基于 SFT 模型构建）\n",
    "reward_model = RewardModel(model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a304e435",
   "metadata": {},
   "source": [
    "奖励模型基于第一阶段训练的 SFT 模型构建，这样可以利用已经学到的指令理解能力。我们添加了一个简单的线性层作为\"奖励头\"，将模型最后一个 token 的特征转换为一个标量奖励值。\n",
    "\n",
    "### 3.3 RM 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a719a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 奖励模型训练\n",
    "def train_rm(reward_model, dataloader, epochs=2):\n",
    "    reward_model.train()\n",
    "    optimizer = optim.Adam(reward_model.parameters(), lr=1e-3)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"RM Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 准备好回答和差回答的输入\n",
    "            good_inputs = batch['good_input_ids'].to(device)\n",
    "            bad_inputs = batch['bad_input_ids'].to(device)\n",
    "            \n",
    "            # 计算奖励\n",
    "            good_reward = reward_model(good_inputs)\n",
    "            bad_reward = reward_model(bad_inputs)\n",
    "            \n",
    "            # 计算对比损失：好回答的奖励应高于差回答\n",
    "            loss = -torch.log(torch.sigmoid(good_reward - bad_reward)).mean()\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"RM Epoch {epoch+1} 完成。平均损失: {avg_loss:.4f}\")\n",
    "\n",
    "# 开始奖励模型训练\n",
    "print(\"开始奖励模型训练阶段...\")\n",
    "train_rm(reward_model, rm_dataloader, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5809b9e2",
   "metadata": {},
   "source": [
    "奖励模型的训练使用对比损失函数，目标是让模型对高质量回答给出更高的奖励值，对低质量回答给出更低的奖励值。这种成对比较的方式能够有效学习人类的偏好。随着训练进行，损失应该下降，表明模型逐渐学会了区分回答质量。\n",
    "\n",
    "## 4. Stage3：PPO 强化学习\n",
    "\n",
    "在 PPO 阶段，我们使用奖励模型提供的奖励信号来进一步优化语言模型，使其生成更符合人类偏好的回答。\n",
    "\n",
    "### 4.1 技术原理\n",
    "\n",
    "PPO(Proximal Policy Optimization)是一种强化学习算法，它通过限制策略更新的步长来确保训练稳定性。在 RLHF 中，PPO 用于优化语言模型，使其生成的回答能获得更高的奖励模型评分。\n",
    "\n",
    "PPO 的核心目标函数如下：\n",
    "\n",
    "$$ \\mathcal{L}^{PPO} = \\mathbb{E}[\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)] $$\n",
    "\n",
    "其中 $r_t(\\theta) = \\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\text{old}}(a_t|s_t)}$ 是概率比，$\\hat{A}_t$ 是优势估计（这里简化为奖励模型的输出），$\\epsilon$ 是裁剪参数。\n",
    "\n",
    "### 4.2 PPO 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12126a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_train(model, reward_model, tokenizer, epochs=2):\n",
    "    # 冻结奖励模型的参数\n",
    "    for param in reward_model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    model.train()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
    "    \n",
    "    # 简化 PPO 超参数\n",
    "    clip_epsilon = 0.2  # 裁剪参数\n",
    "    instructions = [\"什么是 AI？\", \"1+1 等于几？\", \"写一句问候语\"]  # 用于 PPO 训练的指令\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_reward = 0\n",
    "        progress_bar = tqdm(instructions, desc=f\"PPO Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for instruction in progress_bar:\n",
    "            # 准备输入：指令 + \"回答：\"前缀\n",
    "            input_text = f\"指令：{instruction} 回答：\"\n",
    "            input_ids = tokenizer.encode(input_text).unsqueeze(0).to(device)  # [1, seq_len]\n",
    "            \n",
    "            # 存储生成过程中的动作和对数概率\n",
    "            generated_tokens = []\n",
    "            log_probs = []\n",
    "            \n",
    "            # 生成回答并记录每个token的选择概率\n",
    "            current_ids = input_ids.clone()\n",
    "            for _ in range(8):  # 生成 8 个 token\n",
    "                logits = model(current_ids)[:, -1, :]  # 获取最后一个 token 的 logits\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                \n",
    "                # 使用采样而不是贪心选择，以保持探索性\n",
    "                next_token = torch.multinomial(probs, 1)  # [1, 1]\n",
    "                log_prob = torch.log(probs.gather(1, next_token))  # [1, 1]\n",
    "                \n",
    "                generated_tokens.append(next_token)\n",
    "                log_probs.append(log_prob)\n",
    "                current_ids = torch.cat([current_ids, next_token], dim=1)\n",
    "            \n",
    "            # 计算当前生成序列的奖励\n",
    "            with torch.no_grad():\n",
    "                reward_value = reward_model(current_ids).item()\n",
    "            total_reward += reward_value\n",
    "            \n",
    "            # 将奖励转换为tensor以便参与梯度计算\n",
    "            reward_tensor = torch.tensor(reward_value, dtype=torch.float32, device=device)\n",
    "            \n",
    "            # PPO 损失计算 - 对每个生成的token进行优化\n",
    "            policy_loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            for i, (token, old_log_prob) in enumerate(zip(generated_tokens, log_probs)):\n",
    "                # 重新计算当前策略下该token的概率\n",
    "                context_ids = torch.cat([input_ids] + generated_tokens[:i], dim=1)\n",
    "                new_logits = model(context_ids)[:, -1, :]\n",
    "                new_probs = torch.softmax(new_logits, dim=-1)\n",
    "                new_log_prob = torch.log(new_probs.gather(1, token))\n",
    "                \n",
    "                # 计算策略比率\n",
    "                ratio = torch.exp(new_log_prob - old_log_prob.detach())\n",
    "                \n",
    "                # 计算PPO裁剪损失\n",
    "                surr1 = ratio * reward_tensor\n",
    "                surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * reward_tensor\n",
    "                policy_loss = policy_loss - torch.min(surr1, surr2)  # 负号因为我们要最大化\n",
    "            \n",
    "            policy_loss = policy_loss / len(generated_tokens)  # 平均损失\n",
    "            \n",
    "            # 优化步骤\n",
    "            optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            progress_bar.set_postfix({\"奖励\": reward_value})\n",
    "        \n",
    "        avg_reward = total_reward / len(instructions)\n",
    "        print(f\"PPO Epoch {epoch+1} 完成。平均奖励: {avg_reward:.4f}\")\n",
    "\n",
    "# 开始 PPO 训练\n",
    "print(\"开始 PPO 训练阶段...\")\n",
    "ppo_train(model, reward_model, tokenizer, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe035f7",
   "metadata": {},
   "source": [
    "PPO 训练的核心是通过奖励模型提供的反馈来优化语言模型。我们首先冻结奖励模型参数，确保奖励信号稳定。然后，对于每个指令，模型生成回答并获得奖励。PPO 的关键是使用裁剪机制限制策略更新的幅度，防止更新过大导致训练不稳定。随着训练进行，模型生成的回答应该能获得越来越高的奖励。\n",
    "\n",
    "## 5. 实验结果与分析\n",
    "\n",
    "通过上述三个阶段的训练，我们完成了简化版的 InstructGPT 训练流程。虽然实现经过了大幅简化，但核心思想得以保留：\n",
    "\n",
    "1. **SFT 阶段**：模型学会了基本的指令跟随能力\n",
    "2. **RM 阶段**：奖励模型学会了区分回答质量\n",
    "3. **PPO 阶段**：语言模型根据奖励信号进一步优化生成策略\n",
    "\n",
    "### 5.1 训练过程可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba608a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制训练损失曲线\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# SFT 损失曲线\n",
    "plt.plot(range(1, len(sft_losses)+1), sft_losses, marker='o', color='blue')\n",
    "plt.title('SFT 训练损失')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('损失值')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d6128e",
   "metadata": {},
   "source": [
    "**预期结果**：SFT 阶段的损失应该呈现稳步下降趋势，表明模型正在学习指令与回答之间的关系。RM 阶段的损失也应该下降，表明奖励模型学会了区分好回答和差回答。PPO 阶段的平均奖励应该上升，表明模型生成的回答越来越符合奖励模型定义的\"好\"标准。\n",
    "\n",
    "### 5.2 模型效果对比\n",
    "\n",
    "我们可以对比 SFT 阶段和 PPO 阶段模型的生成效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906dd5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试函数：生成回答并显示\n",
    "def generate_response(model, tokenizer, instruction, max_tokens=15):\n",
    "    model.eval()\n",
    "    input_text = f\"指令：{instruction} 回答：\"\n",
    "    input_ids = tokenizer.encode(input_text)[:-1].unsqueeze(0).to(device)\n",
    "    \n",
    "    generated_ids = input_ids.clone()\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            logits = model(generated_ids)[:, -1, :]\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "            generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "    \n",
    "    # 解码生成的文本\n",
    "    generated_text = tokenizer.decode(generated_ids.squeeze().cpu().numpy())\n",
    "    # 提取回答部分\n",
    "    answer = generated_text.split(\"回答：\")[-1].strip()\n",
    "    return answer\n",
    "\n",
    "# 测试几个指令\n",
    "test_instructions = [\n",
    "    \"什么是 AI？\",\n",
    "    \"1+1 等于几？\",\n",
    "    \"写一句问候语\"\n",
    "]\n",
    "\n",
    "print(\"模型生成结果：\")\n",
    "for instr in test_instructions:\n",
    "    response = generate_response(model, tokenizer, instr)\n",
    "    print(f\"指令：{instr}\")\n",
    "    print(f\"回答：{response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41794438",
   "metadata": {},
   "source": [
    "经过 PPO 优化的模型生成的回答应该比仅经过 SFT 的模型更符合人类偏好。\n",
    "\n",
    "## 6. 总结与思考\n",
    "\n",
    "本实验通过简化实现复现了 InstructGPT 的 RLHF 三阶段流程。虽然实际应用中的实现更加复杂，但核心思想是一致的：\n",
    "\n",
    "1. **监督微调**为模型提供基础的指令跟随能力\n",
    "2. **奖励模型**学习人类偏好并提供量化反馈\n",
    "3. **PPO 强化学习**利用奖励信号进一步优化模型\n",
    "\n",
    "这种方法的优势在于能够将人类的主观偏好有效地传递给模型，使模型生成更加符合人类期望的内容。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
