## 概述
越来越多的工作证明将RL 应用到 LLMs 训练是一个非常有前景的方向，比如DeepSeek-R1，Qwen3等。本节内容首先介绍一个通用大模型在预训练后还会进行哪些阶段，后面着重介绍将RL应用与大模型训练的RLHF框架。

### 全流程训练
在预训练阶段，模型通过自回归方式在海量无标注文本中学习“预测下一个 token”的能力，最终得到一个强大的“续写”引擎。借助 few-shot 提示，该引擎已能解决不少复杂任务，但其行为更多体现为“继续写下去”，而非“按人类意图完成指令”。

有监督微调（SFT）能够让大模型按照人们的意图进行输入。它以（输入，目标）成对数据为核心，让模型学习“看到输入→给出人类真正想要的输出”，从而显式地获得指令遵循与问答能力。经过 SFT 的模型，不再是单纯的续写器，而成为能够听懂并执行人类指令的对话系统。

强化：PPO，DPO，GRPO等算法本质上是将强化应用于大模型的训练中，从而有效的提升了模型回答与人们的偏好对齐。



### RLHF

在典型的强化学习里，一个智能体Agent在状态s下会在多个动作里面选择一个，强化学习的目标是要在每一个状态下选择的动作是最好的，也就是说它的长期奖赏是最大的。对应于大模型里面，模型是一个token一个token输出的，所以这就是每一轮的状态，它的动作是输出的token是什么，这对应了强化学习中的动作。大模型中的动作空间恰好是整个词汇表的大小。

在RLHF中有四个模型

  - Actor Model：由SFT之后的模型初始化而来。作为策略（policy）模型，用于接收上文，做出动作，预测下一个字符。我们的目标就是要更新这个模型。
  - Reference Model：和Actor Model同样初始化自SFT Model，用于和Actor Model做对比，保证模型不要偏离原始SFT Model太多。这个模型相当于是一个参考模型，在训练的时候不需要更新参数。
  - Reward Model：奖励模型，针对每一个状态，给出奖励分数。这个模型也不需要训练。
  - Critic Model：可以由Reward Model初始化而来，用于近似价值函数，输入为状态s，估计当前状态的价值V。该模型需要训练和迭代。

训练过程整体分为两步：maker experience和learn。
  - 首先是make_experience，首先在训练数据中抽取一部分query，然后Actor Model生成答案。然后我们依据这条答案获取我们所需要的经验：
  - actor_logits:由Actor Model产生，包含对答案所有词的概率分布。
  - reference_logits：由Reference Model产生，包含对答案所有词语的概率分布，用于和actor logits进行对比，防止actor model偏离SFT Model太远。
  - reward_score: 由Reward Model产生，为当前句子状态下，立即获取的收益分数。
  - values：由Critic Model产生，估计当前句子状态下到完成生成，可以获取的回报，也就是状态价值函数。

![alt text](./images/instruct_gpt.png)