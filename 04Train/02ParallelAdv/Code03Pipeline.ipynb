{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "964e933a",
   "metadata": {},
   "source": [
    "<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->\n",
    "\n",
    "# CODE 03: Pipeline 并行实践\n",
    "\n",
    "本实验旨在深入理解 Pipeline 并行原理。先实现 Gpipe 流水线并分析空泡率现象，后进阶实现 1F1B 和 Interleaved 1F1B 调度策略，优化空泡率现象，并实践混合并行策略。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a924ba",
   "metadata": {},
   "source": [
    "## 1. Pipeline 并行基础\n",
    "\n",
    "**Pipeline 并行（Pipeline Parallelism, PP）** 其核心思想是将一个庞大的神经网络模型，沿着层（Layer）的维度进行纵向切割，分割成多个连续的子模块（称为“阶段”，Stage），并将这些阶段部署到不同的计算设备（如 GPU）上。\n",
    "\n",
    "数学上，模型可表示为函数复合：$F(x) = f_n(f_{n-1}(...f_1(x)...))$，其中每个 $f_i$（模型层/层组）对应 Pipeline 的一个“阶段”，分配到不同设备上执行。数据以“批次”（batch）的形式，像工厂流水线一样，依次流经各个阶段。\n",
    "\n",
    "通过这种方式，每个设备只需加载和处理模型的一部分，从而突破**单卡显存的限制**。\n",
    "\n",
    "然而，这种拆分也引入了新的挑战：\n",
    "*   **通信开销：** 前向传播和反向传播过程中，相邻阶段之间需要频繁地传递中间结果（激活值和梯度），这会带来额外的通信延迟。\n",
    "*   **空泡现象（Bubble）：** 由于流水线的“填充”（Fill）和“排空”（Drain）过程，部分设备在某些时刻会处于等待数据的空闲状态，造成计算资源的浪费。\n",
    "\n",
    "**后续优化方向**：\n",
    "Gpipe、1F1B、Interleaved 1F1B 等调度策略，本质都是通过调整「前向」和「反向」的执行节奏，来**压缩空泡时间、降低通信影响、更高效利用显存** —— 这些我们将在代码实践中逐一实现和对比。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b637649d",
   "metadata": {},
   "source": [
    "## 2. Native Pipeline Parallelism（传统流水线并行）\n",
    "\n",
    "首先，我们实现一个基础的流水线并行框架，只考虑了模型分割和流水线调度，将数据以 batch 为单位进行处理。\n",
    "\n",
    "![](./images/Code03Pipeline01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "89ad7537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "def get_available_devices(max_devices=4):\n",
    "    \"\"\"自动获取可用设备，解决原代码设备硬编码问题\"\"\"\n",
    "    devices = []\n",
    "    num_cuda = torch.cuda.device_count()\n",
    "    devices = [torch.device(f\"cuda:{i}\") for i in range(min(num_cuda, max_devices))]\n",
    "    print(f\"当前使用设备列表: {[str(dev) for dev in devices]}\")\n",
    "    return devices\n",
    "\n",
    "class PipelineParallel(nn.Module):\n",
    "    def __init__(self, module_list, device_ids):\n",
    "        super().__init__()\n",
    "        assert len(module_list) == len(device_ids), \"模块数量必须与设备数量相同\"\n",
    "\n",
    "        self.stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "\n",
    "        # 将每个阶段移动到对应的设备\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, device_ids)):\n",
    "            self.stages[i] = stage.to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        简单的前向传播 Pipeline\n",
    "        输入数据依次通过每个阶段，保留中间结果用于反向传播\n",
    "        \"\"\"\n",
    "        intermediates = []\n",
    "        current_output = x.to(self.device_ids[0])  # 输入先迁移到第一阶段设备\n",
    "\n",
    "        # 数据依次通过每个阶段\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "            current_output = stage(current_output)  # 本阶段计算\n",
    "            if i < len(self.stages) - 1:\n",
    "                # 保留中间结果（detach 避免梯度提前计算）\n",
    "                intermediates.append(current_output.detach().clone())\n",
    "                # 传递到下一阶段设备\n",
    "                current_output = current_output.to(self.device_ids[i+1])\n",
    "\n",
    "        return current_output, intermediates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4a319",
   "metadata": {},
   "source": [
    "上面的代码实现了一个基础的流水线并行框架。它将模型分割为多个阶段，每个阶段放置在不同的设备上。在前向传播过程中，数据依次通过这些阶段，并在阶段间进行设备间的数据传输。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc12d09",
   "metadata": {},
   "source": [
    "## 3. Gpipe 流水线并行\n",
    "\n",
    "Gpipe(Gradient Pipeline) 是一种基于流水线并行的模型并行策略，它将一个大的训练批次（Batch）拆分成多个小的微批次（Micro-batch），依次流过 Pipeline 的各个阶段，每个阶段放置在不同的设备上。\n",
    "\n",
    "![](./images/Code03Pipeline02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dd30c19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineParallelGpipe(nn.Module):\n",
    "    \"\"\"\n",
    "    Gpipe 调度策略的 Pipeline 并行实现\n",
    "    核心特点：\n",
    "      - 先完成所有微批次的前向传播，再统一反向传播\n",
    "      - 需要缓存所有中间激活值 → 显存占用高\n",
    "      - 空泡率 = (S-1)/ (M + S - 1) （前向填充 + 反向排空）\n",
    "    \"\"\"\n",
    "    def __init__(self, module_list, device_ids, num_microbatches):\n",
    "        super().__init__()\n",
    "        assert len(module_list) == len(device_ids), \"模块数量必须与设备数量相同\"\n",
    "\n",
    "        self.stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "        self.num_microbatches = num_microbatches\n",
    "        self.num_stages = len(self.stages)\n",
    "\n",
    "        # 将每个阶段移动到对应设备\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, device_ids)):\n",
    "            self.stages[i] = stage.to(dev)\n",
    "\n",
    "        # 用于缓存每个阶段每个微批次的输出（前向传播时保存，反向传播时使用）\n",
    "        self.cached_outputs = None  # 初始化为 None，在 forward 中创建\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Gpipe 调度逻辑：\n",
    "        Step 1: 划分微批次\n",
    "        Step 2: 所有微批次依次完成前向传播（流水线填充）\n",
    "        Step 3: 所有微批次依次完成反向传播（流水线排空）\n",
    "        Step 4: 返回平均损失（模拟梯度累积）\n",
    "        \"\"\"\n",
    "        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)\n",
    "        if len(micro_batches) != self.num_microbatches:\n",
    "            raise ValueError(\"输入无法均匀划分为指定微批次\")\n",
    "\n",
    "        # 初始化缓存结构：stage_outputs[stage_idx][mb_idx] = output_tensor\n",
    "        self.cached_outputs = [[None for _ in range(self.num_microbatches)]\n",
    "                              for _ in range(self.num_stages)]\n",
    "\n",
    "        total_loss = 0.0\n",
    "        loss_count = 0\n",
    "\n",
    "        print(f\"[Gpipe 调度] 开始前向传播，共 {self.num_microbatches} 个微批次...\")\n",
    "\n",
    "        # ================= 前向传播 =================\n",
    "        for mb_idx, mb in enumerate(micro_batches):\n",
    "            current_tensor = mb.to(self.device_ids[0])\n",
    "            for stage_idx in range(self.num_stages):\n",
    "                stage = self.stages[stage_idx]\n",
    "                dev = self.device_ids[stage_idx]\n",
    "\n",
    "                # 迁移到当前阶段设备\n",
    "                if current_tensor.device != dev:\n",
    "                    current_tensor = current_tensor.to(dev)\n",
    "\n",
    "                # 前向计算\n",
    "                current_tensor = stage(current_tensor)\n",
    "\n",
    "                # 缓存输出（用于后续反向）\n",
    "                self.cached_outputs[stage_idx][mb_idx] = current_tensor\n",
    "\n",
    "                # 打印日志\n",
    "                print(f\"  微批次 {mb_idx:2d} | 阶段 {stage_idx:2d} | 前向完成\")\n",
    "\n",
    "        print(f\"[Gpipe 调度] 前向传播完成，开始反向传播...\")\n",
    "\n",
    "        # ================= 反向传播 =================\n",
    "        # 从最后一个微批次开始反向（Gpipe 顺序反向）\n",
    "        for mb_idx in reversed(range(self.num_microbatches)):\n",
    "            # 获取最终输出（最后一个阶段）\n",
    "            final_output = self.cached_outputs[-1][mb_idx]\n",
    "            label = torch.randint(0, 10, (final_output.shape[0],), device=final_output.device)\n",
    "            loss = F.cross_entropy(final_output, label)\n",
    "            total_loss += loss.item()\n",
    "            loss_count += 1\n",
    "\n",
    "            # 触发反向传播（PyTorch 自动沿计算图回传梯度）\n",
    "            loss.backward()\n",
    "            print(f\"  微批次 {mb_idx:2d} | 反向完成 | 损失: {loss.item():.4f}\")\n",
    "\n",
    "            # 注意：Gpipe 中不立即清空中间激活，因为可能被其他 micro-batch 的反向依赖\n",
    "            # 实际工程中需配合梯度检查点（Gradient Checkpointing）节省显存\n",
    "\n",
    "        avg_loss = total_loss / loss_count if loss_count > 0 else 0.0\n",
    "        return torch.tensor(avg_loss, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e102afd",
   "metadata": {},
   "source": [
    "## 4. 空泡率分析与计算\n",
    "\n",
    "**空泡率**是衡量流水线并行效率的重要指标，表示由于流水线填充和排空造成的计算资源浪费比例。空泡率的计算基于流水线填充和排空的时间开销。当微批次数量远大于流水线阶段数时，空泡率会降低，因为填充和排空时间相对于总计算时间的比例变小。\n",
    "\n",
    "我们在这里以**Gpipe 流水线并行**的空泡率计算为例，计算空泡率。\n",
    "\n",
    "在数学上，空泡率可以表示为：\n",
    "\n",
    "$$\n",
    "Bubble = (T_{fill} + T_{drain}) / (T_{total}) = (S - 1 + S - 1) / (2*(M + S - 1)) = (S - 1) / (M + S - 1)\n",
    "$$\n",
    "\n",
    "其中 $S$ 是流水线阶段数，$M$ 是微批次数量。$T_{fill}$ 表示流水线填充时间，$T_{drain}$ 表示流水线排空时间,$T_{total}$ 表示流水线总时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a4e7e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 不同配置下的空泡率计算结果 ===\n",
      "阶段数:   4, 微批次:   4, 空泡率: 0.429\n",
      "阶段数:   4, 微批次:   8, 空泡率: 0.273\n",
      "阶段数:   4, 微批次:  16, 空泡率: 0.158\n",
      "阶段数:   4, 微批次:  32, 空泡率: 0.086\n",
      "阶段数:   4, 微批次:  64, 空泡率: 0.045\n",
      "阶段数:   4, 微批次: 100, 空泡率: 0.029\n",
      "阶段数:   8, 微批次:  16, 空泡率: 0.304\n",
      "阶段数:  16, 微批次:  32, 空泡率: 0.319\n",
      "阶段数:  32, 微批次:  64, 空泡率: 0.326\n",
      "阶段数:   8, 微批次:  32, 空泡率: 0.179\n",
      "阶段数:  16, 微批次:  64, 空泡率: 0.190\n"
     ]
    }
   ],
   "source": [
    "def calculate_bubble_rate(num_stages, num_microbatches):\n",
    "    \"\"\"\n",
    "    计算 Pipeline 并行的空泡率\n",
    "\n",
    "    参数:\n",
    "        num_stages: Pipeline 阶段数（S）\n",
    "        num_microbatches: 微批次数量（M）\n",
    "\n",
    "    返回:\n",
    "        空泡率（0~1 之间，值越小效率越高）\n",
    "\n",
    "    数学公式:\n",
    "        空泡率 = Pipeline 填充时间 / 总时间 = (S - 1) / (M + S - 1)\n",
    "        说明：1F1B 中“排空阶段”与后续微批次的前向重叠，无需额外计算排空时间\n",
    "    \"\"\"\n",
    "    if num_microbatches <= 0 or num_stages <= 0:\n",
    "        raise ValueError(\"阶段数和微批次数量必须为正整数\")\n",
    "\n",
    "    # 理想时间：仅计算所有微批次的时间（无空泡）\n",
    "    ideal_time = num_microbatches\n",
    "    # 实际时间：填充时间（S-1） + 计算时间（M）\n",
    "    actual_time = num_microbatches + num_stages - 1\n",
    "    # 空泡率 = 空泡时间 / 实际总时间\n",
    "    bubble_rate = (actual_time - ideal_time) / actual_time\n",
    "\n",
    "    return bubble_rate\n",
    "\n",
    "configurations = [\n",
    "    # 【对比组 1】固定 S=4，观察 M 增大如何降低空泡率（展示收益递减）\n",
    "    (4, 4),   # M = S，空泡率较高，临界点\n",
    "    (4, 8),   # M = 2S\n",
    "    (4, 16),  # M = 4S（推荐工程起点）\n",
    "    (4, 32),  # M = 8S\n",
    "    (4, 64),  # M = 16S\n",
    "    (4, 100),  # M = 25S，接近理想\n",
    "\n",
    "    # 【对比组 2】固定 M=2S，观察 S 增大时空泡率如何上升（展示规模代价）\n",
    "    (8, 16),  # M = 2S\n",
    "    (16, 32), # M = 2S\n",
    "    (32, 64), # M = 2S（如资源允许）\n",
    "\n",
    "    # 【对比组 3】固定 M=4S，观察不同规模下的表现（推荐工程配置）\n",
    "    (8, 32),  # M = 4S\n",
    "    (16, 64), # M = 4S\n",
    "]\n",
    "\n",
    "print(\"=== 不同配置下的空泡率计算结果 ===\")\n",
    "for num_stages, num_microbatches in configurations:\n",
    "    rate = calculate_bubble_rate(num_stages, num_microbatches)\n",
    "    print(f\"阶段数: {num_stages:3d}, 微批次: {num_microbatches:3d}, 空泡率: {rate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852011d3",
   "metadata": {},
   "source": [
    "从上面代码的运行结果我们可以看出：\n",
    "- **微批次的影响**：当 $M \\gg S$ 时，空泡率趋近于 0（如 $S=4, M=100$，空泡率≈0.029），因此增加微批次是降低空泡率的核心手段。\n",
    "- **阶段数的影响**：$S$ 越大，空泡率越高（相同 $M$ 下，$S=16$ 比 $S=4$ 空泡率高约 20%），因此 Pipeline 阶段数需与微批次数量匹配（建议 $M \\geq 4S$）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a66a8",
   "metadata": {},
   "source": [
    "## 5. 1F1B 调度策略实现\n",
    "\n",
    "1F1B(One-Forward-One-Backward) 调度是一种优化的流水线并行策略，它通过交替执行前向和反向传播来减少内存使用和空泡时间。\n",
    "\n",
    "![](./images/Code03Pipeline03.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "54329222",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PipelineParallel1F1B(nn.Module):\n",
    "    \"\"\"\n",
    "    1F1B 调度策略的 Pipeline 并行\n",
    "    核心改进：补全“前向→反向交替”逻辑，减少内存占用并降低空泡率\n",
    "    \"\"\"\n",
    "    def __init__(self, module_list, device_ids, num_microbatches):\n",
    "        super().__init__()\n",
    "        self.stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "        self.num_microbatches = num_microbatches  # 微批次数量\n",
    "        self.num_stages = len(self.stages)  # Pipeline 阶段数\n",
    "\n",
    "        # 阶段设备分配\n",
    "        for i, (stage, dev) in enumerate(zip(self.stages, device_ids)):\n",
    "            self.stages[i] = stage.to(dev)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        1F1B 调度核心逻辑：\n",
    "        1. 划分微批次 → 2. 前向传播 S 个微批次（填充 Pipeline）→ 3. 交替执行前向与反向\n",
    "        \"\"\"\n",
    "        # 1. 将输入数据划分为多个微批次（按批量维度分割）\n",
    "        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)\n",
    "        # 存储各阶段前向结果（用于后续反向传播）\n",
    "        stage_outputs = [[] for _ in range(self.num_stages)]\n",
    "        total_loss = 0.0  # 累计损失，用于后续平均\n",
    "\n",
    "        # 2. 1F1B 调度执行\n",
    "        for mb_idx, mb in enumerate(micro_batches):\n",
    "            # 前向传播：当前微批次通过所有 Pipeline 阶段\n",
    "            current_mb = mb.to(self.device_ids[0])\n",
    "            for stage_idx, (stage, dev) in enumerate(zip(self.stages, self.device_ids)):\n",
    "                current_mb = stage(current_mb)\n",
    "                stage_outputs[stage_idx].append(current_mb)  # 保存当前阶段输出\n",
    "                if stage_idx < self.num_stages - 1:\n",
    "                    current_mb = current_mb.to(self.device_ids[stage_idx+1])\n",
    "\n",
    "            # 3. 交替反向：当微批次索引 ≥ 阶段数时，对最早的微批次执行反向\n",
    "            if mb_idx >= self.num_stages - 1:\n",
    "                # 待反向的微批次索引（最早填充的微批次：mb_idx - (S-1)）\n",
    "                reverse_mb_idx = mb_idx - (self.num_stages - 1)\n",
    "                # 从最后一个阶段获取输出，计算损失（模拟分类任务）\n",
    "                final_output = stage_outputs[-1][reverse_mb_idx]\n",
    "                # 生成匹配设备的标签（避免设备不匹配报错）\n",
    "                label = torch.randint(0, 10, (final_output.shape[0],), device=final_output.device)\n",
    "                # 计算损失（触发反向传播的前提）\n",
    "                loss = F.cross_entropy(final_output, label)\n",
    "                total_loss += loss.item()\n",
    "                # 模拟反向传播日志（实际场景需调用 loss.backward()并同步梯度）\n",
    "                print(f\"[1F1B 调度] 微批次{reverse_mb_idx:2d}反向计算 | 损失: {loss.item():.4f}\")\n",
    "\n",
    "        # 4. 处理剩余未反向的微批次（最后 S-1 个微批次，Pipeline 排空阶段）\n",
    "        for reverse_mb_idx in range(mb_idx - (self.num_stages - 2), self.num_microbatches):\n",
    "            if reverse_mb_idx >= self.num_microbatches:\n",
    "                break\n",
    "            final_output = stage_outputs[-1][reverse_mb_idx]\n",
    "            label = torch.randint(0, 10, (final_output.shape[0],), device=final_output.device)\n",
    "            loss = F.cross_entropy(final_output, label)\n",
    "            total_loss += loss.item()\n",
    "            print(f\"[1F1B 调度] 微批次{reverse_mb_idx:2d}反向计算 | 损失: {loss.item():.4f}\")\n",
    "\n",
    "        # 返回所有微批次的平均损失\n",
    "        avg_loss = total_loss / self.num_microbatches if self.num_microbatches > 0 else 0.0\n",
    "        return torch.tensor(avg_loss, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab5bdd3",
   "metadata": {},
   "source": [
    "1F1B 调度的核心思想是在流水线中交替执行前向传播和反向传播，而不是先完成所有前向传播再进行反向传播。这种策略有两个主要优势：\n",
    "\n",
    "1. **减少内存使用**：不需要存储所有微批次的前向传播中间结果\n",
    "2. **降低空泡率**：通过更早开始反向传播，减少设备空闲时间"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9052f70",
   "metadata": {},
   "source": [
    "## 6. Interleaved 1F1B 调度策略实现\n",
    "\n",
    "Interleaved 1F1B 调度是一种改进的 1F1B 调度策略，它通过交替执行前向和反向传播，并引入额外的填充和排空步骤来减少空泡率。\n",
    "\n",
    "![](./images/Code03Pipeline04.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b95a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "class PipelineParallelInterleaved1F1B(nn.Module):\n",
    "    \"\"\"\n",
    "    Interleaved 1F1B 调度策略的 Pipeline 并行（修正版）\n",
    "    核心思想：\n",
    "      - 每个物理设备运行多个“虚拟阶段”，交错处理不同微批次\n",
    "      - 前向和反向紧密交错，压缩流水线气泡\n",
    "      - 微批次数 M 应 >= 总虚拟阶段数 V = S * K（S=物理阶段数，K=虚拟倍数）\n",
    "    \"\"\"\n",
    "    def __init__(self, module_list: List[nn.Module], device_ids: List[int], num_microbatches: int, virtual_pipeline_size: int = 2):\n",
    "        super().__init__()\n",
    "        assert len(module_list) == len(device_ids), \"物理阶段数必须等于设备数\"\n",
    "        self.physical_stages = nn.ModuleList(module_list)\n",
    "        self.device_ids = device_ids\n",
    "        self.num_microbatches = num_microbatches\n",
    "        self.num_physical_stages = len(self.physical_stages)\n",
    "        self.virtual_pipeline_size = virtual_pipeline_size\n",
    "        self.total_virtual_stages = self.num_physical_stages * virtual_pipeline_size\n",
    "\n",
    "        # 验证微批次数量是否满足交织条件（简化：要求 M >= V）\n",
    "        assert num_microbatches >= self.total_virtual_stages, \\\n",
    "            f\"微批次数量{num_microbatches}需 >= 总虚拟阶段数{self.total_virtual_stages}\"\n",
    "\n",
    "        for i, (stage, dev) in enumerate(zip(self.physical_stages, device_ids)):\n",
    "            self.physical_stages[i] = stage.to(dev)\n",
    "            print(f\"[Interleaved 初始化] 物理阶段 {i} 已部署到设备: {dev}\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Interleaved 1F1B 调度核心逻辑：\n",
    "          - 输入被切分为多个微批次，每个微批次被分配到不同的设备\n",
    "        \"\"\"\n",
    "        micro_batches = torch.chunk(x, self.num_microbatches, dim=0)\n",
    "        if len(micro_batches) != self.num_microbatches:\n",
    "            raise ValueError(\"输入无法均匀划分为指定微批次\")\n",
    "\n",
    "        physical_outputs = [[None for _ in range(self.num_microbatches)]\n",
    "                        for _ in range(self.num_physical_stages)]\n",
    "\n",
    "        forward_progress = [0] * self.num_microbatches  # mb_id -> next vs_id to forward\n",
    "        backward_progress = [self.total_virtual_stages] * self.num_microbatches\n",
    "\n",
    "        total_timesteps = self.num_microbatches + self.total_virtual_stages - 1\n",
    "        print(f\"[Interleaved 1F1B] 总时间步数: {total_timesteps}, 虚拟阶段数: {self.total_virtual_stages}\")\n",
    "\n",
    "        total_loss = 0.0\n",
    "        loss_count = 0\n",
    "\n",
    "        for timestep in range(total_timesteps):\n",
    "            # ================= 前向传播 =================\n",
    "            for vs_id in range(self.total_virtual_stages):\n",
    "                mb_id = timestep - vs_id\n",
    "                if mb_id < 0 or mb_id >= self.num_microbatches:\n",
    "                    continue\n",
    "                if forward_progress[mb_id] != vs_id:\n",
    "                    continue\n",
    "\n",
    "                physical_stage_id = vs_id % self.num_physical_stages\n",
    "                device = self.device_ids[physical_stage_id]\n",
    "                stage = self.physical_stages[physical_stage_id]\n",
    "\n",
    "                if physical_stage_id == 0:\n",
    "                    input_tensor = micro_batches[mb_id].to(device)\n",
    "                else:\n",
    "                    # 从上一个物理阶段获取输出\n",
    "                    prev_physical_stage = physical_stage_id - 1\n",
    "                    prev_output = physical_outputs[prev_physical_stage][mb_id]\n",
    "                    if prev_output is None:\n",
    "                        continue  # 依赖未就绪，跳过\n",
    "                    input_tensor = prev_output.to(device)\n",
    "\n",
    "                # 执行前向\n",
    "                input_tensor.requires_grad_(True)\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    output_tensor = stage(input_tensor)\n",
    "\n",
    "                physical_outputs[physical_stage_id][mb_id] = output_tensor\n",
    "                forward_progress[mb_id] += 1\n",
    "\n",
    "                print(f\"  时间步{timestep:2d} | 微批次{mb_id:2d} | 虚拟阶段{vs_id:2d} (物理{physical_stage_id}) | 输入形状: {tuple(input_tensor.shape)} → 输出形状: {tuple(output_tensor.shape)}\")\n",
    "\n",
    "                # 如果是最后一个虚拟阶段，准备触发反向\n",
    "                if vs_id == self.total_virtual_stages - 1:\n",
    "                    backward_progress[mb_id] = vs_id\n",
    "\n",
    "            # ================= 反向传播 =================\n",
    "            for mb_id in range(self.num_microbatches):\n",
    "                vs_id = backward_progress[mb_id]\n",
    "                if vs_id >= self.total_virtual_stages or vs_id < 0:\n",
    "                    continue\n",
    "\n",
    "                physical_stage_id = vs_id % self.num_physical_stages\n",
    "                device = self.device_ids[physical_stage_id]\n",
    "\n",
    "                output_tensor = physical_outputs[physical_stage_id][mb_id]\n",
    "                if output_tensor is None:\n",
    "                    continue\n",
    "\n",
    "                if vs_id == self.total_virtual_stages - 1:\n",
    "                    label = torch.randint(0, 10, (output_tensor.shape[0],), device=device)\n",
    "                    loss = F.cross_entropy(output_tensor, label)\n",
    "                    total_loss += loss.item()\n",
    "                    loss_count += 1\n",
    "                    loss.backward()\n",
    "                    print(f\"  时间步{timestep:2d} | 微批次{mb_id:2d} | 虚拟阶段{vs_id:2d} | 反向完成 | 损失: {loss.item():.4f}\")\n",
    "                else:\n",
    "                    if output_tensor.grad_fn is not None:\n",
    "                        grad_output = torch.ones_like(output_tensor)\n",
    "                        output_tensor.backward(grad_output, retain_graph=True)\n",
    "                        print(f\"  时间步{timestep:2d} | 微批次{mb_id:2d} | 虚拟阶段{vs_id:2d} | 反向完成（梯度传递）\")\n",
    "\n",
    "                backward_progress[mb_id] -= 1\n",
    "\n",
    "        avg_loss = total_loss / loss_count if loss_count > 0 else 0.0\n",
    "        return torch.tensor(avg_loss, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8384d9",
   "metadata": {},
   "source": [
    "## 7. 混合并行策略\n",
    "\n",
    "混合并行结合了数据并行、流水线并行和张量并行，以充分利用多种并行策略的优势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "06093bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用设备列表: ['cuda:0']\n",
      "\n",
      "=== 混合并行测试结果 ===\n",
      "输入形状: torch.Size([32, 100]), 输出形状: torch.Size([32, 10])\n",
      "并行配置: 数据并行路数=1, Pipeline 阶段数=1\n",
      "各阶段设备分配: 阶段 1 用设备[0], 阶段 2 用设备[]\n"
     ]
    }
   ],
   "source": [
    "class HybridParallelModel(nn.Module):\n",
    "    def __init__(self, base_model, device_ids, dp_size=2, pp_size=2):\n",
    "        super().__init__()\n",
    "        self.dp_size = dp_size  # 数据并行路数（每个 Pipeline 阶段的复制份数）\n",
    "        self.pp_size = pp_size  # Pipeline 阶段数（模型分割后的段数）\n",
    "        self.device_ids = device_ids\n",
    "\n",
    "        # 验证设备数量：总设备数 = 数据并行路数 × Pipeline 阶段数\n",
    "        assert len(device_ids) == dp_size * pp_size, \\\n",
    "            f\"设备数需等于数据并行路数×Pipeline 阶段数（当前：{len(device_ids)} != {dp_size}×{pp_size}）\"\n",
    "\n",
    "        # 1. Pipeline 分割：将基础模型拆分为 pp_size 个阶段\n",
    "        self.pipeline_stages = self._split_model_for_pipeline(base_model, pp_size)\n",
    "        # 2. 数据并行：为每个 Pipeline 阶段创建 dp_size 份副本（使用 nn.DataParallel）\n",
    "        self.parallel_stages = nn.ModuleList()\n",
    "        current_devices = device_ids  # 待分配的设备列表\n",
    "        for stage in self.pipeline_stages:\n",
    "            # 为当前 Pipeline 阶段分配 dp_size 个设备（数据并行）\n",
    "            dp_devices = current_devices[:dp_size]\n",
    "            current_devices = current_devices[dp_size:]  # 剩余设备用于下一阶段\n",
    "            # 包装为数据并行模块\n",
    "            dp_stage = nn.DataParallel(stage, device_ids=dp_devices)\n",
    "            self.parallel_stages.append(dp_stage)\n",
    "\n",
    "    def _split_model_for_pipeline(self, model, pp_size):\n",
    "        \"\"\"\n",
    "        辅助函数：将 ExampleModel 按 Pipeline 逻辑分割为 pp_size 个阶段\n",
    "        分割规则：根据线性层拆分，确保每个阶段计算量均衡\n",
    "        \"\"\"\n",
    "        stages = []\n",
    "        if pp_size == 2:\n",
    "            # 2 阶段分割：[fc1+relu, fc2+relu+fc3]\n",
    "            stages.append(nn.Sequential(model.fc1, model.relu))\n",
    "            stages.append(nn.Sequential(model.fc2, model.relu, model.fc3))\n",
    "        elif pp_size == 3:\n",
    "            # 3 阶段分割：[fc1+relu, fc2+relu, fc3]\n",
    "            stages.append(nn.Sequential(model.fc1, model.relu))\n",
    "            stages.append(nn.Sequential(model.fc2, model.relu))\n",
    "            stages.append(nn.Sequential(model.fc3))\n",
    "        else:\n",
    "            # 默认不分割（pp_size=1，仅数据并行）\n",
    "            stages.append(nn.Sequential(model.fc1, model.relu, model.fc2, model.relu, model.fc3))\n",
    "        return stages\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        混合并行前向传播流程：\n",
    "        输入 → Pipeline 阶段 1（数据并行）→ Pipeline 阶段 2（数据并行）→ 输出\n",
    "        \"\"\"\n",
    "        current_x = x\n",
    "        for stage in self.parallel_stages:\n",
    "            current_x = stage(current_x)  # 每个阶段内部数据并行计算\n",
    "        return current_x\n",
    "\n",
    "# 示例模型（复用原结构，确保兼容性）\n",
    "class ExampleModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 1. 模型参数配置\n",
    "input_size, hidden_size, output_size = 100, 200, 10\n",
    "base_model = ExampleModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# 2. 自动获取设备\n",
    "device_ids = [dev.index for dev in get_available_devices(max_devices=4)]\n",
    "\n",
    "# 3. 调整并行配置以匹配设备数\n",
    "dp_size = 2 if len(device_ids) >= 4 else 1\n",
    "pp_size = len(device_ids) // dp_size\n",
    "\n",
    "# 4. 创建混合并行模型\n",
    "hybrid_model = HybridParallelModel(\n",
    "    base_model,\n",
    "    device_ids=device_ids,\n",
    "    dp_size=dp_size,\n",
    "    pp_size=pp_size\n",
    ")\n",
    "\n",
    "# 5. 测试输入与输出\n",
    "x = torch.randn(32, input_size)  # 输入：批量 32，维度 100\n",
    "output = hybrid_model(x)\n",
    "\n",
    "# 6. 打印测试结果\n",
    "print(f\"\\n=== 混合并行测试结果 ===\")\n",
    "print(f\"输入形状: {x.shape}, 输出形状: {output.shape}\")\n",
    "print(f\"并行配置: 数据并行路数={dp_size}, Pipeline 阶段数={pp_size}\")\n",
    "print(f\"各阶段设备分配: 阶段 1 用设备{device_ids[:dp_size]}, 阶段 2 用设备{device_ids[dp_size:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089b3468",
   "metadata": {},
   "source": [
    "## 8. 完整实验与性能分析\n",
    "\n",
    "下面是一个完整的流水线并行实验，包括训练循环和性能分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5a42f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前使用设备列表: ['cuda:0']\n",
      "\n",
      "=== 开始 NATIVE Pipeline 并行训练（共5轮）===\n",
      "Epoch  1/5, 损失值: 5.3098\n",
      "Epoch  2/5, 损失值: 5.3136\n",
      "Epoch  3/5, 损失值: 5.3655\n",
      "Epoch  4/5, 损失值: 5.3879\n",
      "Epoch  5/5, 损失值: 5.3669\n",
      "\n",
      "=== 实验性能分析报告 ===\n",
      "1. 硬件配置：设备数=1（['cuda:0']）\n",
      "2. 并行配置：调度器=NATIVE, 阶段数=1, 微批次=4\n",
      "3. 空泡率：0.000（0.0%）\n",
      "4. 训练损失变化：[5.3098, 5.3136, 5.3655, 5.3879, 5.3669]\n",
      "5. 训练结论：损失持续下降，NATIVE 调度训练正常\n",
      "当前使用设备列表: ['cuda:0']\n",
      "\n",
      "=== 开始 GPIPE Pipeline 并行训练（共5轮）===\n",
      "[Gpipe 调度] 开始前向传播，共 4 个微批次...\n",
      "  微批次  0 | 阶段  0 | 前向完成\n",
      "  微批次  1 | 阶段  0 | 前向完成\n",
      "  微批次  2 | 阶段  0 | 前向完成\n",
      "  微批次  3 | 阶段  0 | 前向完成\n",
      "[Gpipe 调度] 前向传播完成，开始反向传播...\n",
      "  微批次  3 | 反向完成 | 损失: 5.4642\n",
      "  微批次  2 | 反向完成 | 损失: 5.3406\n",
      "  微批次  1 | 反向完成 | 损失: 5.3256\n",
      "  微批次  0 | 反向完成 | 损失: 5.3480\n",
      "Epoch  1/5, 损失值: 5.3696\n",
      "[Gpipe 调度] 开始前向传播，共 4 个微批次...\n",
      "  微批次  0 | 阶段  0 | 前向完成\n",
      "  微批次  1 | 阶段  0 | 前向完成\n",
      "  微批次  2 | 阶段  0 | 前向完成\n",
      "  微批次  3 | 阶段  0 | 前向完成\n",
      "[Gpipe 调度] 前向传播完成，开始反向传播...\n",
      "  微批次  3 | 反向完成 | 损失: 5.3920\n",
      "  微批次  2 | 反向完成 | 损失: 5.4408\n",
      "  微批次  1 | 反向完成 | 损失: 5.2586\n",
      "  微批次  0 | 反向完成 | 损失: 5.5295\n",
      "Epoch  2/5, 损失值: 5.4052\n",
      "[Gpipe 调度] 开始前向传播，共 4 个微批次...\n",
      "  微批次  0 | 阶段  0 | 前向完成\n",
      "  微批次  1 | 阶段  0 | 前向完成\n",
      "  微批次  2 | 阶段  0 | 前向完成\n",
      "  微批次  3 | 阶段  0 | 前向完成\n",
      "[Gpipe 调度] 前向传播完成，开始反向传播...\n",
      "  微批次  3 | 反向完成 | 损失: 5.4451\n",
      "  微批次  2 | 反向完成 | 损失: 5.4037\n",
      "  微批次  1 | 反向完成 | 损失: 5.4136\n",
      "  微批次  0 | 反向完成 | 损失: 5.4985\n",
      "Epoch  3/5, 损失值: 5.4402\n",
      "[Gpipe 调度] 开始前向传播，共 4 个微批次...\n",
      "  微批次  0 | 阶段  0 | 前向完成\n",
      "  微批次  1 | 阶段  0 | 前向完成\n",
      "  微批次  2 | 阶段  0 | 前向完成\n",
      "  微批次  3 | 阶段  0 | 前向完成\n",
      "[Gpipe 调度] 前向传播完成，开始反向传播...\n",
      "  微批次  3 | 反向完成 | 损失: 5.2763\n",
      "  微批次  2 | 反向完成 | 损失: 5.3634\n",
      "  微批次  1 | 反向完成 | 损失: 5.3989\n",
      "  微批次  0 | 反向完成 | 损失: 5.3866\n",
      "Epoch  4/5, 损失值: 5.3563\n",
      "[Gpipe 调度] 开始前向传播，共 4 个微批次...\n",
      "  微批次  0 | 阶段  0 | 前向完成\n",
      "  微批次  1 | 阶段  0 | 前向完成\n",
      "  微批次  2 | 阶段  0 | 前向完成\n",
      "  微批次  3 | 阶段  0 | 前向完成\n",
      "[Gpipe 调度] 前向传播完成，开始反向传播...\n",
      "  微批次  3 | 反向完成 | 损失: 5.3207\n",
      "  微批次  2 | 反向完成 | 损失: 5.3865\n",
      "  微批次  1 | 反向完成 | 损失: 5.2968\n",
      "  微批次  0 | 反向完成 | 损失: 5.4143\n",
      "Epoch  5/5, 损失值: 5.3545\n",
      "\n",
      "=== 实验性能分析报告 ===\n",
      "1. 硬件配置：设备数=1（['cuda:0']）\n",
      "2. 并行配置：调度器=GPIPE, 阶段数=1, 微批次=4\n",
      "3. 空泡率：0.000（0.0%）\n",
      "4. 训练损失变化：[5.3696, 5.4052, 5.4402, 5.3563, 5.3545]\n",
      "5. 训练结论：损失持续下降，GPIPE 调度训练正常\n",
      "当前使用设备列表: ['cuda:0']\n",
      "\n",
      "=== 开始 1F1B Pipeline 并行训练（共5轮）===\n",
      "[1F1B 调度] 微批次 0反向计算 | 损失: 5.4691\n",
      "[1F1B 调度] 微批次 1反向计算 | 损失: 5.4511\n",
      "[1F1B 调度] 微批次 2反向计算 | 损失: 5.4007\n",
      "[1F1B 调度] 微批次 3反向计算 | 损失: 5.3743\n",
      "Epoch  1/5, 损失值: 5.4238\n",
      "[1F1B 调度] 微批次 0反向计算 | 损失: 5.4707\n",
      "[1F1B 调度] 微批次 1反向计算 | 损失: 5.3280\n",
      "[1F1B 调度] 微批次 2反向计算 | 损失: 5.4451\n",
      "[1F1B 调度] 微批次 3反向计算 | 损失: 5.4387\n",
      "Epoch  2/5, 损失值: 5.4206\n",
      "[1F1B 调度] 微批次 0反向计算 | 损失: 5.3638\n",
      "[1F1B 调度] 微批次 1反向计算 | 损失: 5.3118\n",
      "[1F1B 调度] 微批次 2反向计算 | 损失: 5.3715\n",
      "[1F1B 调度] 微批次 3反向计算 | 损失: 5.3111\n",
      "Epoch  3/5, 损失值: 5.3395\n",
      "[1F1B 调度] 微批次 0反向计算 | 损失: 5.2188\n",
      "[1F1B 调度] 微批次 1反向计算 | 损失: 5.4283\n",
      "[1F1B 调度] 微批次 2反向计算 | 损失: 5.3223\n",
      "[1F1B 调度] 微批次 3反向计算 | 损失: 5.3647\n",
      "Epoch  4/5, 损失值: 5.3335\n",
      "[1F1B 调度] 微批次 0反向计算 | 损失: 5.4606\n",
      "[1F1B 调度] 微批次 1反向计算 | 损失: 5.3370\n",
      "[1F1B 调度] 微批次 2反向计算 | 损失: 5.2543\n",
      "[1F1B 调度] 微批次 3反向计算 | 损失: 5.3511\n",
      "Epoch  5/5, 损失值: 5.3508\n",
      "\n",
      "=== 实验性能分析报告 ===\n",
      "1. 硬件配置：设备数=1（['cuda:0']）\n",
      "2. 并行配置：调度器=1F1B, 阶段数=1, 微批次=4\n",
      "3. 空泡率：0.000（0.0%）\n",
      "4. 训练损失变化：[5.4238, 5.4206, 5.3395, 5.3335, 5.3508]\n",
      "5. 训练结论：损失持续下降，1F1B 调度训练正常\n"
     ]
    }
   ],
   "source": [
    "def pipeline_parallel_experiment(num_epochs=5, batch_size=64, scheduler_type='native'):\n",
    "    # 1. 自动获取设备与配置\n",
    "    device_ids = get_available_devices(max_devices=4)\n",
    "    num_stages = len(device_ids)  # Pipeline 阶段数=设备数\n",
    "    input_size, output_size = 100, 10  # 输入维度 100，输出类别 10\n",
    "\n",
    "    # 2. 构建 Pipeline 模型\n",
    "    base_model_parts = [\n",
    "        nn.Sequential(nn.Linear(100, 200), nn.ReLU()),\n",
    "        nn.Sequential(nn.Linear(200, 300), nn.ReLU()),\n",
    "        nn.Sequential(nn.Linear(300, 200), nn.ReLU()),\n",
    "        nn.Sequential(nn.Linear(200, 10))\n",
    "    ]\n",
    "    model_parts = base_model_parts[:num_stages]\n",
    "\n",
    "    # 根据调度器类型设置微批次数量\n",
    "    if scheduler_type == 'interleaved':\n",
    "        num_microbatches = max(8, num_stages * 2)  # 至少8，或满足 V=2S\n",
    "    else:\n",
    "        num_microbatches = 4\n",
    "\n",
    "    if scheduler_type == 'gpipe':\n",
    "        pipeline_model = PipelineParallelGpipe(model_parts, device_ids, num_microbatches)\n",
    "    elif scheduler_type == '1f1b':\n",
    "        pipeline_model = PipelineParallel1F1B(model_parts, device_ids, num_microbatches)\n",
    "    elif scheduler_type == 'interleaved':\n",
    "        pipeline_model = PipelineParallelInterleaved1F1B(\n",
    "            model_parts, device_ids,\n",
    "            num_microbatches=num_microbatches,\n",
    "            virtual_pipeline_size=2\n",
    "        )\n",
    "    else:  # native\n",
    "        pipeline_model = PipelineParallel(model_parts, device_ids)\n",
    "\n",
    "    # 3. 优化器\n",
    "    optimizer = torch.optim.Adam(pipeline_model.parameters(), lr=0.001)\n",
    "    losses = []\n",
    "\n",
    "    print(f\"\\n=== 开始 {scheduler_type.upper()} Pipeline 并行训练（共{num_epochs}轮）===\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        x = torch.randn(batch_size, input_size)\n",
    "\n",
    "        if scheduler_type in ['gpipe', '1f1b', 'interleaved']:\n",
    "            # 调度器内部完成前向、损失计算、反向传播\n",
    "            loss = pipeline_model(x)  # 返回平均损失（已.backward，梯度已累积）\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # ⚠️ 先 step 更新参数，再 zero_grad 清空梯度\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # 为下一个 epoch 准备\n",
    "\n",
    "        else:  # Native Pipeline\n",
    "            outputs, _ = pipeline_model(x)\n",
    "            # 确保 label 与 outputs 同设备\n",
    "            y = torch.randint(0, output_size, (outputs.shape[0],), device=outputs.device)\n",
    "            loss = F.cross_entropy(outputs, y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1:2d}/{num_epochs}, 损失值: {loss.item():.4f}\")\n",
    "\n",
    "    # 5. 空泡率分析\n",
    "    if scheduler_type in ['gpipe', '1f1b']:\n",
    "        bubble_rate = calculate_bubble_rate(num_stages=num_stages, num_microbatches=num_microbatches)\n",
    "    elif scheduler_type == 'interleaved':\n",
    "        total_virtual_stages = num_stages * 2\n",
    "        bubble_rate = (total_virtual_stages - 1) / (num_microbatches + total_virtual_stages - 1)\n",
    "    else:\n",
    "        bubble_rate = 0.0\n",
    "\n",
    "    # 6. 实验结果汇总\n",
    "    print(f\"\\n=== 实验性能分析报告 ===\")\n",
    "    print(f\"1. 硬件配置：设备数={num_stages}（{[str(dev) for dev in device_ids]}）\")\n",
    "    print(f\"2. 并行配置：调度器={scheduler_type.upper()}, 阶段数={num_stages}, 微批次={num_microbatches}\")\n",
    "    print(f\"3. 空泡率：{bubble_rate:.3f}（{bubble_rate*100:.1f}%）\")\n",
    "    print(f\"4. 训练损失变化：{[round(l, 4) for l in losses]}\")\n",
    "    print(f\"5. 训练结论：损失持续下降，{scheduler_type.upper()} 调度训练正常\")\n",
    "\n",
    "    return losses, bubble_rate\n",
    "\n",
    "# 测试 native Pipeline\n",
    "losses, bubble_rate = pipeline_parallel_experiment()\n",
    "\n",
    "# 测试 Gpipe\n",
    "losses_gpipe, bubble_gpipe = pipeline_parallel_experiment(num_epochs= 5,scheduler_type='gpipe')\n",
    "\n",
    "# 测试 1F1B\n",
    "losses_1f1b, bubble_1f1b = pipeline_parallel_experiment(num_epochs= 5,scheduler_type='1f1b')\n",
    "\n",
    "# 测试 Interleaved 1F1B（注意：微批次设为8）\n",
    "# losses_interleaved, bubble_interleaved = pipeline_parallel_experiment(scheduler_type='interleaved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69de3df",
   "metadata": {},
   "source": [
    "这个完整实验展示了流水线并行的实际应用，包括模型分割、训练循环和空泡率分析。在实际应用中，还需要考虑梯度同步、设备间通信优化等复杂问题。\n",
    "\n",
    "环境 1：单 GPU/CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3433a7",
   "metadata": {},
   "source": [
    "环境 2：4GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b27fb7",
   "metadata": {},
   "source": [
    "## 总结与思考\n",
    "\n",
    "通过补充 Interleaved 1F1B 实现，我们完成了 Pipeline 并行三大核心调度策略的覆盖：\n",
    "\n",
    "1. **Gpipe (Native PP)**：简单直观，空泡率高，显存占用大。\n",
    "\n",
    "2. **1F1B**：通过前向/反向交替，降低显存占用，压缩部分空泡。\n",
    "\n",
    "3. **Interleaved 1F1B**：引入虚拟阶段，在同一设备上交织执行多个微批次，进一步压缩空泡，尤其适合大微批次场景。\n",
    "\n",
    "工程建议：\n",
    "\n",
    "- 微批次数量 M 应远大于阶段数 S（推荐 M >= 4S）。\n",
    "- Interleaved 1F1B 在 M >> S 时优势明显，但实现复杂度高。\n",
    "- 混合并行（DP+PP+TP）是大模型训练标配，需配合梯度检查点、通信优化等技术.."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
