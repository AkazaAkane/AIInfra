<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->

# 服务器节点初了解

Author by: 

在深度学习算力需求激增的背景下，服务器作为算力载体的技术细节愈发关键。本文严格基于《01Server.pdf》内容，从服务器节点基础认知切入，按文档原有逻辑拆解通用服务器、AI服务器（GPU/NPU型）及AI集群的核心技术，深入解读硬件架构、功能特性与部署逻辑，避免无关扩展，确保内容与文档高度契合。


## 一、服务器节点初了解：认知AI算力载体的起点
《01Server.pdf》开篇以“服务器节点”为核心切入点，通过关键问题与框架铺垫，为后续技术解析奠定基础，这也是文档整体逻辑的“第一块基石”。

### 1.1 核心问题：聚焦大模型服务器的本质
文档首先抛出核心疑问——“我们训练大模型的服务器到底长什么样子？”，这一问题并非单纯指向外观，而是引导读者思考“支撑大模型训练的服务器需具备哪些核心能力”。不同于传统通用服务器，大模型训练对并行计算、数据吞吐、跨节点通信的需求极高，这一问题也暗示了后续内容将围绕“通用服务器与AI服务器的差异”“AI服务器的专属设计”展开，为技术解析划定了核心范围。

### 1.2 文档核心框架：从“算存网”到全栈应用的铺垫
在基础认知部分，文档已隐性呈现完整技术框架，后续所有内容均围绕此展开：
- **底层硬件**：以“AI芯片”（CPU/GPU/NPU）为算力核心，“通信与存储”为数据流转与留存基础，对应文档中“01.AI芯片（算）”“02.存储（存）”“02.互联（网）”章节；
- **中间层架构**：涵盖“AI集群”（节点规模化整合）、“计算架构”（编译器与优化技术），对应“03.AI集群”“04.计算架构”内容；
- **上层应用**：延伸至“模型训练与推理”“模型算法与数据”，对应“06.模型算法与数据”“07.大模型训练”“08.大模型推理”章节。
这种“从硬件到软件、从单点到集群”的框架，是理解后续技术细节的关键逻辑线。


## 二、通用服务器：传统计算场景的“均衡型节点”
《01Server.pdf》以“通用服务器:单节点形态”为核心，从形态、结构、功能三个维度展开，重点解析“1U 2路机架式服务器”这一主流型号，凸显其“综合能力均衡”的特性。

### 2.1 通用服务器单节点形态：适配传统场景的设计
通用服务器的“单节点形态”围绕“实用性与扩展性”设计，文档明确其核心参数与外观特征：
- **尺寸与CPU配置**：“1U 2路”是核心标识——“1U”代表服务器高度（1U=4.445cm），适配标准机柜，节省机房空间；“2路”指节点内搭载2颗CPU（如AMD EPYC 9004系列），满足多任务并发处理需求（如同时运行Web服务、数据库查询等），避免单CPU算力瓶颈。
- **存储配置方案**：文档给出两种主流方案，对应不同传统业务场景：
  1. **混合存储方案**：4×3.5英寸硬盘（大容量、低成本，适合存储冷数据，如历史业务日志）+2×E1.S硬盘（高密度、中速，适合存储热数据，如高频访问的业务文件）+2×M.2硬盘（高速、小容量，用于安装操作系统与核心应用，如数据库软件），兼顾“容量、速度、成本”，适用于企业级通用场景；
  2. **高容量存储方案**：10×2.5英寸硬盘，通过增加硬盘数量提升总存储容量（单盘最大20TB时，总容量可达200TB），同时2.5英寸硬盘体积小，可减少节点内部空间占用，适合数据备份、文件存储等对容量需求极高的场景。
- **前后板功能划分**：前面板集成硬盘接口、电源按钮与状态指示灯，运维人员可快速更换硬盘、直观查看节点运行状态（如电源灯、故障灯）；后面板集中布置电源接口、网络接口（NIC网卡模组）、VGA接口（本地调试）与USB接口，满足节点与外部设备的连接需求，体现“运维便利性”设计原则。

### 2.2 通用服务器物理结构：核心部件的协同逻辑
文档通过“编号-部件名称”表格，清晰呈现通用服务器的硬件组成，各部件功能与协同关系是理解其运行机制的关键：

| 编号 | 部件名称       | 技术功能与协同作用                                                                 |
|------|----------------|----------------------------------------------------------------------------------|
| 2    | 电源模块（PSU）| 提供稳定直流供电，文档中明确采用“冗余设计”（如1+1冗余）——当其中一个电源故障时，另一个电源可无缝接管，避免节点宕机，保障Web服务、数据库等传统业务的连续性；同时电源模块需适配节点总功耗（300-500W），避免功率不足导致的性能降频。 |
| 5    | PCle Riser 模组 | 作为PCIe插槽的“扩展桥梁”，将主板上的PCIe通道延伸至节点外部，支持外接网卡（如10Gb/s NIC模组）、RAID卡（用于硬盘阵列管理）等扩展设备，解决主板原生PCIe插槽不足的问题，提升服务器功能扩展性。 |
| 8    | NIC 网卡模组    | 实现服务器与网络的连接，文档中提及的NIC模组通常支持10Gb/s或25Gb/s速率，满足传统场景下的数据传输需求（如Web服务的用户请求接收、数据库的数据同步）；部分高端型号支持100Gb/s速率，适配高速局域网环境。 |
| 15   | 主板           | 服务器的“硬件骨架”，负责连接CPU、内存、硬盘、电源等所有部件，其设计需兼顾“信号完整性”与“散热布局”——避免CPU与内存的信号干扰，同时为风扇模块预留风道空间，确保各部件协同运行。 |
| 16   | 内存           | 临时存储CPU运算所需的数据与指令，文档中通用服务器最大支持24条DDR5内存，DDR5相比DDR4速率提升（最高6400MT/s），且支持ECC错误校验（降低数据传输错误率），避免传统计算中（如Excel数据处理、小批量数据统计）因内存错误导致的任务失败。 |
| 19   | CPU 处理器      | 通用服务器的“计算核心”，文档重点提及AMD EPYC 9004系列，该系列支持最多96个核心，具备强大的多线程处理能力，可同时运行多个并发任务（如100个Web服务进程），是传统计算场景的“算力中枢”。 |
| 25   | 风扇模块       | 采用“风冷散热”，通过“前进后出”的固定风道，将CPU、内存等发热部件的热量排出节点；风扇支持“智能调速”——根据硬件温度（如CPU温度超过70℃时提速）自动调整转速，平衡散热效率与能耗（避免风扇满速运行导致的高噪音与高功耗）。 |

### 2.3 通用服务器逻辑结构：性能上限的技术支撑
逻辑结构反映硬件部件的“数据流转关系”，文档通过表格明确其核心支持能力，这些参数直接决定通用服务器的性能边界：
1. **CPU与互联支持**：支持1-2颗AMD EPYC 9004系列处理器，处理器间通过4组xGMI总线互连（速率32GT/s）——xGMI总线是AMD专为多CPU架构设计的高速互连技术，可实现两颗CPU的内存共享（如CPU0可直接访问CPU1的内存数据），避免“CPU间数据传输瓶颈”，提升多任务并发效率。
2. **内存与扩展支持**：最大24条内存插槽（DDR5），若采用单条64GB内存，总容量可达1.5TB，满足传统场景下（如中型数据库、中间件部署）对内存的需求；同时支持3个PCIe 5.0扩展插槽与2个OCP 3.0网卡专用插槽——PCIe 5.0单通道速率32GB/s，适配高速NVMe SSD阵列卡（提升存储吞吐），OCP 3.0网卡支持热插拔，降低运维成本。
3. **管理接口支持**：通过DC-SCM板上的AST2600管理芯片，提供VGA（本地显示）、BMC管理网口（远程管理）、串口（调试）与TF卡（存储配置文件）接口——BMC（基板管理控制器）是运维核心，可远程实现服务器开机/关机、硬件状态监控（如CPU温度、内存占用）、固件升级，无需现场操作，提升传统集群的运维效率。

### 2.4 服务器“长条形态”的技术原因：需求导向的设计
文档专门提出“为什么服务器节点要做这么长长长？”，其答案围绕“散热、扩展、兼容性、运维”四大核心需求，这也是通用服务器形态设计的底层逻辑：
- **散热需求**：通用服务器虽功率密度低于AI服务器，但CPU、内存仍会产热。长条形态可设计“线性风道”，冷空气从前端均匀流过所有发热部件（如CPU→内存→硬盘），再从后端排出，避免短机身导致的“局部热量堆积”（如CPU与内存的散热气流相互干扰，引发高温降频）。
- **扩展性要求**：长条形态能容纳更多硬盘插槽（如10×2.5英寸硬盘）与扩展插槽，满足业务增长需求——例如企业数据量从10TB增至50TB时，可直接在原有节点上增加硬盘，无需更换整个服务器，降低升级成本。
- **硬件布局与兼容性**：服务器硬件（CPU、内存、电源、风扇）尺寸与安装位置有严格标准，长条形态可按“功能分区”布局（前端存储、中部计算、后端电源），避免硬件物理冲突；同时兼容不同厂商的标准化配件（如通用DDR5内存、PCIe扩展卡），降低采购与维护难度。
- **管理与维护便利性**：长条形态支持“前后维护”——前端可直接插拔硬盘，后端可更换电源与风扇，无需拆卸节点或移出机柜，运维时间从小时级缩短至分钟级，减少传统业务（如Web服务）的中断时长。


## 三、AI服务器：大模型场景的“异构算力节点”
《01Server.pdf》将AI服务器分为“NV GPU型”与“Ascend NPU型”两类，围绕“单节点形态、物理结构、逻辑结构”展开，凸显其“CPU+加速卡”的异构架构与“并行计算”核心能力，这是与通用服务器的本质区别。

### 3.1 AI服务器与通用服务器的核心差异
文档在“通用服务器vs AI服务器”章节中，明确两者的本质区别，为后续技术解析提供对比基准：

| 对比维度       | 通用服务器                     | AI服务器（GPU/NPU型）          |
|----------------|--------------------------------|--------------------------------|
| 核心架构       | 同构架构（仅CPU）              | 异构架构（CPU+GPU/NPU）        |
| 算力侧重       | 通用计算（逻辑控制、单线程任务）| 并行计算（矩阵乘法、向量运算，大模型核心需求） |
| 加速卡配置     | 无（或仅1张入门级显卡）        | 4-8张高性能GPU/NPU（如NVIDIA HGX H100、昇腾910） |
| 内存/存储需求  | GB级内存，TB级存储（机械硬盘为主） | TB级内存（缓存模型参数），PB级存储（NVMe SSD为主，高吞吐读训练数据） |
| 功耗与散热     | 300-500W，风冷即可             | 2000-5000W，需液冷/风液混合散热 |
| 典型应用       | Web服务、数据库、办公自动化    | 大模型训练/推理、多模态生成（如Stable Diffusion文生图）、自动驾驶算法训练 |

### 3.2 NV GPU型AI服务器：以GIGABYTE G593-ZD1-AAX1为例
文档以技嘉G593-ZD1-AAX1（Rev. 3.x）为样本，详细解析NV GPU型AI服务器的技术细节，该机型是目前大模型训练的主流选择，核心适配NVIDIA HGX H100 GPU。

#### 3.2.1 单节点形态：4U机架式的“算力密集设计”
该AI服务器采用“4U机架式”设计（高度约17.78cm），相比1U通用服务器，更大的内部空间用于容纳更多GPU与散热组件：
- **核心硬件配置**：
  - CPU：2颗AMD EPYC 9004/9005系列处理器（Socket SP5插槽），虽不直接参与AI并行计算，但负责“任务调度”（如将训练样本分配给8张GPU）、“数据预处理”（如文本tokenize）与“GPU协同管理”，是AI服务器的“控制中枢”；
  - GPU：8张NVIDIA HGX H100，这是核心算力来源——HGX H100基于Hopper架构，支持FP8精度计算（相比FP16，算力提升1倍、内存占用降低50%），单卡FP8算力达200PFlops，8张总算力1.6PFlops，可支撑千亿参数模型（如LLaMA 2 70B）的单机训练；
  - GPU互联：通过NVIDIA NVLink与NVSwitch实现GPU间高速通信，总带宽900GB/s——在分布式训练中，GPU需频繁传输梯度数据，高带宽可避免“梯度同步瓶颈”（如训练GPT-3时，梯度同步延迟从秒级降至微秒级）；
  - 内存与存储：12通道DDR5 RDIMM（24个插槽，最大1.5TB），用于缓存CPU预处理数据与GPU中间结果；8×2.5英寸Gen5 NVMe热插拔硬盘（单盘速率7.4GB/s），满足大模型训练“海量数据高速读取”需求（如每次迭代读取20GB训练样本，8块NVMe可实现59.2GB/s总吞吐）；
  - 电源：5+1冗余3000W电源（80 PLUS Titanium认证，转换效率96%），总功率15000W，满足8张HGX H100（单卡700W）与其他部件的供电需求，冗余设计避免电源故障导致的训练中断。

#### 3.2.2 物理结构：围绕GPU算力的部件协同
NV GPU型AI服务器的物理结构以“最大化GPU性能”为核心，各部件功能与协同逻辑如下：

| 编号 | 部件名称       | 技术功能与协同作用                                                                 |
|------|----------------|----------------------------------------------------------------------------------|
| 6    | 系统风扇模组   | 负责CPU、内存、硬盘的散热，采用“分区散热”设计（与GPU散热风道分离），避免热气流干扰；风扇支持“温度联动调速”——当CPU温度超过75℃时自动提速，平衡散热与能耗。 |
| 8    | PCle Switch 板  | CPU与GPU的“数据桥梁”，文档采用PEX89104型号（支持PCIe 5.0），将CPU的PCIe通道分配给8张GPU，实现CPU与GPU间32GB/s单通道速率，避免数据传输瓶颈。 |
| 9    | 散热器 & CPU   | CPU散热器采用“热管+大面积鳍片”设计，应对AMD EPYC 9004的360W高功耗；CPU负责调度GPU任务，如收集8张GPU的计算结果并汇总为全局梯度，再分发给所有GPU更新参数。 |
| 10   | 内存           | DDR5 RDIMM支持ECC校验，避免内存错误导致的训练中断；内存与CPU通过Infinity Fabric总线连接（速率数百GB/s），快速传输预处理后的训练数据至GPU。 |
| 14   | NVIDIA HGX H100 | 算力核心，单卡集成4个GPU芯片，8张HGX H100通过NVLink组成“GPU集群”，实现模型参数的分布式存储与并行计算（如将70B参数拆分到8张GPU，每张承担8.75B参数计算）。 |
| 15   | DPU            | 数据处理单元，卸载CPU的网络任务（如TCP/IP解析、数据压缩），释放CPU算力用于AI调度；同时支持RDMA（远程直接内存访问），实现不同AI节点间内存直接通信，降低分布式训练延迟。 |
| 18   | NIC 模组        | 支持2×10Gb/s或2×25Gb/s速率，用于AI服务器与存储集群的连接（读取训练数据）、与前端应用的连接（输出推理结果），部分型号支持100Gb/s速率，适配大规模集群环境。 |
| 22   | 风扇模组       | GPU专属散热风扇，“下吹式”风道直接对准GPU散热器，转速最高6000转/分钟，将HGX H100产生的热量快速排出，确保GPU温度低于85℃（避免高温降频）。 |

#### 3.2.3 逻辑结构：算力与通信的高效协同
逻辑结构体现“CPU-GPU-DPU”的数据流关系：CPU通过PCIe Switch将训练任务分配给8张GPU，GPU间通过NVLink同步参数，DPU负责节点间通信与网络任务卸载。文档明确其核心支持能力：
- 支持NVIDIA HGX H100 8-GPU，GPU间NVLink带宽900GB/s；
- 双AMD EPYC 9005/9004处理器，12通道DDR5内存（24个DIMM）；
- 8×2.5英寸Gen5 NVMe硬盘、4个FHHL PCIe Gen5 x16插槽（扩展设备）；
- 5+1冗余3000W电源，保障高负载下的供电稳定。

### 3.3 Ascend NPU型AI服务器：以Atlas 800T A2为例
文档以华为Atlas 800T A2推理服务器为样本，解析国产Ascend NPU型AI服务器的技术细节，该机型适配昇腾910 NPU，聚焦大模型推理与中小规模训练。

#### 3.3.1 单节点形态：4U机架式的“国产生态适配”
该服务器同样采用4U设计，核心配置围绕“国产硬件生态”展开：
- **CPU与NPU配置**：
  - CPU：4路鲲鹏920处理器（ARM架构），每路支持8条DDR4 DIMM，4路CPU通过“Hydra mesh”实现Full Mesh连接（CPU间双向带宽60GB/s），高效同步多CPU数据；
  - NPU：2路昇腾910 NPU板，每路通过4×PCIe 4.0 x16与CPU对接（单通道16GB/s，4通道64GB/s），昇腾910集成数千个AI Core，FP16算力256TFlops，适合ChatGLM-6B等模型的实时推理（每秒处理数千次对话请求）；
- **内存与存储**：4路CPU共32条DDR4插槽（最大2TB容量），支持ECC校验；存储支持SAS/SATA硬盘与NVMe SSD，满足推理场景的模型文件存储（如6B参数模型约12GB）与输入数据缓存需求；
- **通信与管理**：灵活IO卡支持2×100GE或4×25GE网卡，适配高速网络；华为Hi1711 BMC芯片提供VGA、管理网口、调试串口，兼容iBMC管理系统，支持远程运维。

#### 3.3.2 物理结构：国产硬件的协同设计
Ascend NPU型AI服务器的物理结构适配鲲鹏、昇腾芯片特性，核心部件功能如下：

| 编号 | 部件名称       | 技术功能与协同作用                                                                 |
|------|----------------|----------------------------------------------------------------------------------|
| 2    | NPU风冷散热器  | 专为昇腾910设计，“均热板+鳍片”结构，散热效率比传统散热器高30%，应对300W单卡功耗，避免NPU降频影响推理速度。 |
| 3    | NPU主板        | 承载昇腾910芯片与周边电路，支持NPU间PCIe互联，同时兼容华为MindSpore框架，可快速部署AI模型（如通过MindSpore Lite优化推理性能）。 |
| 7    | 风扇模块       | “大风量、低噪音”设计，“前进后出”风道为CPU、NPU散热，支持热插拔——更换时无需关机，保障推理服务连续性。 |
| 11   | 电源模块       | 2+1冗余2000W电源（80 PLUS Platinum认证，效率94%），总功率满足4路鲲鹏920与2路昇腾910供电，冗余设计避免电源故障。 |
| 13   | Riser 模组      | 扩展PCIe插槽，支持外接NVMe阵列卡、高速网卡，金属外壳减少电磁干扰，保障PCIe 4.0信号稳定性（避免数据传输错误）。 |
| 15   | CPU主板        | 承载4路鲲鹏920，集成Hydra mesh互联芯片，实现CPU间Full Mesh连接，同时支持DDR4 ECC内存，提升数据可靠性。 |
| 16   | CPU风冷散热器  | “多热管+大面积鳍片”，快速带走鲲鹏920的热量（单CPU功耗180W），确保CPU稳定运行，不影响NPU任务调度。 |
| 17   | DIMM（内存）   | DDR4 RDIMM速率3200MT/s，32条插槽最大2TB容量，缓存推理时的模型参数与输入数据（如ChatGLM-6B推理需13GB内存），避免频繁读取硬盘导致延迟。 |

### 3.4 AI服务器超节点扩展：规模化算力的实现
文档提及“昇腾A3:超节点384”，展现AI服务器从单节点到超节点的规模化整合逻辑：
- **硬件组成**：1个昇腾A3超节点包含384个昇腾处理器、192个鲲鹏处理器、6912个400G光模块与3168根16芯光纤，通过高速交换机实现全互联；
- **协同逻辑**：软件层面通过MindSpore框架实现多节点任务调度，硬件层面通过光纤与交换机减少节点间延迟（通信延迟低于10微秒），整体算力利用率超90%，可支撑万亿参数大模型（如盘古大模型）的训练。

## 五、总结：通用与AI服务器的定位与AI Infra愿景
《01Server.pdf》在“总结与思考”章节中，明确两类服务器的核心定位，并提出AI基础设施的发展目标：

- **通用服务器**：配备2个CPU，综合计算、存储、网络能力均衡，适用于传统计算任务（Web服务、数据库），是IT基础设施的“基石”；
- **AI服务器**：异构架构（2个CPU+4-8张GPU/NPU），具备强大并行计算能力，需高性能处理器、TB级内存与NVMe存储，是大模型训练/推理的“专属算力载体”。

## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?isOutside=true&aid=114988018571687&bvid=BV1DktBzLEvb&cid=31551195779&p=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>
