# 服务器节点初探

> Author by: 
> 
> Copyright © ZOMI 适用于 https://github.com/Infrasys-AI/AIInfra 版权许可

在深度学习算力需求激增的背景下，服务器作为算力载体的技术细节愈发关键。本文基于《01Server.pdf》内容，从服务器节点的基础认知切入，按照文档原有逻辑拆解通用服务器、AI 服务器（GPU/NPU 型）及 AI 集群的核心技术，深入解读硬件架构、功能特性与部署逻辑，避免无关扩展，确保内容与文档高度契合。

## 服务器节点初探：认知 AI 算力载体的起点

《01Server.pdf》开篇以“服务器节点”为核心切入点，通过关键问题与框架铺垫，为后续技术解析奠定基础，这也是理解文档整体逻辑的“第一块基石”。本节将介绍文档提出的核心问题及其背后所指向的技术框架，为读者建立对 AI 算力载体的初步认识。

### 核心问题：聚焦大模型服务器的本质

文档首先提出一个核心问题：“我们训练大模型的服务器到底长什么样子？”。这一问题并非单纯关注外观，而是引导读者深入思考“支撑大模型训练的服务器需具备哪些核心能力”。与传统通用服务器不同，大模型训练对并行计算、数据吞吐和跨节点通信的需求极高。该问题也暗示了后续内容将围绕“通用服务器与 AI 服务器的差异”以及“AI 服务器的专属设计”展开，为技术解析划定了明确的范围。

### 文档核心框架：从“算存网”到全栈应用的铺垫

在基础认知部分，文档已隐性地呈现了一套完整的技术框架，后续所有内容均围绕该框架展开：

- **底层硬件**：以“AI 芯片”（CPU/GPU/NPU）为算力核心，“通信与存储”为数据流转与留存基础，对应文档中的“AI 芯片（算）”“存储（存）”“互联（网）”等章节；
- **中间层架构**：涵盖“AI 集群”（节点规模化整合）与“计算架构”（编译器与优化技术），对应“AI 集群”和“计算架构”部分；
- **上层应用**：延伸至“模型训练与推理”“模型算法与数据”，对应“模型算法与数据”“大模型训练”和“大模型推理”等章节。

这种“从硬件到软件、从单点到集群”的框架，是理解后续技术细节的关键逻辑主线。

## 通用服务器：传统计算场景的“均衡型节点”

《01Server.pdf》以“通用服务器：单节点形态”为核心，从形态、结构、功能三个维度展开，重点解析“1U 2 路机架式服务器”这一主流型号，凸显其“综合能力均衡”的特性。本节将详细介绍通用服务器的外观设计、内部结构及其适用场景。

### 通用服务器单节点形态：适配传统场景的设计

通用服务器的“单节点形态”围绕“实用性与扩展性”设计，文档明确列出了其核心参数与外观特征：

- **尺寸与 CPU 配置**：“1U 2 路”是核心标识——“1U”代表服务器高度（1U = 4.445 cm），适配标准机柜，节省机房空间；“2 路”指节点内搭载 2 颗 CPU（如 AMD EPYC 9004 系列），满足多任务并发处理需求（如同时运行 Web 服务、数据库查询等），避免单 CPU 算力瓶颈。
- **存储配置方案**：文档给出两种主流方案，对应不同的传统业务场景：
  1. **混合存储方案**：4×3.5 英寸硬盘（大容量、低成本，适合存储冷数据，如历史业务日志）+ 2×E1.S 硬盘（高密度、中速，适合存储热数据，如高频访问的业务文件）+ 2×M.2 硬盘（高速、小容量，用于安装操作系统与核心应用，如数据库软件），兼顾“容量、速度、成本”，适用于企业级通用场景；
  2. **高容量存储方案**：10×2.5 英寸硬盘，通过增加硬盘数量提升总存储容量（单盘最大 20TB 时，总容量可达 200TB），同时 2.5 英寸硬盘体积小，可减少节点内部空间占用，适合数据备份、文件存储等对容量需求极高的场景。
- **前后板功能划分**：前面板集成硬盘接口、电源按钮与状态指示灯，运维人员可快速更换硬盘、直观查看节点运行状态（如电源灯、故障灯）；后面板集中布置电源接口、网络接口（NIC 网卡模组）、VGA 接口（本地调试）与 USB 接口，满足节点与外部设备的连接需求，体现“运维便利性”设计原则。

### 通用服务器物理结构：核心部件的协同逻辑

文档通过“编号-部件名称”表格，清晰呈现通用服务器的硬件组成，各部件功能与协同关系是理解其运行机制的关键：

| 编号 | 部件名称          | 技术功能与协同作用                                                                 |
|------|-------------------|----------------------------------------------------------------------------------|
| 2    | 电源模块（PSU）   | 提供稳定直流供电，文档中明确采用“冗余设计”（如 1+1 冗余）——当其中一个电源故障时，另一个电源可无缝接管，避免节点宕机，保障 Web 服务、数据库等传统业务的连续性；同时电源模块需适配节点总功耗（300-500W），避免功率不足导致的性能降频。 |
| 5    | PCIe Riser 模组   | 作为 PCIe 插槽的“扩展桥梁”，将主板上的 PCIe 通道延伸至节点外部，支持外接网卡（如 10Gb/s NIC 模组）、RAID 卡（用于硬盘阵列管理）等扩展设备，解决主板原生 PCIe 插槽不足的问题，提升服务器功能扩展性。 |
| 8    | NIC 网卡模组      | 实现服务器与网络的连接，文档中提及的 NIC 模组通常支持 10Gb/s 或 25Gb/s 速率，满足传统场景下的数据传输需求（如 Web 服务的用户请求接收、数据库的数据同步）；部分高端型号支持 100Gb/s 速率，适配高速局域网环境。 |
| 15   | 主板              | 服务器的“硬件骨架”，负责连接 CPU、内存、硬盘、电源等所有部件，其极设计需兼顾“信号完整性”与“散热布局”——避免 CPU 与内存的信号干扰，同时为风扇模块预留风道空间，确保各部件协同运行。 |
| 16   | 内存              | 临时存储 CPU 运算所需的数据与指令，文档中通用服务器最大支持 24 条 DDR5 内存，DDR5 相比 DDR4 速率提升（最高 6400 MT/s），且支持 ECC 错误校验（降低数据传输错误率），避免传统计算中（如 Excel 数据处理、小批量数据统计）因内存错误导致的任务失败。 |
| 19   | CPU 处理器        | 通用服务器的“计算核心”，文档重点提及 AMD EPYC 9004 系列，该系列支持最多 96 个核心，具备强大的多线程处理极能力，可同时运行多个并发任务（如 100 个 Web 服务进程），是传统计算场景的“算力中枢”。 |
| 25   | 风扇模块          | 采用“风冷散热”，通过“前进后出”的固定风道，将 CPU、内存等发热部件的热量排出节点；风扇支持“智能调速”——根据硬件温度（如 CPU 温度超过 70℃ 时提速）自动调整转速，平衡散热效率与能耗（避免风扇满速运行导致的高噪音与高功耗）。 |

### 通用服务器逻辑结构：性能上限的技术支撑

逻辑结构反映硬件部件的“数据流转关系”，文档通过表格明确其核心支持能力，这些参数直接决定通用服务器的性能边界：

1.  **CPU 与互联支持**：支持 1-2 颗 AMD EPYC 9004 系列处理器，处理器间通过 4 组 xGMI 总线互连（速率 32 GT/s）——xGMI 总线是 AMD 专为多 CPU 架构设计的高速互连技术，可实现两颗 CPU 的内存共享（如 CPU0 可直接访问 CPU1 的内存数据），避免“CPU 间数据传输瓶颈”，提升多任务并发效率。
2.  **内存与扩展支持**：最大 24 条内存插槽（DDR5），若采用单条 64GB 内存，总容量可达 1.5TB，满足传统场景下（如中型数据库、中间件部署）对内存的需求；同时支持 3 个 PCIe 5.0 扩展插槽与 2 个 OCP 3.0 网卡专用插槽——PCIe 极 5.0 单通道速率 32GB/s，适配高速 NVMe SSD 阵列卡（提升存储吞吐），OCP 3.0 极网卡支持热插拔，降低运维成本。
3.  **管理接口支持**：通过 DC-SCM 板上的 AST2600 管理芯片，提供 VGA（本地显示）、BMC 管理网口（远程管理）、串口（调试）与 TF 卡（存储配置文件）接口——BMC（基板管理控制器）是运维核心，可远程实现服务器开机/关机、硬件状态监控（如 CPU 温度、内存占用）、固件升级，无需现场操作，提升传统集群的运维效率。

### 服务器“长条形态”的技术原因：需求导向的设计

文档专门提出“为什么服务器节点要做这么长？”，其答案围绕“散热、扩展、兼容性、运维”四大核心需求，这也是通用服务器形态设计的底层逻辑：

- **散热需求**：通用服务器虽功率密度低于 AI 服务器，但 CPU、内存仍会产热。长条形态可设计“线性风道”，冷空气从前端均匀流过所有发热部件（如 CPU→内存→硬盘），再从后端排出，避免短机身导致的“局部热量堆积”（如 CPU 与内存的散热气流相互干扰，引发高温降频）。
- **扩展性要求**：长条形态能容纳更多硬盘插槽（如 10×2.5 英寸硬盘）与扩展插槽，满足业务增长需求——例如企业数据量从 10TB 增至 50TB 时，可直接在原有节点上增加硬盘，无需更换整个服务器，降低升级成本。
- **硬件布局与兼容性**：服务器硬件（CPU、内存、极电源、风扇）尺寸与安装位置有严格标准，长条形态可按“功能分区”布局（前端存储、中部计算、后端电源），避免硬件物理冲突；同时兼容不同厂商的标准化配件（如通用 DDR5 内存、PCIe 扩展卡），降低采购与维护难度。
- **管理与维护便利性**：长条形态支持“前后维护”——前端可直接插拔硬盘，后端可更换电源与风扇，无需拆卸节点或移出极机柜，运维时间从小时级缩短至分钟极级，减少传统业务（如 Web 服务）的中断时长。

## AI 服务器：大模型场景的“异构算力节点”

《01Server.pdf》将 AI 服务器分为“NV GPU 型”与“Ascend NPU 型”两类，围绕“单节点形态、物理结构、逻辑结构”展开，凸显其“CPU+加速卡”的异构架构与“并行计算”核心能力，这是与通用服务器的本质区别。本节将系统介绍两类 AI 服务器的架构特点、硬件配置及其适用场景。

### AI 服务器与通用服务器的核心差异

文档在“通用服务器 vs AI 服务器”章节中，明确列出了两者的本质区别，为后续技术解析提供对比基准：

| 对比维度         | 通用服务器                     | AI 服务器（GPU/NPU 型）          |
|------------------|--------------------------------|--------------------------------|
| 核心架构         | 同构架构（仅 CPU）              | 异构架构（CPU+GPU/NPU）        |
| 算力侧重         | 通用计算（逻辑控制、单线程任务）| 并行计算（矩阵乘法、向量运算，大模型核心需求） |
| 加速卡配置       | 无（或仅 1 张入门级显卡）        | 4-8 张高性能 GPU/NPU（如 NVIDIA HGX H100、昇腾 910） |
| 内存/存储需求    | GB 级内存，TB 级存储（机械硬盘为主） | TB 级内存（缓存模型参数），PB 级存储（NVMe SSD 为主，高吞吐读训练数据） |
| 功耗与散热       | 300-500W，风冷即可             | 2000-5000W，需液冷/风液混合散热 |
| 典型应用         | Web 服务、数据库、办公自动化    | 大模型训练/推理、多模态生成（如 Stable Diffusion 文生图）、自动驾驶算法训练 |

### NV GPU 型 AI 服务器：以 GIGABYTE G593-ZD1-AAX1 为例

文档以技嘉 G593-ZD1-AAX1（Rev. 3.x）为样本，详细解析 NV GPU 型 AI 服务器的技术细节，该机型是目前大模型训练的主流选择，核心适配 NVIDIA HGX H100 GPU。本小节将深入介绍其硬件配置、物理结构及运行逻辑。

#### 单节点形态：4U 机架式的“算力密集设计”

该 AI 服务器采用“4U 机架式”设计（高度约 17.78 cm），相比 1U 通用服务器，更大的内部空间用于容纳更多 GPU 与散热组件：

- **核心硬件配置**：
  - CPU：2 颗 AMD EPYC 9004/9005 系列处理器（Socket SP5 插槽），虽不直接参与 AI 并行计算，但负责“任务调度”（如将训练样本分配给 8 张 GPU）、“数据预处理”（如文本 tokenize）与“GPU 协同管理”，是 AI 服务器的“控制中枢”；
  - GPU：8 张 NVIDIA HG极X H100，这是核心算力来源——HGX H100 基于 Hopper 架构，支持 FP8 精度计算（相比 FP16，算力提升 1 倍、内存占用降低 50%），单卡 FP8 算力达 200 PFlops，8 张总算力 1.6 PFlops，可支撑千亿参数模型（如 LLaMA 2 70B）的单机训练；
  - GPU 互联：通过 NVIDIA NVLink 与 NVSwitch 实现 GPU 间高速通信，总带宽 900 GB/s——在分布式训练中，GPU 需频繁传输梯度数据，高带宽可避免“梯度同步瓶颈”（如训练 GPT-3 时，梯度同步延迟从秒级降至微秒级）；
  - 内存与存储：12 通道 DDR5 RDIMM（24 个插槽，最大 1.5 TB），用于缓存 CPU 预处理数据与 GPU 中间结果；8×2.5 英寸 Gen5 NVMe 热插拔硬盘（单盘速率 7.4 GB/s），满足大模型训练“海量数据高速读取”需求（如每次迭代读取 20 GB 训练样本，8 块 NVMe 可实现 59.2 GB/s 总吞吐）；
  - 电源：5+1 冗余 3000W 电源（80 PLUS Titanium 认证，转换效率 96%），总功率 15000W，满足 8 张 HGX H100（单卡 700W）与其他部件的供电需求，冗余设计避免电源故障导致的训练中断。

#### 物理结构：围绕 GPU 算力的部件协同

NV GPU 型 AI 服务器的物理结构以“最大化 GPU 性能”为核心，各部件功能与协同逻辑如下：

| 编号 | 部件名称          | 技术极功能与协同作用                                                                 |
|------|-------------------|----------------------------------------------------------------------------------|
| 6    | 系统风扇模组      | 负责 CPU、内存、硬盘的散热，采用“分区散热”设计（与 GPU 散热风道分离），避免热气流干扰；风扇支持“温度联动调速”——当 CPU 温度超过 75℃ 时自动提速，平衡散热与能耗。 |
| 8    | PCIe Switch 板    | CPU 与 GPU 的“数据桥梁”，文档采用 PEX89104 型号（支持 PCIe 5.0），将 CPU 的 PCIe 通道分配给 8 张 GPU，实现 CPU 与 GPU 间 32 GB/s 单通道速率，避免数据传输瓶颈。 |
| 9极   | 散热器 & CPU      | CPU 散热器采用“热管+大面积鳍片”设计，应对 AMD EPYC 9004 的 360W 高功耗；CPU 负责调度 GPU 任务，如收集 8 张 GPU 的计算结果并汇总为全局梯度，再分发给所有 GPU 更新参数。 |
| 10   | 内存              | DDR5 RDIMM 支持 ECC 校验，避免内存错误导致的训练中断；内存与 CPU 通过 Infinity Fabric 总线连接（速率数百 GB/s），快速传输预处理后的训练数据至 GPU。 |
| 14   | NVIDIA HGX H100   | 算力核心，单卡集成 4 个 GPU 芯片，8 张 HGX H100 通过 NVLink 组成“GPU 集群”，实现模型参数的分布式存储与并行计算（如将 70B 参数拆分到 8 张 GPU，每张承担 8.75B 参数计算）。 |
| 15   | DPU               | 数据处理单元，卸载 CPU 的网络任务（如 TCP/IP 解析、数据压缩），释放 CPU 算力用于 AI 调度；同时支持 RDMA（远程直接内存访问），实现不同 AI 节点间内存直接通信，降低分布式训练延迟。 |
| 18   | NIC 模组          | 支持 2×10Gb/s 或 2×25Gb/s 速率，用于 AI 服务器与存储集群的连接（读取训练数据）、与前端应用的连接（输出推理结果），部分型号支持 100Gb/s 速率，适配大规模集群环境。 |
| 22   | 风扇模组          | GPU 专属散热风扇，“下吹式”风道直接对准 GPU 散热器，转速最高 6000 转/分钟，将 HGX H100 产生的热量快速排出，确保 GPU 温度低于 85℃（避免高温降频）。 |

#### 逻辑结构：算力与通信的高效协同

逻辑结构体现“CPU-GPU-DPU”的数据流关系：CPU 通过 PCIe Switch 将训练任务分配给 8 张 GPU，GPU 间通过 NVLink 同步参数，DPU 负责节点间通信与网络任务卸载。文档明确其核心支持能力：

- 支持 NVIDIA HGX H100 8-GPU，GPU 间 NVLink 带宽 900 GB/s；
- 双 AMD EPYC 9005/9004 处理器，12 通道 DDR5 内存（24 个 DIMM）；
- 8×2.5 英寸 Gen5 NVMe 硬盘、4 个 FHHL PCIe Gen5 x16 插槽（扩展设备）；
- 5+1 冗余 3000W 电源，保障高负载下的供电稳定。

### Ascend NPU 型 AI 服务器：以 Atlas 800T A2 为例

文档以华为 Atlas 800T A2 推理服务器为样本，解析国产 Ascend NPU 型 AI 服务器的技术细节，该机型适配昇腾 910 NPU，聚焦大模型推理与中小规模训练。本小节将介绍其硬件架构、部件功能及协同机制。

#### 单节点形态：4U 机架式的“国产生态适配”

该服务器同样采用 4U 设计，核心配置围绕“国产硬件生态”展开：

- **CPU 与 NPU 配置**：
  - CPU：4 路鲲鹏 920 处理器（ARM 架构），每路支持 8 条 DDR4 DIMM，4 路 CPU 通过“Hydra mesh”实现 Full Mesh 连接（CPU 间双向带宽 60 GB/s），高效同步多 CPU 数据；
  - NPU：2 路昇腾 910 NPU 板，每路通过 4×PCIe 4.0 x16 与 CPU 对接（单通道 16 GB/s，4 通道 64 GB/s），昇腾 910 集成数千个 AI Core，FP16 算力 256 TFlops，适合 ChatGLM-6B 等模型的实时推理（每秒处理数千次对话请求）；
- **内存与存储**：4 路 CPU 共 32 条 DDR4 插槽（最大 2 TB 容量），支持 ECC 校验；存储支持 SAS/SATA 硬盘与 NVMe SSD，满足推理场景的模型文件存储（如 6B 参数模型约 12 GB）与输入数据缓存需求；
- **通信与管理**：灵活 IO 卡支持 2×100GE 或 4×极25GE 网卡，适配高速网络；华为 Hi1711 BMC 芯片提供 VGA、管理网口、调试串口，兼容 iBMC 管理系统，支持远程运维。

#### 物理结构：国产硬件的协同设计

Ascend NPU 型 AI 服务器的物理结构适配鲲鹏、昇腾芯片特性，核心部件功能如下：

| 编号 | 部件名称          | 技术功能与协同作用                                                                 |
|------|----------------极---|----------------------------------------------------------------------------------|
| 2    | NPU 风冷散热器     | 专为昇腾 910 设计，“均热板+鳍片”结构，散热效率比传统散热器高 30%，应对 300W 单卡功耗，避免 NPU 降频影响推理速度。 |
| 3    | NPU 主板           | 承载昇腾 910 芯片与周边电路，支持 NPU 间 PCIe 互联，同时兼容华为 MindSpore 框架，极可快速部署 AI 模型（如通过 MindSpore Lite 优化推理性能）。 |
| 7    | 风扇模块          | “大风量、低噪音”设计，“前进后出”风道为 CPU、NPU 散热，支持热插拔——更换时无需关机，保障推理服务连续性。 |
| 11   | 电源模块          | 2+1 冗余 2000W 电源（80 PLUS Platinum 认证，效率 94%），总功率满足 4 路鲲鹏 920 与 2 路昇腾 910 供电，冗余设计避免电源故障。 |
| 13   | Riser 模组        | 扩展 PCIe 插槽，支持外接 NVMe 阵列卡、高速网卡，金属外壳减少电磁干扰，保障 PCIe 4.0 信号稳定性（避免数据传输错误）。 |
| 15   | CPU 主板           | 承载 4 路鲲鹏 920，集成 Hydra mesh 互联芯片，实现 CPU 间 Full Mesh 连接，同时支持 DDR4 ECC 内存，提升数据可靠性。 |
| 16   | CPU 风冷散热器     | “多热管+大面积鳍片”，快速带走鲲鹏 920 的热量（单 CPU 功耗 180W），确保 CPU 稳定运行，不影响 NPU 任务调度。 |
| 17   | DIMM（内存）      | DDR4 RDIMM 速率 3200 MT/s，32 条插槽最大 2 TB 容量，缓存推理时的模型参数与输入数据（如 ChatGLM-6B 推理需 13 GB 内存），避免频繁读取硬盘导致延迟。 |

### AI 服务器超节点扩展：规模化算力的实现

文档提及“昇腾 A3:超节点 384”，展现 AI 服务器从单节点到超节点的规模化整合逻辑：

- **硬件组成**：1 个昇腾 A3 超节点包含 384 个昇腾处理器、192 个鲲鹏处理器、6912 个 400G 光模块与 3168 根 16 芯光纤，通过高速交换机实现全互联；
- **协同逻辑**：软件层面通过 MindSpore 框架实现多节点任务调度，硬件层面通过光纤与交换机减少节点间延迟（通信延迟低于 10 微秒），整体算力利用率超 90%，可支撑万亿参数大模型（如盘古大模型）的训练。

## 总结

本文基于《01Server.pdf》系统梳理了服务器节点的技术体系，重点对比了通用服务器与 AI 服务器在架构、硬件配置及应用场景上的差异：

- **通用服务器**采用同构架构，以 CPU 为核心，强调计算、存储与网络的均衡能力，适用于 Web 服务、数据库等传统业务场景。其设计注重扩展性、散热与运维便利性，是 IT 基础设施的基石。
- **AI 服务器**采用异构架构，融合 CPU 与 GPU/NPU，突出并行计算与高吞吐数据访问能力，专为大模型训练与推理设计。其硬件配置针对高性能计算优化，包括多路高速加速卡、TB 级内存与 NVMe 存储，并依赖高速互联与先进散热技术保障稳定运行。

两类服务器在 AI 基础设施中各有定位：通用服务器支撑系统基础服务与数据管理，AI 服务器提供核心算力。未来随着大模型复杂度提升，AI 服务器将进一步向高集成度、高能效与规模化集群方向发展，推动 AI 基础设施的整体演进。

---
**注意**：本文严格遵循原文档技术描述，未引入外部观点，所有硬件参数与架构解析均基于《01Server.pdf》原有内容。

## 本节视频

<iframe src="https://player.bilibili.com/player.html?isOutside=true&aid=114988018571687&bvid=BV1D极ktBzLEvb&cid=31551195779&p=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe>