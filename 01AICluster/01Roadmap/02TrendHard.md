<!--Copyright © ZOMI 适用于[License](https://github.com/Infrasys-AI/AIInfra)版权许可-->

# HPC 硬件发展趋势

Author by: 陈悦孜

承接上文我们介绍了高性能计算和集群的定义，这章我们介绍高性能计算发展的趋势。我们将从核心硬件、基础软件和应用软件三个维度来分析高性能计算发展的趋势。

高性能计算硬件发展历程和未来趋势从高性能网络、处理器、服务器、存储器四个核心维度展开分析。其演进逻辑始终围绕性能突破、能效优化与场景适配展开：

- 高性能处理器：从通用多核到异构计算。
- 高性能网络：从低延迟到高带宽互联。
- 高性能存储器：从容量扩展到存算协同。
- 高性能服务器：从单机性能到绿色化集群。

## 高性能处理器

### CPU 主导时代

早期高性能计算依赖于并行 CPU 集群，比如 Intel Xeon, AMD Opteron 系列，通过提升主频、增加核心数量（多核/众核）和优化指令集实现性能增长。

Intel Xeon和AMD Opteron（及其后续的EPYC系列）的性能演进是一个多维度的系统工程。

在提升主频方面，厂商通过半导体制程工艺的迭代（如从22nm到更先进的制程）和微架构的改进（如Intel的P-Core与AMD的Zen系列架构），不断突破单核效率的极限。然而，物理定律决定了高频率会带来功耗和发热的指数级增长，因此单纯提升频率已不可持续。

与此同时，增加核心数量成为提升整体吞吐量的关键手段。从早期的双核、四核发展到如今的众核设计（如AMD EPYC 9004系列高达96核，Intel Xeon Platinum 8592+提供64核），这一演进依赖于先进的封装技术，核心数量的暴增使得服务器能够并行处理海量任务，完美契合云计算虚拟化、大数据分析和高性能计算等现代工作负载，大幅提升了系统的整体吞吐能力。

在指令集优化方面，SIMD（单指令多数据流）扩展和专用指令集的引入显著提升了单位计算效率。例如，Intel的AVX-512指令集允许一条指令同时处理16个32位浮点数，在科学计算和人工智能推理中带来数倍性能提升；而AMD支持的AES-NI指令集则通过硬件加速加密解密操作，极大提升了网络安全协议的处理效率。这些优化让每个核心在单位时间内能完成更多有用的工作，从而在架构层面实现了“事半功倍”的效果。

优化指令集提升了单个核心的计算效率，增加核心数量将这种效率扩展至大规模并行处理，而提升主频则在功耗允许范围内为所有核心提供更快的执行节奏。现代服务器处理器的设计哲学由此从追求单一核心的极致速度，转变为在给定功耗和成本下，通过架构创新与协同，实现整个系统计算吞吐量的最大化。

![Intel Xeon 和 AMD Opteron](images/02TrendHard01.png)

#### 主频提升达瓶颈

2000 年代初，Intel Prescott 核心尝试通过 90nm 工艺提升主频，然而高主频必然会带来功耗和发热的增长，功耗突破 100W，导致“高频低效”问题，最终转向多核架构随着工艺微缩接近物理极限，单核主频停滞在 3-4GHz 范围，性能增长转而依赖多核并行和指令级并行（ILP）优化。

早期代表性 HPC CPU 参数对比

|系列|代表型号|	发布时间|核心数|主频|关键技术特点 |
|-|-|-|-|-|-|
AMD Opteron|	Opteron 6180 SE	|2010|	12|	2.5GHz|	集成内存控制器，HyperTransport 总线|
AMD Opteron|	Opteron 6166 HE|	2010|	12	|1.8GHz|	低功耗设计（65W）|
Intel Xeon|	Xeon E5-2600 v4	|2016|	22|	2.2GHz|	超线程，AVX2 指令集|
Intel Xeon	|Xeon Platinum 8380	|2021|	40|	2.3GHz|	支持 PCIe 4.0，8 通道 DDR4|

![Intel Xeon 和 AMD Opteron](images/02TrendHard02.png)

#### 协处理器兴起

协处理器是一种辅助处理器，设计用于与主 CPU 协同工作，专门处理特定类型的任务以提高系统整体性能。协处理器是计算机系统中与主处理器（CPU）配合工作的专用处理单元，它能够分担 CPU 的特定计算任务、提高特定类型运算的效率，并且优化系统整体性能。

协处理器可用于专用加速，比如 Intel Xeon Phi（MIC 架构，2012-2020）尝试众核路线。以下为 Phi 初期型号及其参数。

型号|	核心数|	浮点性能|	内存容量|	内存带宽|	TDP|	售价|
|-|-|-|-|-|-|-|
Phi 3100|	57|	1 TFlops|	6 GB|	240 GB/s|	300W	|<=$2,000|
Phi 5110P|	60|	1.01 TFlops|	8 GB|	320 GB/s|	225W|	$2,649|
Phi 7120|	61|	1.2 TFlops|	16 GB|	352 GB/s	|300W|	$4,129|

![PHI](images/02TrendHard03.png)

​​GPU 加速​​也属于协处理器范畴，NV CUDA 革命性地将 GPU 用于通用计算， Tesla 成为通用并行计算标杆。首代 Tesla 架构（如 G80）引入统一着色器设计，将矢量计算单元拆分为标量核心（CUDA Core），支持 C 语言编程，实现 SIMT（单指令多线程）执行模型，奠定通用计算基础，将 GPU 从图形协处理器升级为通用计算引擎，定义“CPU+GPU”异构标准。

![Tesla](images/02TrendHard04.png)

#### 国产 CPU 突破

中国自主研发的申威 SW26010 处理器，凭借其创新的异构众核架构（包含 256 个高性能计算核心和 1 个管理核心），实现了单芯片高达每秒 3 万亿次（3 TFlops）的峰值浮点计算能力。这一突破性设计，成为支撑中国首台、也是世界首批 E 级（Exascale， 百亿亿次）超级计算机——“神威·太湖之光”的核心动力源泉。

尤为关键的是，申威 SW26010 采用了完全独立自主的申威指令集架构（SW ISA），彻底摆脱了对国外主流指令集的依赖，确保了核心技术自主可控的安全性与战略意义。

搭载了超过 40,000 颗申威 SW26010 处理器的“神威·太湖之光”超级计算机，于 2016 年 6 月在全球超级计算机 TOP500 排行榜上震撼登顶，终结了美国超算长达 23 年的榜首垄断地位。它不仅以 93 PFlops（每秒 9.3 亿亿次）的 Linpack 实测持续性能创造了当时的世界纪录，更是全球首台突破 10 亿亿次/秒（100 PFlops）大关的超级计算机，标志着人类正式迈入百亿亿次计算时代（E 级超算时代） 的门槛。

![申威 SW26010](images/02TrendHard05.png)

#### Arm 高性能计算突破

Arm架构是挑战x86在个人电脑和服务处理器市场长达数十年主导地位的新的一种计算机指令集架构，有着RISC（精简指令集）的独特指令集设计，并且由此而来的低功耗和高能效比的特点。Arm架构采用IP授权模式，非常灵活，苹果、华为、NVIDIA、高通购买ARM的架构授权或核心设计授权，然后根据自己的特定需求进行深度定制和优化。这种开放性催生了多样化的创新。Arm架构天生的高能效和多核特性，非常适合原生横向拓展的大规模集群的模式。

NVIDIA Grace CPU和鲲鹏 920 (Kunpeng 920) 都是基于ARM架构的服务器级处理器。它们代表了从传统x86架构（Intel/AMD）向ARM架构转型的重要力量。

NVIDIA Grace CPU是基于ARMv9指令集的自研的核心架构，引入LPDDR5X实现内存子系统革新，提供超高带宽和能效，​​LPDDR5X和纠错码​​的设计使得能效提升2倍。CPU-GPU协同一致性缓存的机制和​NVLink-C2C直连使​​延迟降至 1/10。

![NVIDIA Grace CPU](images/02TrendHard06.png)

鲲鹏920处理器是基于ARMv8.2指令集的自研的核心架构，采用7nm 工艺，多核集成，最高可到64核，综合算力强劲，为华为自主设计。它通过优化分支预测算法、提升运算单元数量、改进内存子系统架构等一系列微架构设计提高处理性能。

![鲲鹏 920ARM-based 处理器](images/02TrendHard07.png)

以下表格对比传统 Arm（移动端）和高性能计算优化版（服务器级）的架构对比。

|特性 | 传统 Arm（移动端）|HPC 优化版（服务器级）|
|-|-|-|
|指令集|精简指令集（RISC）|拓展 SIMD 之路（SVE/SVE2）|
|核心规模|多核低频（能效优先）|512 核以上众核架构（Fujitsu A64FX）|
|内存系统|低带宽 LPDDR|HBM2e（>1TB/s 带宽）|
|功耗管理|动态调频（DVFS）|精细功耗门控（Per-core PowerGating）|

### 处理器发展趋势

#### GPU 主导加速市场，国产力量野蛮生长​

NVIDIA H100/AMD MI300X 成为 AI/HPC 核心算力，国产替代（比如寒武纪、华为昇腾）加速发展。NVIDIA凭借其H100/H200及新一代Blackwell GPU，以其强大的算力、成熟的CUDA生态及NVLink全栈技术，牢牢占据全球AI训练市场的绝对统治地位，构建了难以撼动的“护城河”。AMD则以其MI300系列（尤其是专为AI优化的MI300X）的异构设计和高带宽内存（HBM3）优势，成为最重要的挑战者和替代选择。

在此背景下，国产替代已上升为关乎产业安全的战略任务。华为昇腾（Ascend）依托其达芬奇架构和昇思（MindSpore）框架，在国内互联网、智算中心等市场快速落地，是国产阵营中技术栈最完整、生态建设最成熟的代表。寒武纪等企业也在特定领域持续迭代。然而，国产力量仍面临严峻挑战：其一是尖端制造（如先进封装）被“卡脖子”的硬件限制；其二是相比CUDA，软件生态和应用成熟度仍有巨大差距。当前格局呈现出“国际巨头引领创新，国产芯片在特定赛道加速替代”的并行双轨态势。

#### ​​Chiplet 技术​​和存算一体技术加速异构多样化

1. Chiplet 技术

AI 技术蓬勃发展，数据中心对高算力芯片需求技术增长，算力芯片与传统消费级芯片相比，算力芯片面积更大，存储容量更大，对互连速度要求更高，Chiplet 技术可以很好的满足这些大规模芯片的性能和成本需求，因而得到广泛运用。

Chiplet 是一种先进的芯片设计和制造方法。Chiplet 即小芯粒，它将一类满足特定功能的 die（裸片），通过 die-to-die内部互联技术将多个模块芯片与底层基础芯片封装在一起，形成一个系统芯片。 

它不再像传统方式那样把包含处理器核心、内存控制器、I/O 接口等所有功能都集成在一个巨大的单片硅芯片（Monolithic Die）上，而是将复杂的大芯片拆分成多个更小、功能更单一、工艺更优化的独立小芯片。这些小芯片（例如CPU 核心、GPU 核心、高速缓存、I/O 模块）各自可以采用最适合其功能和成本的半导体工艺（如逻辑用先进工艺，I/O 用成熟工艺）独立制造。然后，它们通过高速、高密度的先进封装技术（如硅中介层、EMIB、CoWoS 等）像“搭乐高”一样集成封装在一个基板上，形成一个功能完整的系统级芯片（SoC）。

![Chiplet](images/02TrendHard08.png)

这种技术提高良率、降低制造成本（尤其是先进工艺成本）、设计更灵活（可复用成熟 Chiplet）、加速产品上市、实现异构集成（不同工艺、不同厂商的 Chiplet 组合）。AMD、Intel都通过多芯片封装提升集成度和良率。

2. 存算一体技术

算力的需求急速发展使得业界也通过变革当前计算架构来实现算力突破。主流芯片的冯诺依曼结构设计将季赛和存储分离，二者配合完成数据存取与运算。但是由于处理器的设计以提升计算速度为主，存储则更注重容量提升和成本优化，“存”“算”之间性能失配，从而导致了访存带宽低、时延长、功耗高等问题，即通常所说的“存储墙”和“功耗墙”访存愈密集，“墙”的问题愈严重算力提升愈困难。

由此，存算一体的技术应运而生。它的技术核心是将存储与计算完全融合，有效克服冯·诺依曼架构瓶颈，并结合后摩尔时代先进封装、新型存储器件等技术，实现计算能效的数量级提升。

存算技术是一种旨在突破传统计算机“冯·诺依曼瓶颈”的革命性架构。目前学术界和工业界均在开展存算一体技术研究，学术界主要关注狭义的存算一体，即利用存储介质进行计算;工业界关注商用化进程，着重宣传广义存算一体概念，但分类方法尚未完全统一。本章节将对广义存算一体技术进行分类，望达成广泛共识。

根据存储与计算的距离远近，我们将广义存算一体的技术方案分为三大类，分别是近存计算、存内处理和存内计算。存内计算即狭义的存算一体。

- 近存计算通过芯片封装和板卡组装等方式，将存储单元和计算单元集成，增加访存带宽、减少数据搬移，提升整体计算效率。近存计算仍是存算分离架构，本质上计算操作由位于存储外部、独立的计算单元完成其技术成熟度较高，主要包括存储上移、计算下移两种方式。

- 存内处理是在芯片制造的过程中，将存和算集成在同一个晶粒中，使存储器本身具备了一定算的能力。存内处理本质上仍是存算分离相比于近存计算，“存”与“算”距离更近。当前存内处理方案大多在内存芯片中实现部分数据处理，在DRAM Die中内置处理单元，提供大吞吐低延迟片上处理能力，可应用于语音识别、数据库索引搜索、基因匹配等场景。

- 存内计算即狭义的存算一体在芯片设计过程中，不再区分存储单元和计算单元，真正实现存算融合。存内计算是计算新范式的研究热点，其本质是利用不同存储介质的物理特性，对存储电路进行重新设计使其同时具备计算和存储能力，直接消除“存”"算个界限，使计算能效达到数量级提升的目标。

存算技术这种技术大幅减少数据搬运，从而显著提升能效比 和计算速度，特别适用于数据密集型的 AI 推理/训练、大数据分析等场景。台积电 3D Fabric 技术将计算单元堆叠至存储层突破冯·诺依曼瓶颈。

## 高性能网络

### 网络技术早期阶段 (1990s-2000s)

一方面以太网主导网络应用领域。千兆以太网成本低被广泛采用，但延迟高大于100μs、带宽瓶颈明显。千兆以太网是 IEEE 802.3ab/z 标准定义的以太网技术，传输速率达 1 Gbps，是传统百兆以太网的 10 倍。它延续了以太网的帧结构和基础设施。它具有布线低成本，场景多样，和现有设备高兼容性的优点。它是企业局域网、家庭宽带、工业控制、IP 监控摄像头等领域的主流选择，同时因成熟稳定，成为打印机、IoT 设备等中低速设备的默认网络接口。

另一方面大量的专有网络兴起。Myrinet、Quadrics 等私有协议网络出现，延迟约为10μs并且有较高吞吐量。然而，这些技术的生态高度封闭成为其致命短板：它们依赖特定网卡、交换机等专属的硬件，和私有软件栈，导致成本高昂、兼容性差、用户被厂商锁定且不同系统间互操作性困难。最终，这种封闭性严重制约了其普及和发展，在 2009 年前后，它们被更具开放性和成本效益的技术，如InfiniBand和基于以太网的RDMA所取代而退出主流市场。

![Ethernet 和 Myrinet](images/02TrendHard09.png)

### 网络主流技术成型（2010s 至今）

这里介绍主流技术有 InfiniBand、RoCE 和 NVLink。

- InfiniBand由Mellanox主导，通过RDMA技术彻底绕开操作系统协议栈，实现可低于1μs的微秒级超低延迟和当前达400Gbps，约50GB/s的超高带宽，完美匹配HPC与AI算力需求。其核心优势在于开放生态，统一标准，兼容多厂商设备，同时支持与以太网融合。凭借性能与通用性的平衡，InfiniBand 自 2010 年代起取代旧式专有网络，成为超算中心和云数据中心的主流互连方案。

- RoCE是基于以太网 RDMA 的技术，兼顾低成本与高性能，是华为、阿里云等国产厂商加速布局的重要方向。它通过绕过操作系统内核协议栈，使数据直接从应用内存访问网卡，实现典型约为10-20μs的微秒级延迟和可达400Gbps的高吞吐量，逼近InfiniBand性能。其核心价值在于兼顾高性能与低成本：复用现有以太网交换机和布线设施，大幅降低部署门槛。正因这一优势，华为、阿里云、腾讯云等中国厂商积极布局RoCEv2协议，推动其在云数据中心、AI 训练集群及存储网络中的规模化应用，成为突破InfiniBand生态垄断的关键国产化路径。

- NVLink 是英伟达专为 GPU 互联设计高速总线，用于节点内多卡互联，演进 NV Fusion。NVLink 是 NVIDIA 专为 GPU 高性能互联设计的私有高速总线协议，核心目标是解决节点内多 GPU 卡间的通信瓶颈。它通过点对点直连架构，替代传统 PCIe 总线，实现远超 PCIe 的带宽，第四代达 900GB/s纳秒级延迟，显著加速 GPU 间数据交换。

### 高性能网络发展趋势

在人工智能、高性能计算和超大规模数据中心蓬勃发展的浪潮下，网络性能，尤其是低延迟和高带宽，已成为制约整体系统效能的关键瓶颈。同时，不同技术路径的融合趋势日益明显，共同推动网络基础设施向更高性能、更智能、更灵活的方向演进。核心发展趋势聚焦于以下几个方面。

- InfiniBand 持续领先超低延迟领域​​。代表当前IB发展巅峰的 NVIDIA Quantum-2 支持 400Gb/s 单端口带宽，显著提升了节点间通信效率，NDR 1.6Tb/s 超高速率将满足未来千卡、万卡级 AI 训练集群对极致带宽的需求。

- RoCE v2 与智能网卡 (DPU/IPU) 重塑高性能以太网​​。以太网借助 DPU/IPU 降低 CPU 负载，基于RoCE v2技术实现类似IB的RDMA能力提升性能和效率，使其在高性能网络领域具备了强大的竞争力。智能网卡比如数据处理器 (DPU) 或基础设施处理器 (IPU)这些专用硬件将网络、存储和安全功能从主机 CPU 卸载下来，通过处理网络协议栈、虚拟化、存储加速、安全加密等技术，释放宝贵的 CPU 核心用于运行应用。硬件级 RDMA 加速显著降低延迟，提升带宽利用率，使以太网接近 IB 的性能水平。

- ​光互连技术突破铜缆限制，奠定下一代网络基石​​。随着速率演进，传统铜缆在距离、功耗、密度和信号完整性方面面临严峻挑战。光互连是突破这些瓶颈的可行方案。硅光集成、共封装光学和光学链路开关成为下一代网络关键。硅光集成利用成熟的硅基半导体工艺制造光学器件，实现高集成度、低成本、低功耗的光模块和芯片，是规模化应用的关键。共封装光学将光引擎与交换机芯片或ASIC共同封装在同一基板上，而非传统可插拔模块，这大大缩短了电信号传输距离，显著降低功耗，提升带宽密度和信号质量。光学链路开关利用光开关技术直接在光层动态配置连接路径，而无需经过电交换芯片，特别适用于超大规模数据中心中计算、存储资源池间的极低延迟、高带宽、可重构的互连，能效比极高。

- 新互联协议面向异构计算与 AI 的专用优化。传统的CPU间互联协议（如 PCIe）在连接 CPU 与加速器（如 GPU、NPU）或加速器之间的高效通信时，存在延迟、带宽和扩展性瓶颈，对于CPU与加速器或者加速器提速通信的需求，新互联协议成为新一代的集群互联技术。华为灵渠总线互联是一个典型的代表，是专门为CPU-to-NPU和NPU-to-NPU通信设计的新一代集群互联技术。它提供超高带宽、超低延迟的直接连接，显著优化大规模 AI 训练和推理集群中异构计算单元的数据交换效率。

图为谷歌推出的 OCS（光学链路开关），光互连技术。

![高性能网络发展趋势](images/02TrendHard11.png)

## 高性能存储

### 硬盘时代

SSD 取代 HDD 提升 I/O 速度引发的硬盘革命。

HDD（普通硬盘）通过涂有磁性材料的旋转磁盘盘片来存储数据。它们的机械臂上具有读写头，通过在这些旋转磁盘上来回移动来访问数据。由于机械构造上的局限性，传统 HDD 的运行速度比较慢。它们的数据传输速率较低，且延迟较高。HDD包含旋转磁盘和机械臂等的活动部件，因此更容易出现机械故障。与 SSD 相比，HDD 通常可提供更大的存储容量，而且每 GB 成本更低。希捷、西数推出 HDD，受限于输入输出的ms级延迟，难满足高性能计算的需求。

SSD（固态硬盘）是硬盘技术后起之秀，相比 HDD 具有明显的性能优势，因而广受欢迎。SSD 使用基于半导体的非易失性闪存，利用硅芯片的物理和化学特性来提供更大的存储容量，能够加快数据访问和传输速度。在读取速度，耐用性和能效上和 HDD 比都有明显的优势。速度上 SSD 比 HDD 快得多，启动速度和文件传输速度都更快，并且可缩短应用程序的加载时间。耐用性更高，由于 SSD 没有活动部件，因此它们更加耐用，不容易因为遭到冲击或跌落，而造成物理损坏和数据丢失。SSD 的功耗比 HDD 低，因此适合在笔记本电脑和移动设备中使用，这些设备的电池续航时间至关重要。三星 PM1733（30TB）顺序读写达 7GB/s，延迟降至 50μs。

![硬盘革命](images/02TrendHard12.png)

### 分布式存储和新存储文件系统

#### 分布式文件系统

分布式文件系统（Distributed File System，DFS）是一种通过计算机网络与节点（可简单的理解为一台计算机）相连或是若干不同的逻辑磁盘分区或卷标组合在一起而形成的完整的有层次的文件系统。DFS管理的物理存储资源不一定直接连接在本地节点上。DFS为分布在网络上任意位置的资源提供一个逻辑上的树形文件系统结构，用户访问分布在网络上的共享文件更加简便。Lustre、Ceph 等文件系统支撑 EB 级数据吞吐，比如 Lustre 带宽突破 1TB/s。

![Lustre](images/02TrendHard18.png)

#### AI 驱动存储优化 

随着 LLM 的爆火，在训练和推理层面都对 Infra 提出新挑战，传统深度学习时代模型规模较小，猝存储上面临的都是小文件问题，对性能没有太多需求，而 LLM 动辄 100B 的参数量，在数据管道的各个阶段需要花费大量时间在不同系统之间复制数据，对 GPU 利用率较低的现象有很大影响，所以 AI 存储技术需要有新的进展。

全闪文件系统（All-Flash File System）是专门为全闪存存储阵列设计的文件系统，这种存储阵列完全由固态驱动器组成。与传统的文件系统相比，全闪文件系统针对固态驱动器的特性进行了优化，以提高性能和效率，延长固态驱动器的使用寿命。

全闪方案通过 NVMe-OF 和 RDMA 实现存储集群。NVMe 规范是从零开始专为 SSD 而设计的规范，NVMe-oF是支持创建超高性能存储网络，其时延能够比肩直接连接的存储器。因而可在服务器之间按需共享快速存储设备。NVMe-oF 可视为基于光纤通道的 SCSI 或 iSCSI 的替代品，其优势在于时延更低、I/O 速率更高，且生产力更优。NVMe-OF+RDMA 采用不中断远程机器系统 CPU 处理的情况下直接存取内存的技术实现 AI 高性能存储。

需要注意的是，全闪方案基本没有开源方案，全部都是商业公司的支持，有些还需要购买专门的硬件，或者需要在公有云依赖特定的硬件支持。目前比较火的是 weka.io 和 VAST，国内的云厂商都有提供对应的解决方案，比如阿里云的 CPFS 以及百度云的 PFS 等。开源的目前发现有 Intel 的 DAOS。

![WekaIO](images/02TrendHard19.png)

如图展示 wekaIO 全闪存储集群，它是一个 8 节点的 WEKApod，具有 PCIe Gen 5 连接，通过 Nvidia 的 Quantum-2 QM9700 64 端口 400 Gbps InfiniBand 交换机连接到 Nvidia DGX H100 服务器。

![wekaIO 全闪存储集群](images_src/02TrendHard19.png)

### 新存储技术革命

#### SCM（存储级内存）

SCM 的全称为 Storage Class Memory，即“存储级内存”。有时它也被人们称作 Persisent Memory（持久内存）或 Non-Volatile Memory（非易失性内存）。它是一种拥有近似于硬盘的持久性，又有如内存般高速的存储介质。

在数据中心的结构中，DRAM 内存与 SSD 硬盘之间存在巨大的性能鸿沟。SRAM 和 DRAM 在数据存取延迟较小，性能佳，但与此同时需要更高的成本，也有着更快的损耗速度。数据在内存与硬盘之间的往返，成为了制约整体性能的瓶颈所在。为了填补 DRAM 同 SSD 之间的巨大性能落差，SCM 以分层存储结构中“DRAM 缓冲区”的身份被研制出来。

Intel Optance 傲腾是英特尔率先投入商用的 SCM 产品，P4800X 固态硬盘于 2017 年 3 月发布。傲腾不同于传统硬盘的地方在于它采用 3DXpoint 的相变存储（PCM）技术。傲腾减少电子泄露，增强断电后的数据保持能力，改善“冷数据”问题，同时大幅降低数据的读取延迟，有卓越的随机读写性能。由于不需要 DRAM 缓存的优化，无需断电保护电容，傲腾自身就能达成停电保护的能力。虽然英特尔宣称“傲腾性能接近内存”，但是成本高昂，没能占据市场重要地位。

图为傲腾 P4800X。

![傲腾 P4800X](images/02TrendHard20.png)



#### HBM（高带宽内存）  

存储与运算之间的“内存墙”于计算速度的影响由于数据、算力需求的增长愈发显现，提升内存带宽是存储芯片关注的重要问题。HBM（High Bandwidth Memory）高带宽存储器，被视作是新一代 DRAM 技术，它通过使用硅通孔垂直堆叠多个DRAM，和NPU或GPU封装实现高度集成和依赖，实现大容量、高位宽的 DDR 组合阵列。

![HBM 技术示意图](images/02TrendHard14.png)

HBM 在 2013 年由 SK 海力士将 TSV 技术应用于 DRAM 而首次研发出来，HBM1 的带宽高于 DDR4 和 GDDR5 产品，同时以较小的外形尺寸消耗较低的功率，更能满足 GPU 等带宽需求较高的处理器。2016 年，三星和 SK 海力士开始量产 HBM2 DRAM。2018 年末，JEDEC 推出 HBM2E 规范，以支持增加的带宽和容量。当传输速率上升到每管脚 3.6Gbps 时，HBM2E 可以实现每堆栈 461GB/s 的内存带宽。此外，HBM2E 支持最多 12 个 DRAM 的堆栈，内存容量高达每堆栈 24GB。与 HBM2 相比，HBM2E 具有技术更先进、应用范围更广泛、速度更快、容量更大等特点。2019 年和 2020 年，SK 海力士和三星分别研发量产出 HBM2E 产品。2020 年，另一家存储巨头美光宣布加入到这一赛场中来。

2022 年，新一代高内宽内存 HBM3 的标准规范发布，在存储密度、带宽、通道、可靠性、能效等各个层面进行扩充升级，HBM3 每引脚数据速率提高到 6.4Gb/s，单设备带宽 819GB/s 。


在 AI 芯片中，高带宽存储主要用于加速模型执行。对于大型神经网络，模型权重和中间激活会占据数 GB 到数十 GB 内存，HBM 提供的高带宽使得这些数据可以在算力单元和内存之间高速交换，减少等待时间。例如，NVIDIA A100 GPU 配备了 40GB HBM2e 显存，带宽达 1.5TB/s，能够高效训练上百亿参数的模型；在推理时，HBM 也使得批处理、并发推理的吞吐大幅提升。AI 芯片通常会将 HBM 带宽与片上 SRAM 缓存相配合：频繁访问的数据尽量留存在片上缓存，不常用的大数据则从 HBM 取用。高带宽存储的出现甚至影响了算法设计，当内存带宽不再是主要限制，研究人员可以设计更宽的网络或更大的并行度。

### 高性能存储发展趋势

这里总结高性能存储未来发展趋势。

1. 存储级内存 SCM：CXL 3.0 协议推动存储池化，实现 CPU/GPU 共享内存资源。

存储级内存（SCM）在 CXL 3.0 协议的推动下，将实现跨 CPU/GPU/FPGA 的内存资源池化，打破传统内存墙限制。通过高速互联技术，不同处理器可动态共享 SCM 资源，显著提升异构计算的效率，同时降低延迟和系统复杂度。

2. HBM3e 与 HBM4​​：2025 年 HBM3 带宽突破 1TB/s，成 AI 芯片标配。

HBM 技术持续迭代，2025 年 HBM3e 将实现 1TB/s 以上的带宽，HBM4 更可能采用 3D 堆叠与硅通孔优化，成为 AI 训练芯片的标配。其高带宽特性可满足大模型参数实时加载的需求，推动千亿级参数模型的部署。

3. 存算一体架构：计算单元嵌入存储层，减少数据搬运。

存算一体架构通过将计算单元嵌入存储层，彻底重构传统冯·诺依曼架构。近内存计算（如三星 HBM-PIM）和内存内计算可减少数据搬运能耗，特别适合矩阵运算密集型任务。

4. 存算一体器件：忆阻器 Memristor、相变存储器 PCM 支持内存计算，降低搬运功耗。

新型存算一体器件如忆阻器（Memristor）、相变存储器（PCM）和铁电存储器（FeRAM）利用其物理特性直接进行模拟计算，在边缘 AI 场景中展现出超低功耗优势。例如，基于忆阻器的神经形态芯片可实现能效比提升 100 倍，为端侧智能设备带来突破。

![高性能发展趋势](images/02TrendHard15.png)

## 高性能服务器

### 服务器性能导向阶段（2000s）

#### 刀片服务器普及

刀片服务器其实是一种采用模块化设计的“精简版”服务器，为了尽量少占用机柜空间而进行了优化。整个刀片系统由刀片机箱和刀片设备组成。刀片机箱可容纳多个刀片服务器，它同时提供了共享的电源、冷却、网络及各种互连和管理组件。当然，不同刀片厂商在刀片和机箱里面添加哪些组件，有着不一样的设计思想。

![刀片式服务器和机架式服务器](images/02TrendHard16.png)

高密度是刀片最主要的优势之一。在标准的高 42U、宽 19 英寸的服务器机柜中，1U（高 1.75 英寸）规定了任何设备的最小尺寸，对于机架式设备的安装台数有着严格的限制，比如最多放 24 台 1U 的机架服务器。而刀片计算的主要优点就在于消除了这种限制，截至 2009 年，通过刀片系统，一个机柜可实现容纳 128 个独立服务器的超高密度。“高计算密度、资源整合、易管理、低功耗”等是刀片服务器的天然优势。

从 2000 年开始，刀片服务器以“超高的计算密集性”为核心，如 RLX Technologies 公司的 System 324 刀片服务器，可以在标准 42U 机箱中集成 324 个 CPU。但是为了支持如此高的计算密集度，设计者不得不在单个 CPU 的功耗和散热问题上做出很大的让步。所以早期采用的 CPU 都是性能相对较弱的低功耗的处理器。对于简单的任务实现，这是个合适的实现方法。

发展到 2003 年，高性能计算需求增长，这些性能较弱的高密度刀片服务器跟不上时代需求。IBM 推出 BladeCenter 刀片式服务器。刀片服务器不再过分强调计算密集性，而是把计算密集性和单个刀片性能进行了一次权衡，在两者之中选择了一个合适的平衡点。2006 年，IBM 推出了革命性的 BladeCenter H，增加了紧凑型刀片服务器的带宽，可将其网络传输数据容量提高 10 倍。同时，IBM 还推出了基于 Cell 宽带引擎（Cell BE）的刀片计算系统，专为那些需要密集计算能力和独特性能来应对大量图形处理和数字应用的公司而设计。同年，HP 发布其历时三年研发的第三代产品——BladeSystem c-class 刀片服务器，主要针对企业下一代数据中心而设计，具备虚拟连接、能量智控和洞察管理等三项创新技术。

刀片服务器发展里程碑————IBM 的 BladeCenter H。

![IBM BladeCenter](images/02TrendHard21.jpg)

HP 潜心三年的力作——BladeSystem c-class 刀片服务器。

![HP BladeSystem](images/02TrendHard22.jpg)

#### 服务器遇到风冷散热瓶颈

高密度服务器如刀片服务器机柜能够达到的最大功率达 20 kW 以上。传统风冷冷却介质是空气适用于小于 15kW 功率的机柜，PUE 大于 1.5。这种负载大大超出了传统数据中心的供电和散热设计能力。

P.S. PUE（Power Usage Effectiveness，电能使用效率） 是衡量数据中心能源效率的核心指标，计算公式为 $PUE= \frac{IT 设备耗电量}{数据中心总耗电量}$ 。理想 PUE = 1.0，所有电力均用于 IT 设备，无损耗。实际上大多数数据中心 PUE 在 1.2~2.0 之间，超算/AI 数据中心通过液冷等技术可降至 1.1 以下。

同时因为服务器工作负载量大，且在对安全性和可靠性要求较高的场合，如政府机关、银行、证券、通信、企业等领域，特殊的工作性质，要求服务 7X24 不间断工作，服务器的处理器始终处于高负载状态，使之发热量也随之加大。如今，散热设计与散热能力已经成为服务器设计者的首要考虑问题，它不仅关系到服务器的稳定性和安全性，而且还要考虑到散热系统所需的能耗。

### 服务器绿色化阶段（2010s ~ 至今）

#### 散热技术突破

##### 冷板液冷

冷板式液冷作为非接触式液冷，通过液冷板将CPU和GPU等发热器件的热量间接传递给封闭在循环管路中的冷却液体带走热量。冷板式液冷对于服务器芯片组件及附属部件改动较小，主要途径为加装液冷模块，采用集中式或分布式 CDU 供液、Manifold 分液，对芯片、内存等部件进行精准制冷。主要可分为部分冷板式、全冷板式以及冷板背门式。

曙光数创作为液冷数据中心的先行者，旗下的 SLiquid 液冷技术品牌旗舰产品——相变浸没液冷数据中心基础设施解决方案 C8000 系列，可实现 PUE 低至 1.04，不仅符合国家的各项政策要求，更是把节能做到了极致。能达到这样的高效散热效果，又绿色低碳。

图为曙光浸没式液冷服务器。

<img src="images/02TrendHard23.jpeg" style="width: 30%; height: auto;" alt="曙光浸没式液冷服务器">

图为曙光浸没式液冷服务器内部结构。

<img src="images/02TrendHard24.jpeg" style="width: 50%; height: auto;" alt="曙光浸没式液冷解决方案">

##### 浸没式液冷

浸没式液冷是将发热电子元器件直接浸没在非导电冷却工质中,通过冷却工质循环流动来进行散热的接触式冷却技术,可实现 100%液体冷却,散热节能优势明显，且几乎没有噪音。浸没式液冷技术根据冷却液是否发生相变,分为单相和相变两种类型。

阿里云的“麒麟”采用了浸没式液冷散热的服务器部署方案，利用绝缘冷却液取代传统风冷。由于无需风扇、空调等制冷设备，可在任何地方部署，因此也极大地降低了冷却能耗和空间消耗。这种浸没液冷服务器集群技术，可将数据中心能源使用效率大幅降低到接近于理论极限值 1.0，约为1.09，达到世界领先水平。“麒麟”节能，可大幅降低数据中心的冷却能耗；绿色环保，冷却液吸收元器件的热量后，再沸腾气化，气体上升遇到冷凝板后液化落回主机柜中循环利用，热量在几乎零功耗的情况下被冷凝管传递出去，与此同时高密封性也使得元器件远离湿度、灰尘、振动的影响，受干扰情况几乎为 0。“麒麟”所使用的绝缘冷却液，完全绝缘且无腐蚀性，即使浸没元器件 20 年以上，成分不会发生任何变化，也不会对电子元器件产生任何影响。而得益于浸没式液冷高效的散热效率，“麒麟”无需空调等大型制冷设备，可以在任何地方部署，节省空间 75%以上。“麒麟”在单位体积内的计算能力比过去提升了 10 倍。

![阿里云“麒麟”浸没式液冷解决方案](images/02TrendHard25.png)

#### 模块化设计

高性能服务器走向模块化，超算系统的模块化设计通过标准化机柜单元集成计算、存储、网络等核心组件，实现快速部署与弹性扩展。

以 Frontier 超级计算机为例，其采用模块化机柜架构，每个机柜单元包含计算节点、液冷散热系统、分布式存储模块及高速互联网络，单个模块即可独立运行，74 个机柜通过高速网络互联可以连接一起运行。

Frontier 超级计算机部署于美国能源部橡树岭国家实验室，由 74 个标准 19 英寸（48 厘米）机柜组成，每个机柜重达 8,000 磅。系统采用高度集成的模块化设计，每个机柜配备 64 个刀片服务器，每个刀片包含 2 个计算节点。Frontier系统通过 HPE Cray Slingshot-11 高速网络互联，采用基于以太网的定制拓扑结构，实现 CPU-GPU 间高效协同。该设计体现了超算系统在模块化架构、异构计算与高密度集成方面的前沿水平。

![Frontier 超级计算机](images/02TrendHard27.jpeg)

### 高性能服务器未来趋势

未来高性能服务器将呈现两大核心发展趋势：在散热技术方面，液冷技术正从实验性方案快速演进为标准配置；在系统架构方面，模块化整机柜设计变得流行，如曙光硅立方率先实现机柜级全浸没液冷方案，华为昇腾也推出 Cloud Matrix 超节点架构。这些创新技术共同推动高性能服务器向高效节能、智能集成的方向发展。

#### 液冷普及

全球范围内，数据中心行业面临日益严格的能效和碳排要求。欧盟的《能效指令》要求到2030年数据中心PUE不超过1.3，中国"东数西算"工程对枢纽节点的PUE要求更为严格，不得超过 1.3。液冷凭借物理性散热优势，比风冷效率高 1000 倍，成为唯一达标路径，进而从边缘实验技术跃升为全球数据中心的新一代基础设施标准。Intel/AMD 的部分商用服务器采用冷板式液冷；阿里云“麒麟”浸没液冷集群，谷歌 AI 数据中心采用浸没式液冷技术。

#### 整机柜设计

整机柜设计是数据中心技术演进的核心方向，它通过系统级整合大幅提升算力密度与能效。

曙光硅立方单机柜整合计算、冷却、供电单元，支持 300kW 制冷能力，采用浸没相变液冷技术，将计算部件整体浸入液态冷媒中，实现 PUE 值低至 1.04，单机柜功率密度达 160kW，较传统风冷提升 4-5 倍。曙光硅立方呈现独特的“硅立方”形态，采用层次化、模块化、立体化的创新设计，大幅节省物理空间。

![曙光硅立方](images/02TrendHard26.jpeg)

华为昇腾 384 超节点，首创将 384 颗昇腾 NPU 和 192 颗鲲鹏 CPU 通过全新高速网络 MatrixLink 全对等互联，形成一台超级“AI 服务器”，单卡推理吞吐量跃升到 2300 Tokens/s。在高速互联总线联接下，昇腾 384 超节点共由 12 个计算柜和 4 个总线柜构成，是目前业界最大规模的超节点，算力总规模达 300Pflops，网络互联总带宽达 269TB/s，内存总带宽达 1229TB/s，通过最佳负载均衡组网等方案，未来可以支撑更大规模的模型演进。

![华为昇腾 384 超节点](images/02TrendHard18.jpg)

## 本节视频

<html>
<iframe src="https://player.bilibili.com/player.html?isOutside=true&aid=114767700172746&bvid=BV16A3wzEELU&cid=30765482532&p=1&as_wide=1&high_quality=1&danmaku=0&t=30&autoplay=0" width="100%" height="500" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"> </iframe>
</html>

## reference

- https://www.witintech.com/upfiles/202312051029031006456460.pdf
- https://pdf.dfcfw.com/pdf/H3_AP202307091592122354_1.pdf 
- https://www.dell.com/support/contents/zh-cn/article/product-support/self-support-knowledgebase/data-storage-backup-and-recovery/support-for-hard-disk-drive
- https://aws.amazon.com/cn/compare/the-difference-between-ssd-hard-drive/
- https://post.smzdm.com/p/a90el8pp/
- https://zhuanlan.zhihu.com/p/23269431888
- https://zhuanlan.zhihu.com/p/597784274
- https://server.51cto.com/article/143125.html
- https://dataenergy.sugon.com/c48283.jsp
- https://dataenergy.sugon.com/c81871.jsp
- https://mp.weixin.qq.com/s/ZVklrCPV7oNHvfcJpnPdJA
- https://developer.aliyun.com/article/74471
- https://www.eet-china.com/mp/a135747.html
- https://zh.wikipedia.org/wiki/%E5%89%8D%E6%B2%BF_(%E8%B6%85%E7%B4%9A%E9%9B%BB%E8%85%A6)
- https://www.baogaobox.com/insights/250602000011116.html
- https://server.zol.com.cn/731/7319835.html
- https://baike.baidu.com/item/%E7%A1%85%E7%AB%8B%E6%96%B9/24229490?anchor=1&fragment=1#1
- https://www.sohu.com/a/915158063_115479?scm=10001.325_13-325_13.0.0.5_32
- https://www.jiuyangongshe.com/a/2g6hckcy8nz
